2025-07-07 11:27:22,599 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 11:27:22,604 - INFO - Loaded 6236 verses for validation
2025-07-07 11:27:22,604 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 11:27:22,604 - INFO - Models to test: qwen
2025-07-07 11:27:22,604 - INFO - Questions limit: All
2025-07-07 11:27:22,627 - INFO - Using device: mps
2025-07-07 11:27:22,627 - INFO - Using Apple Silicon MPS backend
2025-07-07 11:27:22,627 - INFO - 
============================================================
2025-07-07 11:27:22,627 - INFO - Starting evaluation for QWEN
2025-07-07 11:27:22,627 - INFO - ============================================================
2025-07-07 11:27:22,627 - INFO - Starting benchmark for QWEN
2025-07-07 11:27:22,627 - INFO - Using device: mps
2025-07-07 11:27:22,628 - INFO - Using Apple Silicon MPS backend
2025-07-07 11:27:22,628 - INFO - Processing 881 questions for qwen (starting from 1)
2025-07-07 11:27:22,628 - INFO - Processing question 1/881 (ID: 0) for qwen
2025-07-07 11:27:22,628 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 11:27:22,628 - INFO - Loading model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 11:27:22,628 - INFO - Device: mps, Dtype: torch.float16, Quantization: False
2025-07-07 11:27:33,374 - INFO - Successfully loaded Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 11:33:08,139 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 11:33:08,152 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 11:33:08,152 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 11:33:08,154 - INFO - Normalized predicted verses: []
2025-07-07 11:33:08,154 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 11:33:08,154 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 11:33:08,154 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 11:33:08,154 - INFO -   Matched verses: []
2025-07-07 11:33:09,160 - INFO - Processing question 2/881 (ID: 1) for qwen
2025-07-07 11:33:09,160 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 11:33:09,161 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 11:35:21,383 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 11:35:21,389 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 11:35:21,391 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 11:35:21,391 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 11:35:21,392 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 11:35:21,392 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 11:35:21,392 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 11:35:21,392 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 11:35:21,392 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 11:35:21,393 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 11:35:21,393 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 11:35:21,394 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 11:35:21,394 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 11:35:21,394 - INFO -   Matched verses: []
2025-07-07 11:35:22,400 - INFO - Processing question 3/881 (ID: 2) for qwen
2025-07-07 11:35:22,400 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 11:35:22,400 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 11:37:08,612 - INFO - Extracted JSON from code block: 762 characters
2025-07-07 11:37:08,614 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 11:37:08,614 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 11:37:08,614 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 11:37:08,615 - WARNING - Invalid colon format: '[At-Tawbah:106]' -> 'At-Tawbah:106'
2025-07-07 11:37:08,615 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 11:37:08,615 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 11:37:08,616 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 11:37:08,616 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 11:37:08,616 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 11:37:08,616 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 11:37:08,616 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 11:37:08,616 - WARNING - Invalid colon format: '[At-Tawbah:106]' -> 'At-Tawbah:106'
2025-07-07 11:37:08,616 - WARNING - Invalid colon format: '[At-Tawbah:106]' -> 'At-Tawbah:106'
2025-07-07 11:37:08,616 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 11:37:08,616 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 11:37:08,616 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 11:37:08,617 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 11:37:08,624 - INFO - Normalized predicted verses: ['2:255', '49:11', '5:10', '6:84', '9:106']
2025-07-07 11:37:08,624 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 11:37:08,624 - INFO -   Response: 762 chars, Found 5 verses
2025-07-07 11:37:08,624 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 11:37:08,624 - INFO -   Matched verses: []
2025-07-07 11:37:09,629 - INFO - Processing question 4/881 (ID: 3) for qwen
2025-07-07 11:37:09,630 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 11:37:09,630 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 11:39:00,433 - INFO - Extracted JSON from code block: 749 characters
2025-07-07 11:39:00,438 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 11:39:00,438 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 11:39:00,438 - WARNING - Invalid colon format: '[Al-An'am:83]' -> 'Al-An'am:83'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 11:39:00,440 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:83]' -> 'Al-An'am:83'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:83]' -> 'Al-An'am:83'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 11:39:00,440 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 11:39:00,440 - INFO - Normalized predicted verses: ['4:82', '4:83', '4:86', '4:87', '4:88']
2025-07-07 11:39:00,440 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 11:39:00,441 - INFO -   Response: 749 chars, Found 5 verses
2025-07-07 11:39:00,441 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 11:39:00,441 - INFO -   Matched verses: []
2025-07-07 11:39:01,443 - INFO - Processing question 5/881 (ID: 4) for qwen
2025-07-07 11:39:01,443 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 11:39:01,444 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:53:00,259 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 08:53:00,263 - INFO - Loaded 6236 verses for validation
2025-07-07 08:53:00,264 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 08:53:00,264 - INFO - Models to test: qwen
2025-07-07 08:53:00,264 - INFO - Questions limit: All
2025-07-07 08:53:00,264 - INFO - Using device: cuda
2025-07-07 08:53:00,471 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 08:53:00,472 - INFO - 
============================================================
2025-07-07 08:53:00,472 - INFO - Starting evaluation for QWEN
2025-07-07 08:53:00,472 - INFO - ============================================================
2025-07-07 08:53:00,472 - INFO - Starting benchmark for QWEN
2025-07-07 08:53:00,472 - INFO - Using device: cuda
2025-07-07 08:53:00,472 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 08:53:00,473 - INFO - Processing 881 questions for qwen (starting from 1)
2025-07-07 08:53:00,473 - INFO - Processing question 1/881 (ID: 0) for qwen
2025-07-07 08:53:00,473 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 08:53:00,473 - INFO - Loading model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:53:00,473 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 08:53:20,537 - INFO - Successfully loaded Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:53:36,945 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 08:53:36,945 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 08:53:36,945 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 08:53:36,945 - INFO - Normalized predicted verses: []
2025-07-07 08:53:36,945 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 08:53:36,945 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 08:53:36,945 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:53:36,945 - INFO -   Matched verses: []
2025-07-07 08:53:37,947 - INFO - Processing question 2/881 (ID: 1) for qwen
2025-07-07 08:53:37,947 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 08:53:37,947 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:53:52,388 - INFO - Extracted JSON from code block: 751 characters
2025-07-07 08:53:52,388 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 08:53:52,388 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:53:52,388 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 08:53:52,388 - WARNING - Invalid colon format: '[Yunus:10]' -> 'Yunus:10'
2025-07-07 08:53:52,388 - WARNING - Invalid colon format: '[Sad:11]' -> 'Sad:11'
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Fussilat:56]' -> 'Fussilat:56'
2025-07-07 08:53:52,389 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Yunus:10]' -> 'Yunus:10'
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Yunus:10]' -> 'Yunus:10'
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Sad:11]' -> 'Sad:11'
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Sad:11]' -> 'Sad:11'
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Fussilat:56]' -> 'Fussilat:56'
2025-07-07 08:53:52,389 - WARNING - Invalid colon format: '[Fussilat:56]' -> 'Fussilat:56'
2025-07-07 08:53:52,389 - INFO - Normalized predicted verses: ['10:10', '2:255', '38:11', '7:113']
2025-07-07 08:53:52,389 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 08:53:52,389 - INFO -   Response: 751 chars, Found 5 verses
2025-07-07 08:53:52,389 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:53:52,389 - INFO -   Matched verses: []
2025-07-07 08:53:53,390 - INFO - Processing question 3/881 (ID: 2) for qwen
2025-07-07 08:53:53,390 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 08:53:53,390 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:54:07,988 - INFO - Extracted JSON from code block: 762 characters
2025-07-07 08:54:07,988 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 08:54:07,988 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[At-Tawbah:101]' -> 'At-Tawbah:101'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 08:54:07,989 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[At-Tawbah:101]' -> 'At-Tawbah:101'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[At-Tawbah:101]' -> 'At-Tawbah:101'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 08:54:07,989 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 08:54:07,989 - INFO - Normalized predicted verses: ['2:255', '49:11', '5:10', '6:84', '9:101']
2025-07-07 08:54:07,989 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 08:54:07,989 - INFO -   Response: 762 chars, Found 5 verses
2025-07-07 08:54:07,989 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:54:07,989 - INFO -   Matched verses: []
2025-07-07 08:54:08,990 - INFO - Processing question 4/881 (ID: 3) for qwen
2025-07-07 08:54:08,990 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 08:54:08,990 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:54:23,328 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 08:54:23,328 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Tawbah:10]' -> 'Al-Tawbah:10'
2025-07-07 08:54:23,328 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Tawbah:10]' -> 'Al-Tawbah:10'
2025-07-07 08:54:23,328 - WARNING - Invalid colon format: '[Al-Tawbah:10]' -> 'Al-Tawbah:10'
2025-07-07 08:54:23,328 - INFO - Normalized predicted verses: ['2:255', '49:11', '4:89', '5:16', '9:10']
2025-07-07 08:54:23,328 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 08:54:23,328 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 08:54:23,328 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:54:23,328 - INFO -   Matched verses: []
2025-07-07 08:54:24,330 - INFO - Processing question 5/881 (ID: 4) for qwen
2025-07-07 08:54:24,330 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 08:54:24,330 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:54:38,780 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 08:54:38,781 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 08:54:38,781 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 08:54:38,781 - INFO - Normalized predicted verses: []
2025-07-07 08:54:38,781 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 08:54:38,781 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 08:54:38,781 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:54:38,781 - INFO -   Matched verses: []
2025-07-07 08:54:38,786 - INFO - Saved data to results/qwen_checkpoint_4.json
2025-07-07 08:54:38,786 - INFO - Checkpoint saved for qwen at question 5
2025-07-07 08:54:38,786 - INFO -   Progress: 5/881
2025-07-07 08:54:38,786 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:54:39,787 - INFO - Processing question 6/881 (ID: 5) for qwen
2025-07-07 08:54:39,787 - INFO -   Ground truth verses: ['6:43']
2025-07-07 08:54:39,787 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:54:54,257 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 08:54:54,257 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 08:54:54,257 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 08:54:54,257 - INFO - Normalized predicted verses: []
2025-07-07 08:54:54,257 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 08:54:54,257 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 08:54:54,257 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:54:54,258 - INFO -   Matched verses: []
2025-07-07 08:54:55,259 - INFO - Processing question 7/881 (ID: 6) for qwen
2025-07-07 08:54:55,259 - INFO -   Ground truth verses: ['6:43']
2025-07-07 08:54:55,259 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:55:09,666 - INFO - Extracted JSON from code block: 101 characters
2025-07-07 08:55:09,666 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 08:55:09,666 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 08:55:09,666 - INFO - Normalized predicted verses: []
2025-07-07 08:55:09,666 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 08:55:09,666 - INFO -   Response: 101 chars, Found 0 verses
2025-07-07 08:55:09,666 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:55:09,666 - INFO -   Matched verses: []
2025-07-07 08:55:10,668 - INFO - Processing question 8/881 (ID: 7) for qwen
2025-07-07 08:55:10,668 - INFO -   Ground truth verses: ['6:43']
2025-07-07 08:55:10,668 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:55:25,001 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:55:25,001 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:55:25,001 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:55:25,001 - INFO - Regex fallback created 0 detailed verses
2025-07-07 08:55:25,001 - INFO - Normalized predicted verses: []
2025-07-07 08:55:25,001 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 08:55:25,001 - INFO -   Response: 4845 chars, Found 0 verses
2025-07-07 08:55:25,001 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:55:25,001 - INFO -   Matched verses: []
2025-07-07 08:55:26,002 - INFO - Processing question 9/881 (ID: 8) for qwen
2025-07-07 08:55:26,003 - INFO -   Ground truth verses: ['6:43']
2025-07-07 08:55:26,003 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:55:40,481 - INFO - Extracted JSON from code block: 746 characters
2025-07-07 08:55:40,482 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 08:55:40,482 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 08:55:40,482 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 08:55:40,482 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 08:55:40,482 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 08:55:40,482 - WARNING - Could not normalize verse reference: '[Al-Furqan]'
2025-07-07 08:55:40,482 - WARNING - Unrecognized verse format: '[Al-Furqan]' -> 'Al-Furqan'
2025-07-07 08:55:40,482 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 08:55:40,482 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 08:55:40,482 - WARNING - Could not normalize verse reference: '[Al-Hujurat]'
2025-07-07 08:55:40,482 - WARNING - Unrecognized verse format: '[Al-Hujurat]' -> 'Al-Hujurat'
2025-07-07 08:55:40,482 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 08:55:40,482 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 08:55:40,482 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 08:55:40,482 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 08:55:40,482 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 08:55:40,482 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 08:55:40,482 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 08:55:40,482 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 08:55:40,482 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 08:55:40,482 - WARNING - Could not normalize verse reference: '[Al-Furqan]'
2025-07-07 08:55:40,482 - WARNING - Unrecognized verse format: '[Al-Furqan]' -> 'Al-Furqan'
2025-07-07 08:55:40,483 - WARNING - Could not normalize verse reference: '[Al-Furqan]'
2025-07-07 08:55:40,483 - WARNING - Unrecognized verse format: '[Al-Furqan]' -> 'Al-Furqan'
2025-07-07 08:55:40,483 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 08:55:40,483 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 08:55:40,483 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 08:55:40,483 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 08:55:40,483 - WARNING - Could not normalize verse reference: '[Al-Hujurat]'
2025-07-07 08:55:40,483 - WARNING - Unrecognized verse format: '[Al-Hujurat]' -> 'Al-Hujurat'
2025-07-07 08:55:40,483 - WARNING - Could not normalize verse reference: '[Al-Hujurat]'
2025-07-07 08:55:40,483 - WARNING - Unrecognized verse format: '[Al-Hujurat]' -> 'Al-Hujurat'
2025-07-07 08:55:40,483 - INFO - Normalized predicted verses: ['25:44', '2:255', '49:12', '5:68', '6:124']
2025-07-07 08:55:40,483 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 08:55:40,483 - INFO -   Response: 746 chars, Found 5 verses
2025-07-07 08:55:40,483 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:55:40,483 - INFO -   Matched verses: []
2025-07-07 08:55:41,484 - INFO - Processing question 10/881 (ID: 9) for qwen
2025-07-07 08:55:41,484 - INFO -   Ground truth verses: ['6:43']
2025-07-07 08:55:41,485 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:55:56,086 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 08:55:56,086 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 08:55:56,086 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 08:55:56,086 - INFO - Normalized predicted verses: []
2025-07-07 08:55:56,086 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 08:55:56,086 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 08:55:56,086 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:55:56,086 - INFO -   Matched verses: []
2025-07-07 08:55:56,092 - INFO - Saved data to results/qwen_checkpoint_9.json
2025-07-07 08:55:56,092 - INFO - Checkpoint saved for qwen at question 10
2025-07-07 08:55:56,092 - INFO -   Progress: 10/881
2025-07-07 08:55:56,092 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:55:57,093 - INFO - Processing question 11/881 (ID: 10) for qwen
2025-07-07 08:55:57,093 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 08:55:57,093 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:56:11,539 - INFO - Extracted JSON from code block: 754 characters
2025-07-07 08:56:11,539 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 08:56:11,539 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:56:11,539 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:56:11,539 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 08:56:11,539 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 08:56:11,539 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 08:56:11,539 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 08:56:11,539 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:56:11,539 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:56:11,539 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:56:11,539 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:56:11,540 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 08:56:11,540 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 08:56:11,540 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 08:56:11,540 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 08:56:11,540 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 08:56:11,540 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 08:56:11,540 - INFO - Normalized predicted verses: ['2:255', '6:84', '6:85', '6:86', '6:87']
2025-07-07 08:56:11,540 - INFO - Normalized ground truth verses: ['10:85', '60:5']
2025-07-07 08:56:11,540 - INFO -   Response: 754 chars, Found 5 verses
2025-07-07 08:56:11,540 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:56:11,540 - INFO -   Matched verses: []
2025-07-07 08:56:12,541 - INFO - Processing question 12/881 (ID: 11) for qwen
2025-07-07 08:56:12,541 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 08:56:12,541 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:56:27,029 - INFO - Extracted JSON from code block: 757 characters
2025-07-07 08:56:27,029 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 08:56:27,029 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:56:27,029 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:56:27,029 - WARNING - Invalid colon format: '[Al-Ma'idah:54]' -> 'Al-Ma'idah:54'
2025-07-07 08:56:27,029 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 08:56:27,029 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 08:56:27,029 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 08:56:27,030 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:56:27,030 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:56:27,030 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:56:27,030 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:56:27,030 - WARNING - Invalid colon format: '[Al-Ma'idah:54]' -> 'Al-Ma'idah:54'
2025-07-07 08:56:27,030 - WARNING - Invalid colon format: '[Al-Ma'idah:54]' -> 'Al-Ma'idah:54'
2025-07-07 08:56:27,030 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 08:56:27,030 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 08:56:27,030 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 08:56:27,030 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 08:56:27,030 - INFO - Normalized predicted verses: ['15:72', '2:255', '55:55', '5:54', '6:84']
2025-07-07 08:56:27,030 - INFO - Normalized ground truth verses: ['10:85', '60:5']
2025-07-07 08:56:27,030 - INFO -   Response: 757 chars, Found 5 verses
2025-07-07 08:56:27,030 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:56:27,030 - INFO -   Matched verses: []
2025-07-07 08:56:28,031 - INFO - Processing question 13/881 (ID: 12) for qwen
2025-07-07 08:56:28,031 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 08:56:28,031 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:56:42,566 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:56:42,567 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:56:42,567 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:56:42,567 - INFO - Regex fallback created 0 detailed verses
2025-07-07 08:56:42,567 - INFO - Normalized predicted verses: []
2025-07-07 08:56:42,567 - INFO - Normalized ground truth verses: ['10:85', '60:5']
2025-07-07 08:56:42,567 - INFO -   Response: 4825 chars, Found 0 verses
2025-07-07 08:56:42,567 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:56:42,567 - INFO -   Matched verses: []
2025-07-07 08:56:43,568 - INFO - Processing question 14/881 (ID: 13) for qwen
2025-07-07 08:56:43,568 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 08:56:43,568 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:56:57,675 - INFO - Extracted JSON from code block: 751 characters
2025-07-07 08:56:57,675 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Tawbah:10]' -> 'Al-Tawbah:10'
2025-07-07 08:56:57,675 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Tawbah:10]' -> 'Al-Tawbah:10'
2025-07-07 08:56:57,675 - WARNING - Invalid colon format: '[Al-Tawbah:10]' -> 'Al-Tawbah:10'
2025-07-07 08:56:57,675 - INFO - Normalized predicted verses: ['15:79', '57:5', '5:10', '6:84', '9:10']
2025-07-07 08:56:57,675 - INFO - Normalized ground truth verses: ['10:85', '60:5']
2025-07-07 08:56:57,676 - INFO -   Response: 751 chars, Found 5 verses
2025-07-07 08:56:57,676 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:56:57,676 - INFO -   Matched verses: []
2025-07-07 08:56:58,677 - INFO - Processing question 15/881 (ID: 14) for qwen
2025-07-07 08:56:58,677 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 08:56:58,677 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:57:13,014 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:57:13,014 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:57:13,014 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:57:13,014 - INFO - Regex fallback created 0 detailed verses
2025-07-07 08:57:13,014 - INFO - Normalized predicted verses: []
2025-07-07 08:57:13,014 - INFO - Normalized ground truth verses: ['10:85', '60:5']
2025-07-07 08:57:13,014 - INFO -   Response: 3980 chars, Found 0 verses
2025-07-07 08:57:13,014 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:57:13,014 - INFO -   Matched verses: []
2025-07-07 08:57:13,019 - INFO - Saved data to results/qwen_checkpoint_14.json
2025-07-07 08:57:13,019 - INFO - Checkpoint saved for qwen at question 15
2025-07-07 08:57:13,019 - INFO -   Progress: 15/881
2025-07-07 08:57:13,019 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:57:14,021 - INFO - Processing question 16/881 (ID: 15) for qwen
2025-07-07 08:57:14,021 - INFO -   Ground truth verses: ['52:44']
2025-07-07 08:57:14,021 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:57:28,466 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 08:57:28,466 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 08:57:28,467 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 08:57:28,467 - INFO - Normalized predicted verses: []
2025-07-07 08:57:28,467 - INFO - Normalized ground truth verses: ['52:44']
2025-07-07 08:57:28,467 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 08:57:28,467 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:57:28,467 - INFO -   Matched verses: []
2025-07-07 08:57:29,468 - INFO - Processing question 17/881 (ID: 16) for qwen
2025-07-07 08:57:29,468 - INFO -   Ground truth verses: ['52:44']
2025-07-07 08:57:29,468 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:57:43,812 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 08:57:43,812 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 08:57:43,812 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 08:57:43,813 - INFO - Normalized predicted verses: []
2025-07-07 08:57:43,813 - INFO - Normalized ground truth verses: ['52:44']
2025-07-07 08:57:43,813 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 08:57:43,813 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:57:43,813 - INFO -   Matched verses: []
2025-07-07 08:57:44,814 - INFO - Processing question 18/881 (ID: 17) for qwen
2025-07-07 08:57:44,814 - INFO -   Ground truth verses: ['52:44']
2025-07-07 08:57:44,814 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:57:59,093 - INFO - Extracted JSON from code block: 749 characters
2025-07-07 08:57:59,093 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 08:57:59,093 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:57:59,093 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 08:57:59,094 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 08:57:59,094 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 08:57:59,094 - INFO - Normalized predicted verses: ['6:84', '6:85', '6:86', '6:87', '6:88']
2025-07-07 08:57:59,094 - INFO - Normalized ground truth verses: ['52:44']
2025-07-07 08:57:59,094 - INFO -   Response: 749 chars, Found 5 verses
2025-07-07 08:57:59,094 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:57:59,094 - INFO -   Matched verses: []
2025-07-07 08:58:00,095 - INFO - Processing question 19/881 (ID: 18) for qwen
2025-07-07 08:58:00,095 - INFO -   Ground truth verses: ['52:44']
2025-07-07 08:58:00,095 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:58:14,526 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 08:58:14,526 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 08:58:14,526 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 08:58:14,526 - INFO - Regex fallback created 0 detailed verses
2025-07-07 08:58:14,526 - INFO - Normalized predicted verses: []
2025-07-07 08:58:14,526 - INFO - Normalized ground truth verses: ['52:44']
2025-07-07 08:58:14,526 - INFO -   Response: 4444 chars, Found 0 verses
2025-07-07 08:58:14,526 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:58:14,526 - INFO -   Matched verses: []
2025-07-07 08:58:15,527 - INFO - Processing question 20/881 (ID: 19) for qwen
2025-07-07 08:58:15,527 - INFO -   Ground truth verses: ['52:44']
2025-07-07 08:58:15,527 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:58:29,948 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:58:29,948 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:58:29,948 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 08:58:29,948 - INFO - Regex fallback created 0 detailed verses
2025-07-07 08:58:29,948 - INFO - Normalized predicted verses: []
2025-07-07 08:58:29,948 - INFO - Normalized ground truth verses: ['52:44']
2025-07-07 08:58:29,948 - INFO -   Response: 3940 chars, Found 0 verses
2025-07-07 08:58:29,948 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:58:29,948 - INFO -   Matched verses: []
2025-07-07 08:58:29,954 - INFO - Saved data to results/qwen_checkpoint_19.json
2025-07-07 08:58:29,954 - INFO - Checkpoint saved for qwen at question 20
2025-07-07 08:58:29,954 - INFO -   Progress: 20/881
2025-07-07 08:58:29,954 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:58:30,956 - INFO - Processing question 21/881 (ID: 20) for qwen
2025-07-07 08:58:30,956 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 08:58:30,956 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:58:45,295 - INFO - Extracted JSON from code block: 754 characters
2025-07-07 08:58:45,295 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 08:58:45,295 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:58:45,295 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:58:45,295 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 08:58:45,295 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 08:58:45,295 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 08:58:45,295 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 08:58:45,295 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:58:45,295 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 08:58:45,296 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:58:45,296 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 08:58:45,296 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 08:58:45,296 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 08:58:45,296 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 08:58:45,296 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 08:58:45,296 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 08:58:45,296 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 08:58:45,296 - INFO - Normalized predicted verses: ['15:10', '2:255', '57:5', '5:10', '6:84']
2025-07-07 08:58:45,296 - INFO - Normalized ground truth verses: ['101:10', '101:3', '104:5', '69:3', '74:27', '77:14', '82:17', '82:18', '83:19', '83:8', '86:2', '90:12', '97:2']
2025-07-07 08:58:45,296 - INFO -   Response: 754 chars, Found 5 verses
2025-07-07 08:58:45,296 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:58:45,296 - INFO -   Matched verses: []
2025-07-07 08:58:46,297 - INFO - Processing question 22/881 (ID: 21) for qwen
2025-07-07 08:58:46,297 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 08:58:46,297 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:59:00,633 - INFO - Extracted JSON from code block: 101 characters
2025-07-07 08:59:00,633 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 08:59:00,633 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 08:59:00,633 - INFO - Normalized predicted verses: []
2025-07-07 08:59:00,633 - INFO - Normalized ground truth verses: ['101:10', '101:3', '104:5', '69:3', '74:27', '77:14', '82:17', '82:18', '83:19', '83:8', '86:2', '90:12', '97:2']
2025-07-07 08:59:00,634 - INFO -   Response: 101 chars, Found 0 verses
2025-07-07 08:59:00,634 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:59:00,634 - INFO -   Matched verses: []
2025-07-07 08:59:01,635 - INFO - Processing question 23/881 (ID: 22) for qwen
2025-07-07 08:59:01,635 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 08:59:01,635 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:59:16,309 - WARNING - Found ```json but no closing ```, using content after ```json
2025-07-07 08:59:16,309 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 7 column 10 (char 677), falling back to regex parsing
2025-07-07 08:59:16,309 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 7 column 10 (char 677), falling back to regex parsing
2025-07-07 08:59:16,309 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 7 column 10 (char 677), falling back to regex parsing
2025-07-07 08:59:16,309 - INFO - Regex fallback created 0 detailed verses
2025-07-07 08:59:16,310 - INFO - Normalized predicted verses: []
2025-07-07 08:59:16,310 - INFO - Normalized ground truth verses: ['101:10', '101:3', '104:5', '69:3', '74:27', '77:14', '82:17', '82:18', '83:19', '83:8', '86:2', '90:12', '97:2']
2025-07-07 08:59:16,310 - INFO -   Response: 728 chars, Found 0 verses
2025-07-07 08:59:16,310 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:59:16,310 - INFO -   Matched verses: []
2025-07-07 08:59:17,311 - INFO - Processing question 24/881 (ID: 23) for qwen
2025-07-07 08:59:17,311 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 08:59:17,311 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:59:32,579 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 08:59:32,580 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 08:59:32,580 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 08:59:32,580 - INFO - Normalized predicted verses: []
2025-07-07 08:59:32,580 - INFO - Normalized ground truth verses: ['101:10', '101:3', '104:5', '69:3', '74:27', '77:14', '82:17', '82:18', '83:19', '83:8', '86:2', '90:12', '97:2']
2025-07-07 08:59:32,580 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 08:59:32,580 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:59:32,580 - INFO -   Matched verses: []
2025-07-07 08:59:33,581 - INFO - Processing question 25/881 (ID: 24) for qwen
2025-07-07 08:59:33,581 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 08:59:33,581 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 08:59:49,176 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 871), falling back to regex parsing
2025-07-07 08:59:49,176 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 871), falling back to regex parsing
2025-07-07 08:59:49,176 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 871), falling back to regex parsing
2025-07-07 08:59:49,177 - INFO - Regex fallback created 0 detailed verses
2025-07-07 08:59:49,177 - INFO - Normalized predicted verses: []
2025-07-07 08:59:49,177 - INFO - Normalized ground truth verses: ['101:10', '101:3', '104:5', '69:3', '74:27', '77:14', '82:17', '82:18', '83:19', '83:8', '86:2', '90:12', '97:2']
2025-07-07 08:59:49,177 - INFO -   Response: 4897 chars, Found 0 verses
2025-07-07 08:59:49,177 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:59:49,177 - INFO -   Matched verses: []
2025-07-07 08:59:49,183 - INFO - Saved data to results/qwen_checkpoint_24.json
2025-07-07 08:59:49,183 - INFO - Checkpoint saved for qwen at question 25
2025-07-07 08:59:49,183 - INFO -   Progress: 25/881
2025-07-07 08:59:49,183 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 08:59:50,184 - INFO - Processing question 26/881 (ID: 30) for qwen
2025-07-07 08:59:50,184 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 08:59:50,184 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:00:06,126 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:00:06,126 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:00:06,126 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:00:06,126 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:00:06,126 - INFO - Normalized predicted verses: []
2025-07-07 09:00:06,127 - INFO - Normalized ground truth verses: ['15:45', '26:134', '26:147', '26:57', '44:25', '44:52', '51:15']
2025-07-07 09:00:06,127 - INFO -   Response: 3947 chars, Found 0 verses
2025-07-07 09:00:06,127 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:00:06,127 - INFO -   Matched verses: []
2025-07-07 09:00:07,128 - INFO - Processing question 27/881 (ID: 31) for qwen
2025-07-07 09:00:07,128 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 09:00:07,128 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:00:22,982 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:00:22,982 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:00:22,982 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:00:22,982 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:00:22,983 - INFO - Normalized predicted verses: []
2025-07-07 09:00:22,983 - INFO - Normalized ground truth verses: ['15:45', '26:134', '26:147', '26:57', '44:25', '44:52', '51:15']
2025-07-07 09:00:22,983 - INFO -   Response: 4334 chars, Found 0 verses
2025-07-07 09:00:22,983 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:00:22,983 - INFO -   Matched verses: []
2025-07-07 09:00:23,984 - INFO - Processing question 28/881 (ID: 32) for qwen
2025-07-07 09:00:23,984 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 09:00:23,984 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:00:39,587 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:00:39,587 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:00:39,587 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:00:39,587 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:00:39,587 - INFO - Normalized predicted verses: []
2025-07-07 09:00:39,587 - INFO - Normalized ground truth verses: ['15:45', '26:134', '26:147', '26:57', '44:25', '44:52', '51:15']
2025-07-07 09:00:39,587 - INFO -   Response: 4198 chars, Found 0 verses
2025-07-07 09:00:39,587 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:00:39,587 - INFO -   Matched verses: []
2025-07-07 09:00:40,589 - INFO - Processing question 29/881 (ID: 33) for qwen
2025-07-07 09:00:40,589 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 09:00:40,589 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:00:56,325 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:00:56,325 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:00:56,326 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:00:56,326 - INFO - Normalized predicted verses: []
2025-07-07 09:00:56,326 - INFO - Normalized ground truth verses: ['15:45', '26:134', '26:147', '26:57', '44:25', '44:52', '51:15']
2025-07-07 09:00:56,326 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:00:56,326 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:00:56,326 - INFO -   Matched verses: []
2025-07-07 09:00:57,327 - INFO - Processing question 30/881 (ID: 34) for qwen
2025-07-07 09:00:57,327 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 09:00:57,327 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:01:13,156 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:01:13,156 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:01:13,156 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:01:13,156 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:01:13,156 - INFO - Normalized predicted verses: []
2025-07-07 09:01:13,156 - INFO - Normalized ground truth verses: ['15:45', '26:134', '26:147', '26:57', '44:25', '44:52', '51:15']
2025-07-07 09:01:13,156 - INFO -   Response: 3830 chars, Found 0 verses
2025-07-07 09:01:13,156 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:01:13,156 - INFO -   Matched verses: []
2025-07-07 09:01:13,163 - INFO - Saved data to results/qwen_checkpoint_29.json
2025-07-07 09:01:13,163 - INFO - Checkpoint saved for qwen at question 30
2025-07-07 09:01:13,163 - INFO -   Progress: 30/881
2025-07-07 09:01:13,163 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:01:14,165 - INFO - Processing question 31/881 (ID: 35) for qwen
2025-07-07 09:01:14,165 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 09:01:14,165 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:01:30,003 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 872), falling back to regex parsing
2025-07-07 09:01:30,003 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 872), falling back to regex parsing
2025-07-07 09:01:30,003 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 872), falling back to regex parsing
2025-07-07 09:01:30,003 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:01:30,003 - INFO - Normalized predicted verses: []
2025-07-07 09:01:30,003 - INFO - Normalized ground truth verses: ['12:40', '53:23', '7:71']
2025-07-07 09:01:30,003 - INFO -   Response: 4254 chars, Found 0 verses
2025-07-07 09:01:30,003 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:01:30,003 - INFO -   Matched verses: []
2025-07-07 09:01:31,004 - INFO - Processing question 32/881 (ID: 36) for qwen
2025-07-07 09:01:31,004 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 09:01:31,004 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:01:46,783 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:01:46,783 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:01:46,783 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:01:46,783 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:01:46,783 - INFO - Normalized predicted verses: []
2025-07-07 09:01:46,783 - INFO - Normalized ground truth verses: ['12:40', '53:23', '7:71']
2025-07-07 09:01:46,783 - INFO -   Response: 4584 chars, Found 0 verses
2025-07-07 09:01:46,783 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:01:46,783 - INFO -   Matched verses: []
2025-07-07 09:01:47,784 - INFO - Processing question 33/881 (ID: 37) for qwen
2025-07-07 09:01:47,785 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 09:01:47,785 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:02:03,581 - INFO - Extracted JSON from code block: 757 characters
2025-07-07 09:02:03,581 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Tawbah:11]' -> 'Al-Tawbah:11'
2025-07-07 09:02:03,581 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Tawbah:11]' -> 'Al-Tawbah:11'
2025-07-07 09:02:03,581 - WARNING - Invalid colon format: '[Al-Tawbah:11]' -> 'Al-Tawbah:11'
2025-07-07 09:02:03,581 - INFO - Normalized predicted verses: ['15:79', '2:255', '5:16', '6:84', '9:11']
2025-07-07 09:02:03,581 - INFO - Normalized ground truth verses: ['12:40', '53:23', '7:71']
2025-07-07 09:02:03,581 - INFO -   Response: 757 chars, Found 5 verses
2025-07-07 09:02:03,581 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:02:03,581 - INFO -   Matched verses: []
2025-07-07 09:02:04,583 - INFO - Processing question 34/881 (ID: 38) for qwen
2025-07-07 09:02:04,583 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 09:02:04,583 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:02:20,221 - INFO - Extracted JSON from code block: 752 characters
2025-07-07 09:02:20,221 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Qamar:1]' -> 'Al-Qamar:1'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Zumar:12]' -> 'Al-Zumar:12'
2025-07-07 09:02:20,221 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Qamar:1]' -> 'Al-Qamar:1'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Qamar:1]' -> 'Al-Qamar:1'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Zumar:12]' -> 'Al-Zumar:12'
2025-07-07 09:02:20,221 - WARNING - Invalid colon format: '[Al-Zumar:12]' -> 'Al-Zumar:12'
2025-07-07 09:02:20,221 - INFO - Normalized predicted verses: ['39:12', '49:11', '55:1', '5:5', '6:84']
2025-07-07 09:02:20,221 - INFO - Normalized ground truth verses: ['12:40', '53:23', '7:71']
2025-07-07 09:02:20,221 - INFO -   Response: 752 chars, Found 5 verses
2025-07-07 09:02:20,221 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:02:20,221 - INFO -   Matched verses: []
2025-07-07 09:02:21,222 - INFO - Processing question 35/881 (ID: 39) for qwen
2025-07-07 09:02:21,223 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 09:02:21,223 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:02:37,081 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:02:37,081 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:02:37,081 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:02:37,081 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:02:37,081 - INFO - Normalized predicted verses: []
2025-07-07 09:02:37,081 - INFO - Normalized ground truth verses: ['12:40', '53:23', '7:71']
2025-07-07 09:02:37,081 - INFO -   Response: 4967 chars, Found 0 verses
2025-07-07 09:02:37,082 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:02:37,082 - INFO -   Matched verses: []
2025-07-07 09:02:37,089 - INFO - Saved data to results/qwen_checkpoint_34.json
2025-07-07 09:02:37,090 - INFO - Checkpoint saved for qwen at question 35
2025-07-07 09:02:37,090 - INFO -   Progress: 35/881
2025-07-07 09:02:37,090 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:02:38,091 - INFO - Processing question 36/881 (ID: 40) for qwen
2025-07-07 09:02:38,091 - INFO -   Ground truth verses: ['73:11']
2025-07-07 09:02:38,091 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:02:53,834 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:02:53,834 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:02:53,834 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:02:53,834 - INFO - Normalized predicted verses: []
2025-07-07 09:02:53,834 - INFO - Normalized ground truth verses: ['73:11']
2025-07-07 09:02:53,834 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:02:53,834 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:02:53,834 - INFO -   Matched verses: []
2025-07-07 09:02:54,835 - INFO - Processing question 37/881 (ID: 41) for qwen
2025-07-07 09:02:54,836 - INFO -   Ground truth verses: ['73:11']
2025-07-07 09:02:54,836 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:03:10,679 - INFO - Extracted JSON from code block: 754 characters
2025-07-07 09:03:10,679 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:03:10,679 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:94]' -> 'Al-An'am:94'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:95]' -> 'Al-An'am:95'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:96]' -> 'Al-An'am:96'
2025-07-07 09:03:10,680 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:94]' -> 'Al-An'am:94'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:94]' -> 'Al-An'am:94'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:95]' -> 'Al-An'am:95'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:95]' -> 'Al-An'am:95'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:96]' -> 'Al-An'am:96'
2025-07-07 09:03:10,680 - WARNING - Invalid colon format: '[Al-An'am:96]' -> 'Al-An'am:96'
2025-07-07 09:03:10,680 - INFO - Normalized predicted verses: ['2:255', '6:89', '6:94', '6:95', '6:96']
2025-07-07 09:03:10,680 - INFO - Normalized ground truth verses: ['73:11']
2025-07-07 09:03:10,680 - INFO -   Response: 754 chars, Found 5 verses
2025-07-07 09:03:10,680 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:03:10,680 - INFO -   Matched verses: []
2025-07-07 09:03:11,681 - INFO - Processing question 38/881 (ID: 42) for qwen
2025-07-07 09:03:11,681 - INFO -   Ground truth verses: ['73:11']
2025-07-07 09:03:11,681 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:03:27,542 - INFO - Extracted JSON from code block: 759 characters
2025-07-07 09:03:27,543 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[An-Nisa:146]' -> 'An-Nisa:146'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Al-Ankabut:12]' -> 'Al-Ankabut:12'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Ar-Rum:19]' -> 'Ar-Rum:19'
2025-07-07 09:03:27,543 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[An-Nisa:146]' -> 'An-Nisa:146'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[An-Nisa:146]' -> 'An-Nisa:146'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Al-Ankabut:12]' -> 'Al-Ankabut:12'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Al-Ankabut:12]' -> 'Al-Ankabut:12'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Ar-Rum:19]' -> 'Ar-Rum:19'
2025-07-07 09:03:27,543 - WARNING - Invalid colon format: '[Ar-Rum:19]' -> 'Ar-Rum:19'
2025-07-07 09:03:27,543 - INFO - Normalized predicted verses: ['29:12', '2:255', '30:19', '4:146', '7:113']
2025-07-07 09:03:27,543 - INFO - Normalized ground truth verses: ['73:11']
2025-07-07 09:03:27,543 - INFO -   Response: 759 chars, Found 5 verses
2025-07-07 09:03:27,543 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:03:27,543 - INFO -   Matched verses: []
2025-07-07 09:03:28,544 - INFO - Processing question 39/881 (ID: 43) for qwen
2025-07-07 09:03:28,545 - INFO -   Ground truth verses: ['73:11']
2025-07-07 09:03:28,545 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:03:44,318 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:03:44,318 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:03:44,318 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:03:44,318 - INFO - Normalized predicted verses: []
2025-07-07 09:03:44,318 - INFO - Normalized ground truth verses: ['73:11']
2025-07-07 09:03:44,318 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:03:44,318 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:03:44,318 - INFO -   Matched verses: []
2025-07-07 09:03:45,319 - INFO - Processing question 40/881 (ID: 44) for qwen
2025-07-07 09:03:45,319 - INFO -   Ground truth verses: ['73:11']
2025-07-07 09:03:45,319 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:04:01,154 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 09:04:01,154 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:04:01,154 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:04:01,154 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:04:01,154 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:04:01,154 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:04:01,155 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:04:01,155 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:04:01,155 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:04:01,155 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:04:01,155 - INFO - Normalized ground truth verses: ['73:11']
2025-07-07 09:04:01,155 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 09:04:01,155 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:04:01,155 - INFO -   Matched verses: []
2025-07-07 09:04:01,163 - INFO - Saved data to results/qwen_checkpoint_39.json
2025-07-07 09:04:01,163 - INFO - Checkpoint saved for qwen at question 40
2025-07-07 09:04:01,163 - INFO -   Progress: 40/881
2025-07-07 09:04:01,163 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:04:02,164 - INFO - Processing question 41/881 (ID: 45) for qwen
2025-07-07 09:04:02,164 - INFO -   Ground truth verses: ['52:4']
2025-07-07 09:04:02,164 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:04:17,871 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:04:17,871 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:04:17,871 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:04:17,871 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:04:17,872 - INFO - Normalized predicted verses: []
2025-07-07 09:04:17,872 - INFO - Normalized ground truth verses: ['52:4']
2025-07-07 09:04:17,872 - INFO -   Response: 4549 chars, Found 0 verses
2025-07-07 09:04:17,872 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:04:17,872 - INFO -   Matched verses: []
2025-07-07 09:04:18,873 - INFO - Processing question 42/881 (ID: 46) for qwen
2025-07-07 09:04:18,873 - INFO -   Ground truth verses: ['52:4']
2025-07-07 09:04:18,873 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:04:34,391 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:04:34,391 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:04:34,391 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:04:34,391 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:04:34,391 - INFO - Normalized predicted verses: []
2025-07-07 09:04:34,391 - INFO - Normalized ground truth verses: ['52:4']
2025-07-07 09:04:34,391 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 09:04:34,391 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:04:34,391 - INFO -   Matched verses: []
2025-07-07 09:04:35,392 - INFO - Processing question 43/881 (ID: 47) for qwen
2025-07-07 09:04:35,392 - INFO -   Ground truth verses: ['52:4']
2025-07-07 09:04:35,392 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:04:51,119 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:04:51,119 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:04:51,119 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:04:51,119 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:04:51,119 - INFO - Normalized predicted verses: []
2025-07-07 09:04:51,119 - INFO - Normalized ground truth verses: ['52:4']
2025-07-07 09:04:51,119 - INFO -   Response: 5197 chars, Found 0 verses
2025-07-07 09:04:51,119 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:04:51,120 - INFO -   Matched verses: []
2025-07-07 09:04:52,121 - INFO - Processing question 44/881 (ID: 48) for qwen
2025-07-07 09:04:52,121 - INFO -   Ground truth verses: ['52:4']
2025-07-07 09:04:52,121 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:05:07,871 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:05:07,871 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:05:07,871 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:05:07,871 - INFO - Normalized predicted verses: []
2025-07-07 09:05:07,871 - INFO - Normalized ground truth verses: ['52:4']
2025-07-07 09:05:07,871 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:05:07,871 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:05:07,871 - INFO -   Matched verses: []
2025-07-07 09:05:08,873 - INFO - Processing question 45/881 (ID: 49) for qwen
2025-07-07 09:05:08,873 - INFO -   Ground truth verses: ['52:4']
2025-07-07 09:05:08,873 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:05:24,820 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:05:24,820 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:05:24,820 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:05:24,820 - INFO - Normalized predicted verses: []
2025-07-07 09:05:24,820 - INFO - Normalized ground truth verses: ['52:4']
2025-07-07 09:05:24,820 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:05:24,820 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:05:24,820 - INFO -   Matched verses: []
2025-07-07 09:05:24,828 - INFO - Saved data to results/qwen_checkpoint_44.json
2025-07-07 09:05:24,828 - INFO - Checkpoint saved for qwen at question 45
2025-07-07 09:05:24,828 - INFO -   Progress: 45/881
2025-07-07 09:05:24,828 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:05:25,830 - INFO - Processing question 46/881 (ID: 50) for qwen
2025-07-07 09:05:25,830 - INFO -   Ground truth verses: ['36:29', '36:32', '36:49', '38:15', '36:53']
2025-07-07 09:05:25,830 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:05:41,469 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:05:41,469 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:05:41,469 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:05:41,469 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:05:41,469 - INFO - Normalized predicted verses: []
2025-07-07 09:05:41,469 - INFO - Normalized ground truth verses: ['36:29', '36:32', '36:49', '36:53', '38:15']
2025-07-07 09:05:41,469 - INFO -   Response: 5190 chars, Found 0 verses
2025-07-07 09:05:41,469 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:05:41,469 - INFO -   Matched verses: []
2025-07-07 09:05:42,470 - INFO - Processing question 47/881 (ID: 51) for qwen
2025-07-07 09:05:42,470 - INFO -   Ground truth verses: ['36:29', '36:32', '36:49', '38:15', '36:53']
2025-07-07 09:05:42,470 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:05:58,158 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:05:58,158 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:05:58,158 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:05:58,158 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:05:58,158 - INFO - Normalized predicted verses: []
2025-07-07 09:05:58,158 - INFO - Normalized ground truth verses: ['36:29', '36:32', '36:49', '36:53', '38:15']
2025-07-07 09:05:58,158 - INFO -   Response: 4159 chars, Found 0 verses
2025-07-07 09:05:58,158 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:05:58,158 - INFO -   Matched verses: []
2025-07-07 09:05:59,159 - INFO - Processing question 48/881 (ID: 52) for qwen
2025-07-07 09:05:59,159 - INFO -   Ground truth verses: ['36:29', '36:32', '36:49', '38:15', '36:53']
2025-07-07 09:05:59,159 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:06:14,886 - INFO - Extracted JSON from code block: 757 characters
2025-07-07 09:06:14,886 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-An'am:94]' -> 'Al-An'am:94'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Hijr:70]' -> 'Al-Hijr:70'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Qamar:56]' -> 'Al-Qamar:56'
2025-07-07 09:06:14,887 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-An'am:94]' -> 'Al-An'am:94'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-An'am:94]' -> 'Al-An'am:94'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Hijr:70]' -> 'Al-Hijr:70'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Hijr:70]' -> 'Al-Hijr:70'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Qamar:56]' -> 'Al-Qamar:56'
2025-07-07 09:06:14,887 - WARNING - Invalid colon format: '[Al-Qamar:56]' -> 'Al-Qamar:56'
2025-07-07 09:06:14,887 - INFO - Normalized predicted verses: ['15:70', '2:255', '5:11', '6:94']
2025-07-07 09:06:14,887 - INFO - Normalized ground truth verses: ['36:29', '36:32', '36:49', '36:53', '38:15']
2025-07-07 09:06:14,887 - INFO -   Response: 757 chars, Found 5 verses
2025-07-07 09:06:14,887 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:06:14,887 - INFO -   Matched verses: []
2025-07-07 09:06:15,888 - INFO - Processing question 49/881 (ID: 53) for qwen
2025-07-07 09:06:15,889 - INFO -   Ground truth verses: ['36:29', '36:32', '36:49', '38:15', '36:53']
2025-07-07 09:06:15,889 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:06:31,739 - INFO - Extracted JSON from code block: 724 characters
2025-07-07 09:06:31,739 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[2]' -> '2'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[6]' -> '6'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[5]' -> '5'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Hijr]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Hijr]' -> 'Al-Hijr'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[15]' -> '15'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Qamar]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Qamar]' -> 'Al-Qamar'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[54]' -> '54'
2025-07-07 09:06:31,740 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Hijr]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Hijr]' -> 'Al-Hijr'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Hijr]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Hijr]' -> 'Al-Hijr'
2025-07-07 09:06:31,740 - WARNING - Could not normalize verse reference: '[Al-Qamar]'
2025-07-07 09:06:31,740 - WARNING - Unrecognized verse format: '[Al-Qamar]' -> 'Al-Qamar'
2025-07-07 09:06:31,741 - WARNING - Could not normalize verse reference: '[Al-Qamar]'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[Al-Qamar]' -> 'Al-Qamar'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[2]' -> '2'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[2]' -> '2'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[6]' -> '6'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[6]' -> '6'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[5]' -> '5'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[5]' -> '5'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[15]' -> '15'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[15]' -> '15'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[54]' -> '54'
2025-07-07 09:06:31,741 - WARNING - Unrecognized verse format: '[54]' -> '54'
2025-07-07 09:06:31,741 - INFO - Normalized predicted verses: []
2025-07-07 09:06:31,741 - INFO - Normalized ground truth verses: ['36:29', '36:32', '36:49', '36:53', '38:15']
2025-07-07 09:06:31,741 - INFO -   Response: 724 chars, Found 5 verses
2025-07-07 09:06:31,741 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:06:31,741 - INFO -   Matched verses: []
2025-07-07 09:06:32,742 - INFO - Processing question 50/881 (ID: 54) for qwen
2025-07-07 09:06:32,742 - INFO -   Ground truth verses: ['36:29', '36:32', '36:49', '38:15', '36:53']
2025-07-07 09:06:32,742 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:06:48,529 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:06:48,529 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:06:48,529 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:06:48,529 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:06:48,529 - INFO - Normalized predicted verses: []
2025-07-07 09:06:48,529 - INFO - Normalized ground truth verses: ['36:29', '36:32', '36:49', '36:53', '38:15']
2025-07-07 09:06:48,529 - INFO -   Response: 4582 chars, Found 0 verses
2025-07-07 09:06:48,529 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:06:48,529 - INFO -   Matched verses: []
2025-07-07 09:06:48,538 - INFO - Saved data to results/qwen_checkpoint_49.json
2025-07-07 09:06:48,538 - INFO - Checkpoint saved for qwen at question 50
2025-07-07 09:06:48,538 - INFO -   Progress: 50/881
2025-07-07 09:06:48,538 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:06:49,539 - INFO - Processing question 51/881 (ID: 55) for qwen
2025-07-07 09:06:49,539 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 09:06:49,539 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:07:05,310 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:07:05,310 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:07:05,310 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:07:05,310 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:07:05,310 - INFO - Normalized predicted verses: []
2025-07-07 09:07:05,310 - INFO - Normalized ground truth verses: ['107:5', '51:11']
2025-07-07 09:07:05,310 - INFO -   Response: 4066 chars, Found 0 verses
2025-07-07 09:07:05,310 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:07:05,310 - INFO -   Matched verses: []
2025-07-07 09:07:06,312 - INFO - Processing question 52/881 (ID: 56) for qwen
2025-07-07 09:07:06,312 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 09:07:06,312 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:07:21,928 - INFO - Extracted JSON from code block: 756 characters
2025-07-07 09:07:21,928 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:07:21,928 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:07:21,929 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:07:21,929 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:07:21,929 - INFO - Normalized predicted verses: ['15:10', '25:56', '50:12', '5:104', '6:84']
2025-07-07 09:07:21,929 - INFO - Normalized ground truth verses: ['107:5', '51:11']
2025-07-07 09:07:21,929 - INFO -   Response: 756 chars, Found 5 verses
2025-07-07 09:07:21,929 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:07:21,929 - INFO -   Matched verses: []
2025-07-07 09:07:22,930 - INFO - Processing question 53/881 (ID: 57) for qwen
2025-07-07 09:07:22,930 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 09:07:22,930 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:07:38,745 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 812), falling back to regex parsing
2025-07-07 09:07:38,745 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 812), falling back to regex parsing
2025-07-07 09:07:38,745 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 812), falling back to regex parsing
2025-07-07 09:07:38,745 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:07:38,745 - INFO - Normalized predicted verses: []
2025-07-07 09:07:38,745 - INFO - Normalized ground truth verses: ['107:5', '51:11']
2025-07-07 09:07:38,745 - INFO -   Response: 3166 chars, Found 0 verses
2025-07-07 09:07:38,745 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:07:38,745 - INFO -   Matched verses: []
2025-07-07 09:07:39,747 - INFO - Processing question 54/881 (ID: 58) for qwen
2025-07-07 09:07:39,747 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 09:07:39,747 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:07:55,524 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:07:55,524 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:07:55,524 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:07:55,524 - INFO - Normalized predicted verses: []
2025-07-07 09:07:55,524 - INFO - Normalized ground truth verses: ['107:5', '51:11']
2025-07-07 09:07:55,525 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:07:55,525 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:07:55,525 - INFO -   Matched verses: []
2025-07-07 09:07:56,526 - INFO - Processing question 55/881 (ID: 59) for qwen
2025-07-07 09:07:56,526 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 09:07:56,526 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:08:12,227 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:08:12,227 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:08:12,227 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:08:12,227 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:08:12,228 - INFO - Normalized predicted verses: []
2025-07-07 09:08:12,228 - INFO - Normalized ground truth verses: ['107:5', '51:11']
2025-07-07 09:08:12,228 - INFO -   Response: 4515 chars, Found 0 verses
2025-07-07 09:08:12,228 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:08:12,228 - INFO -   Matched verses: []
2025-07-07 09:08:12,236 - INFO - Saved data to results/qwen_checkpoint_54.json
2025-07-07 09:08:12,236 - INFO - Checkpoint saved for qwen at question 55
2025-07-07 09:08:12,236 - INFO -   Progress: 55/881
2025-07-07 09:08:12,236 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:08:13,237 - INFO - Processing question 56/881 (ID: 60) for qwen
2025-07-07 09:08:13,238 - INFO -   Ground truth verses: ['28:65', '5:109']
2025-07-07 09:08:13,238 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:08:29,097 - INFO - Extracted JSON from code block: 750 characters
2025-07-07 09:08:29,098 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Ma'idah:6]' -> 'Al-Ma'idah:6'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:08:29,098 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Ma'idah:6]' -> 'Al-Ma'idah:6'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Ma'idah:6]' -> 'Al-Ma'idah:6'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:08:29,098 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:08:29,098 - INFO - Normalized predicted verses: ['15:72', '25:56', '4:89', '57:5', '5:6']
2025-07-07 09:08:29,098 - INFO - Normalized ground truth verses: ['28:65', '5:109']
2025-07-07 09:08:29,098 - INFO -   Response: 750 chars, Found 5 verses
2025-07-07 09:08:29,098 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:08:29,098 - INFO -   Matched verses: []
2025-07-07 09:08:30,099 - INFO - Processing question 57/881 (ID: 61) for qwen
2025-07-07 09:08:30,100 - INFO -   Ground truth verses: ['28:65', '5:109']
2025-07-07 09:08:30,100 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:08:45,935 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-An'am:94]' -> 'Al-An'am:94'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-Ma'idah:55]' -> 'Al-Ma'idah:55'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:08:45,935 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-An'am:94]' -> 'Al-An'am:94'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-An'am:94]' -> 'Al-An'am:94'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-Ma'idah:55]' -> 'Al-Ma'idah:55'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-Ma'idah:55]' -> 'Al-Ma'idah:55'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:08:45,935 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:08:45,935 - INFO - Normalized predicted verses: ['15:72', '2:255', '5:55', '6:89', '6:94']
2025-07-07 09:08:45,935 - INFO - Normalized ground truth verses: ['28:65', '5:109']
2025-07-07 09:08:45,935 - INFO -   Response: 4800 chars, Found 5 verses
2025-07-07 09:08:45,935 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:08:45,935 - INFO -   Matched verses: []
2025-07-07 09:08:46,937 - INFO - Processing question 58/881 (ID: 62) for qwen
2025-07-07 09:08:46,937 - INFO -   Ground truth verses: ['28:65', '5:109']
2025-07-07 09:08:46,937 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:09:02,816 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 874), falling back to regex parsing
2025-07-07 09:09:02,816 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 874), falling back to regex parsing
2025-07-07 09:09:02,816 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 874), falling back to regex parsing
2025-07-07 09:09:02,816 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:09:02,816 - INFO - Normalized predicted verses: []
2025-07-07 09:09:02,816 - INFO - Normalized ground truth verses: ['28:65', '5:109']
2025-07-07 09:09:02,816 - INFO -   Response: 4060 chars, Found 0 verses
2025-07-07 09:09:02,816 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:09:02,816 - INFO -   Matched verses: []
2025-07-07 09:09:03,818 - INFO - Processing question 59/881 (ID: 63) for qwen
2025-07-07 09:09:03,818 - INFO -   Ground truth verses: ['28:65', '5:109']
2025-07-07 09:09:03,818 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:09:19,623 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:09:19,623 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:09:19,623 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:09:19,623 - INFO - Normalized predicted verses: []
2025-07-07 09:09:19,623 - INFO - Normalized ground truth verses: ['28:65', '5:109']
2025-07-07 09:09:19,623 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:09:19,623 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:09:19,623 - INFO -   Matched verses: []
2025-07-07 09:09:20,625 - INFO - Processing question 60/881 (ID: 64) for qwen
2025-07-07 09:09:20,625 - INFO -   Ground truth verses: ['28:65', '5:109']
2025-07-07 09:09:20,625 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:09:36,487 - INFO - Extracted JSON from code block: 757 characters
2025-07-07 09:09:36,487 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Ma'idah:12]' -> 'Al-Ma'idah:12'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Hijr:56]' -> 'Al-Hijr:56'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 09:09:36,487 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Ma'idah:12]' -> 'Al-Ma'idah:12'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Ma'idah:12]' -> 'Al-Ma'idah:12'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Hijr:56]' -> 'Al-Hijr:56'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Hijr:56]' -> 'Al-Hijr:56'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 09:09:36,487 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 09:09:36,487 - INFO - Normalized predicted verses: ['15:56', '2:255', '4:86', '57:12', '5:12']
2025-07-07 09:09:36,487 - INFO - Normalized ground truth verses: ['28:65', '5:109']
2025-07-07 09:09:36,487 - INFO -   Response: 757 chars, Found 5 verses
2025-07-07 09:09:36,487 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:09:36,487 - INFO -   Matched verses: []
2025-07-07 09:09:36,497 - INFO - Saved data to results/qwen_checkpoint_59.json
2025-07-07 09:09:36,497 - INFO - Checkpoint saved for qwen at question 60
2025-07-07 09:09:36,497 - INFO -   Progress: 60/881
2025-07-07 09:09:36,497 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:09:37,498 - INFO - Processing question 61/881 (ID: 65) for qwen
2025-07-07 09:09:37,498 - INFO -   Ground truth verses: ['9:6']
2025-07-07 09:09:37,499 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:09:53,277 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:09:53,277 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:09:53,277 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:09:53,277 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:09:53,278 - INFO - Normalized predicted verses: []
2025-07-07 09:09:53,278 - INFO - Normalized ground truth verses: ['9:6']
2025-07-07 09:09:53,278 - INFO -   Response: 3786 chars, Found 0 verses
2025-07-07 09:09:53,278 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:09:53,278 - INFO -   Matched verses: []
2025-07-07 09:09:54,279 - INFO - Processing question 62/881 (ID: 66) for qwen
2025-07-07 09:09:54,279 - INFO -   Ground truth verses: ['9:6']
2025-07-07 09:09:54,279 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:10:10,075 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Hujurat:14]' -> 'Al-Hujurat:14'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Tawbah:9]' -> 'Al-Tawbah:9'
2025-07-07 09:10:10,075 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Hujurat:14]' -> 'Al-Hujurat:14'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Hujurat:14]' -> 'Al-Hujurat:14'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Tawbah:9]' -> 'Al-Tawbah:9'
2025-07-07 09:10:10,075 - WARNING - Invalid colon format: '[Al-Tawbah:9]' -> 'Al-Tawbah:9'
2025-07-07 09:10:10,076 - INFO - Normalized predicted verses: ['2:255', '50:14', '5:5', '6:89', '9:9']
2025-07-07 09:10:10,076 - INFO - Normalized ground truth verses: ['9:6']
2025-07-07 09:10:10,076 - INFO -   Response: 4456 chars, Found 5 verses
2025-07-07 09:10:10,076 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:10:10,076 - INFO -   Matched verses: []
2025-07-07 09:10:11,077 - INFO - Processing question 63/881 (ID: 68) for qwen
2025-07-07 09:10:11,077 - INFO -   Ground truth verses: ['9:6']
2025-07-07 09:10:11,077 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:10:26,990 - INFO - Extracted JSON from code block: 737 characters
2025-07-07 09:10:26,990 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:10:26,990 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:10:26,990 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:10:26,991 - WARNING - Could not normalize verse reference: '[Al-Anfal]'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Al-Anfal]' -> 'Al-Anfal'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Tawbah]' -> 'Tawbah'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Fussilat]' -> 'Fussilat'
2025-07-07 09:10:26,991 - WARNING - Could not normalize verse reference: '[At-Takwir]'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[At-Takwir]' -> 'At-Takwir'
2025-07-07 09:10:26,991 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:10:26,991 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:10:26,991 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:10:26,991 - WARNING - Could not normalize verse reference: '[Al-Anfal]'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Al-Anfal]' -> 'Al-Anfal'
2025-07-07 09:10:26,991 - WARNING - Could not normalize verse reference: '[Al-Anfal]'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Al-Anfal]' -> 'Al-Anfal'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Tawbah]' -> 'Tawbah'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Tawbah]' -> 'Tawbah'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Fussilat]' -> 'Fussilat'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[Fussilat]' -> 'Fussilat'
2025-07-07 09:10:26,991 - WARNING - Could not normalize verse reference: '[At-Takwir]'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[At-Takwir]' -> 'At-Takwir'
2025-07-07 09:10:26,991 - WARNING - Could not normalize verse reference: '[At-Takwir]'
2025-07-07 09:10:26,991 - WARNING - Unrecognized verse format: '[At-Takwir]' -> 'At-Takwir'
2025-07-07 09:10:26,991 - INFO - Normalized predicted verses: ['2:255', '42:26', '81:8', '8:64', '9:6']
2025-07-07 09:10:26,991 - INFO - Normalized ground truth verses: ['9:6']
2025-07-07 09:10:26,991 - INFO -   Response: 737 chars, Found 5 verses
2025-07-07 09:10:26,991 - INFO -   Similarity metrics - F1: 0.333, Precision: 0.200, Recall: 1.000
2025-07-07 09:10:26,991 - INFO -   Matched verses: ['9:6']
2025-07-07 09:10:27,993 - INFO - Processing question 64/881 (ID: 69) for qwen
2025-07-07 09:10:27,993 - INFO -   Ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 09:10:27,993 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:10:43,788 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 816), falling back to regex parsing
2025-07-07 09:10:43,788 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 816), falling back to regex parsing
2025-07-07 09:10:43,788 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 816), falling back to regex parsing
2025-07-07 09:10:43,788 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:10:43,788 - INFO - Normalized predicted verses: []
2025-07-07 09:10:43,788 - INFO - Normalized ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 09:10:43,788 - INFO -   Response: 4129 chars, Found 0 verses
2025-07-07 09:10:43,788 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:10:43,788 - INFO -   Matched verses: []
2025-07-07 09:10:44,789 - INFO - Processing question 65/881 (ID: 70) for qwen
2025-07-07 09:10:44,789 - INFO -   Ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 09:10:44,789 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:11:00,445 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:11:00,445 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:11:00,445 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:11:00,445 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:11:00,445 - INFO - Normalized predicted verses: []
2025-07-07 09:11:00,445 - INFO - Normalized ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 09:11:00,445 - INFO -   Response: 4191 chars, Found 0 verses
2025-07-07 09:11:00,445 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:11:00,445 - INFO -   Matched verses: []
2025-07-07 09:11:00,455 - INFO - Saved data to results/qwen_checkpoint_64.json
2025-07-07 09:11:00,455 - INFO - Checkpoint saved for qwen at question 65
2025-07-07 09:11:00,455 - INFO -   Progress: 65/881
2025-07-07 09:11:00,455 - INFO -   Running averages - F1: 0.005, Precision: 0.003, Recall: 0.015
2025-07-07 09:11:01,456 - INFO - Processing question 66/881 (ID: 71) for qwen
2025-07-07 09:11:01,457 - INFO -   Ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 09:11:01,457 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:11:17,388 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:11:17,388 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:11:17,389 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:11:17,389 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:11:17,389 - INFO - Normalized predicted verses: []
2025-07-07 09:11:17,389 - INFO - Normalized ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 09:11:17,389 - INFO -   Response: 4892 chars, Found 0 verses
2025-07-07 09:11:17,389 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:11:17,389 - INFO -   Matched verses: []
2025-07-07 09:11:18,390 - INFO - Processing question 67/881 (ID: 72) for qwen
2025-07-07 09:11:18,390 - INFO -   Ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 09:11:18,390 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:11:34,052 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:11:34,052 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:11:34,052 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:11:34,052 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:11:34,052 - INFO - Normalized predicted verses: []
2025-07-07 09:11:34,052 - INFO - Normalized ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 09:11:34,052 - INFO -   Response: 5654 chars, Found 0 verses
2025-07-07 09:11:34,053 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:11:34,053 - INFO -   Matched verses: []
2025-07-07 09:11:35,054 - INFO - Processing question 68/881 (ID: 73) for qwen
2025-07-07 09:11:35,054 - INFO -   Ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 09:11:35,054 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:11:50,934 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 877), falling back to regex parsing
2025-07-07 09:11:50,934 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 877), falling back to regex parsing
2025-07-07 09:11:50,934 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 877), falling back to regex parsing
2025-07-07 09:11:50,934 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:11:50,934 - INFO - Normalized predicted verses: []
2025-07-07 09:11:50,934 - INFO - Normalized ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 09:11:50,934 - INFO -   Response: 4634 chars, Found 0 verses
2025-07-07 09:11:50,934 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:11:50,934 - INFO -   Matched verses: []
2025-07-07 09:11:51,935 - INFO - Processing question 69/881 (ID: 74) for qwen
2025-07-07 09:11:51,935 - INFO -   Ground truth verses: ['15:63', '44:50']
2025-07-07 09:11:51,936 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:12:07,785 - INFO - Extracted JSON from code block: 751 characters
2025-07-07 09:12:07,786 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:12:07,786 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:12:07,786 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:12:07,786 - INFO - Normalized predicted verses: ['15:6', '2:255', '4:89', '50:5', '5:5']
2025-07-07 09:12:07,786 - INFO - Normalized ground truth verses: ['15:63', '44:50']
2025-07-07 09:12:07,786 - INFO -   Response: 751 chars, Found 5 verses
2025-07-07 09:12:07,786 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:12:07,786 - INFO -   Matched verses: []
2025-07-07 09:12:08,787 - INFO - Processing question 70/881 (ID: 75) for qwen
2025-07-07 09:12:08,788 - INFO -   Ground truth verses: ['15:63', '44:50']
2025-07-07 09:12:08,788 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:12:24,550 - INFO - Extracted JSON from code block: 748 characters
2025-07-07 09:12:24,551 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Qamar:11]' -> 'Al-Qamar:11'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Furqan:5]' -> 'Al-Furqan:5'
2025-07-07 09:12:24,551 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Qamar:11]' -> 'Al-Qamar:11'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Qamar:11]' -> 'Al-Qamar:11'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Furqan:5]' -> 'Al-Furqan:5'
2025-07-07 09:12:24,551 - WARNING - Invalid colon format: '[Al-Furqan:5]' -> 'Al-Furqan:5'
2025-07-07 09:12:24,551 - INFO - Normalized predicted verses: ['15:6', '25:5', '4:82', '50:11', '5:5']
2025-07-07 09:12:24,551 - INFO - Normalized ground truth verses: ['15:63', '44:50']
2025-07-07 09:12:24,551 - INFO -   Response: 748 chars, Found 5 verses
2025-07-07 09:12:24,551 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:12:24,551 - INFO -   Matched verses: []
2025-07-07 09:12:24,562 - INFO - Saved data to results/qwen_checkpoint_69.json
2025-07-07 09:12:24,562 - INFO - Checkpoint saved for qwen at question 70
2025-07-07 09:12:24,562 - INFO -   Progress: 70/881
2025-07-07 09:12:24,562 - INFO -   Running averages - F1: 0.005, Precision: 0.003, Recall: 0.014
2025-07-07 09:12:25,563 - INFO - Processing question 71/881 (ID: 76) for qwen
2025-07-07 09:12:25,563 - INFO -   Ground truth verses: ['15:63', '44:50']
2025-07-07 09:12:25,564 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:12:41,308 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:12:41,308 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:12:41,308 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:12:41,308 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:12:41,308 - INFO - Normalized predicted verses: []
2025-07-07 09:12:41,308 - INFO - Normalized ground truth verses: ['15:63', '44:50']
2025-07-07 09:12:41,308 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 09:12:41,308 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:12:41,308 - INFO -   Matched verses: []
2025-07-07 09:12:42,309 - INFO - Processing question 72/881 (ID: 79) for qwen
2025-07-07 09:12:42,310 - INFO -   Ground truth verses: ['43:87', '43:9', '31:25', '14:33', '29:61']
2025-07-07 09:12:42,310 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:12:57,990 - INFO - Extracted JSON from code block: 761 characters
2025-07-07 09:12:57,990 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Hijr:16]' -> 'Al-Hijr:16'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Mu'minun:22]' -> 'Al-Mu'minun:22'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:12:57,991 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Hijr:16]' -> 'Al-Hijr:16'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Hijr:16]' -> 'Al-Hijr:16'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Mu'minun:22]' -> 'Al-Mu'minun:22'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Mu'minun:22]' -> 'Al-Mu'minun:22'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:12:57,991 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:12:57,991 - INFO - Normalized predicted verses: ['15:16', '23:22', '2:255', '7:113']
2025-07-07 09:12:57,991 - INFO - Normalized ground truth verses: ['14:33', '29:61', '31:25', '43:87', '43:9']
2025-07-07 09:12:57,991 - INFO -   Response: 761 chars, Found 5 verses
2025-07-07 09:12:57,991 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:12:57,991 - INFO -   Matched verses: []
2025-07-07 09:12:58,992 - INFO - Processing question 73/881 (ID: 80) for qwen
2025-07-07 09:12:58,993 - INFO -   Ground truth verses: ['43:87', '43:9', '31:25', '14:33', '29:61']
2025-07-07 09:12:58,993 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:13:14,672 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:13:14,672 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:13:14,672 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:13:14,672 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:13:14,672 - INFO - Normalized predicted verses: []
2025-07-07 09:13:14,672 - INFO - Normalized ground truth verses: ['14:33', '29:61', '31:25', '43:87', '43:9']
2025-07-07 09:13:14,672 - INFO -   Response: 4127 chars, Found 0 verses
2025-07-07 09:13:14,672 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:13:14,672 - INFO -   Matched verses: []
2025-07-07 09:13:15,673 - INFO - Processing question 74/881 (ID: 81) for qwen
2025-07-07 09:13:15,674 - INFO -   Ground truth verses: ['43:87', '43:9', '31:25', '14:33', '29:61']
2025-07-07 09:13:15,674 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:13:31,405 - INFO - Extracted JSON from code block: 757 characters
2025-07-07 09:13:31,405 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-An'am:80]' -> 'Al-An'am:80'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-Ikhlas:1]' -> 'Al-Ikhlas:1'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-Hijr:46]' -> 'Al-Hijr:46'
2025-07-07 09:13:31,405 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-An'am:80]' -> 'Al-An'am:80'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-An'am:80]' -> 'Al-An'am:80'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-Ikhlas:1]' -> 'Al-Ikhlas:1'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-Ikhlas:1]' -> 'Al-Ikhlas:1'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-Hijr:46]' -> 'Al-Hijr:46'
2025-07-07 09:13:31,405 - WARNING - Invalid colon format: '[Al-Hijr:46]' -> 'Al-Hijr:46'
2025-07-07 09:13:31,405 - INFO - Normalized predicted verses: ['112:1', '15:46', '2:255', '6:80', '7:113']
2025-07-07 09:13:31,405 - INFO - Normalized ground truth verses: ['14:33', '29:61', '31:25', '43:87', '43:9']
2025-07-07 09:13:31,405 - INFO -   Response: 757 chars, Found 5 verses
2025-07-07 09:13:31,405 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:13:31,406 - INFO -   Matched verses: []
2025-07-07 09:13:32,407 - INFO - Processing question 75/881 (ID: 82) for qwen
2025-07-07 09:13:32,407 - INFO -   Ground truth verses: ['43:87', '43:9', '31:25', '14:33', '29:61']
2025-07-07 09:13:32,407 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:13:48,083 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:13:48,083 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:13:48,083 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:13:48,083 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:13:48,083 - INFO - Normalized predicted verses: []
2025-07-07 09:13:48,083 - INFO - Normalized ground truth verses: ['14:33', '29:61', '31:25', '43:87', '43:9']
2025-07-07 09:13:48,083 - INFO -   Response: 5208 chars, Found 0 verses
2025-07-07 09:13:48,084 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:13:48,084 - INFO -   Matched verses: []
2025-07-07 09:13:48,094 - INFO - Saved data to results/qwen_checkpoint_74.json
2025-07-07 09:13:48,094 - INFO - Checkpoint saved for qwen at question 75
2025-07-07 09:13:48,094 - INFO -   Progress: 75/881
2025-07-07 09:13:48,094 - INFO -   Running averages - F1: 0.004, Precision: 0.003, Recall: 0.013
2025-07-07 09:13:49,095 - INFO - Processing question 76/881 (ID: 83) for qwen
2025-07-07 09:13:49,096 - INFO -   Ground truth verses: ['43:87', '43:9', '31:25', '14:33', '29:61']
2025-07-07 09:13:49,096 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:14:04,828 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:14:04,828 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:14:04,829 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:14:04,829 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:14:04,829 - INFO - Normalized predicted verses: []
2025-07-07 09:14:04,829 - INFO - Normalized ground truth verses: ['14:33', '29:61', '31:25', '43:87', '43:9']
2025-07-07 09:14:04,829 - INFO -   Response: 4277 chars, Found 0 verses
2025-07-07 09:14:04,829 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:14:04,829 - INFO -   Matched verses: []
2025-07-07 09:14:05,830 - INFO - Processing question 77/881 (ID: 84) for qwen
2025-07-07 09:14:05,830 - INFO -   Ground truth verses: ['59:1', '57:1', '62:1', '45:37', '64:1', '4:132', '3:109', '4:131', '4:126', '42:4', '59:24', '3:129', '14:2', '22:64', '34:1', '53:31', '20:6', '42:53', '31:26', '30:27', '2:284', '16:49', '45:13', '49:16', '3:29', '10:68', '61:1']
2025-07-07 09:14:05,830 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:14:21,464 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:14:21,464 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:14:21,464 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:14:21,464 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:14:21,464 - INFO - Normalized predicted verses: []
2025-07-07 09:14:21,464 - INFO - Normalized ground truth verses: ['10:68', '14:2', '16:49', '20:6', '22:64', '2:284', '30:27', '31:26', '34:1', '3:109', '3:129', '3:29', '42:4', '42:53', '45:13', '45:37', '49:16', '4:126', '4:131', '4:132', '53:31', '57:1', '59:1', '59:24', '61:1', '62:1', '64:1']
2025-07-07 09:14:21,464 - INFO -   Response: 4266 chars, Found 0 verses
2025-07-07 09:14:21,464 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:14:21,464 - INFO -   Matched verses: []
2025-07-07 09:14:22,465 - INFO - Processing question 78/881 (ID: 85) for qwen
2025-07-07 09:14:22,466 - INFO -   Ground truth verses: ['59:1', '57:1', '62:1', '45:37', '64:1', '4:132', '3:109', '4:131', '4:126', '42:4', '59:24', '3:129', '14:2', '22:64', '34:1', '53:31', '20:6', '42:53', '31:26', '30:27', '2:284', '16:49', '45:13', '49:16', '3:29', '10:68', '61:1']
2025-07-07 09:14:22,466 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:14:38,136 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Mulk:28]' -> 'Al-Mulk:28'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:14:38,136 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Mulk:28]' -> 'Al-Mulk:28'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Mulk:28]' -> 'Al-Mulk:28'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:14:38,136 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:14:38,136 - INFO - Normalized predicted verses: ['15:7', '2:255', '67:28', '6:84']
2025-07-07 09:14:38,136 - INFO - Normalized ground truth verses: ['10:68', '14:2', '16:49', '20:6', '22:64', '2:284', '30:27', '31:26', '34:1', '3:109', '3:129', '3:29', '42:4', '42:53', '45:13', '45:37', '49:16', '4:126', '4:131', '4:132', '53:31', '57:1', '59:1', '59:24', '61:1', '62:1', '64:1']
2025-07-07 09:14:38,136 - INFO -   Response: 4977 chars, Found 5 verses
2025-07-07 09:14:38,136 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:14:38,136 - INFO -   Matched verses: []
2025-07-07 09:14:39,137 - INFO - Processing question 79/881 (ID: 86) for qwen
2025-07-07 09:14:39,138 - INFO -   Ground truth verses: ['59:1', '57:1', '62:1', '45:37', '64:1', '4:132', '3:109', '4:131', '4:126', '42:4', '59:24', '3:129', '14:2', '22:64', '34:1', '53:31', '20:6', '42:53', '31:26', '30:27', '2:284', '16:49', '45:13', '49:16', '3:29', '10:68', '61:1']
2025-07-07 09:14:39,138 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:14:54,679 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:14:54,679 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:14:54,679 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:14:54,679 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:14:54,679 - INFO - Normalized predicted verses: []
2025-07-07 09:14:54,679 - INFO - Normalized ground truth verses: ['10:68', '14:2', '16:49', '20:6', '22:64', '2:284', '30:27', '31:26', '34:1', '3:109', '3:129', '3:29', '42:4', '42:53', '45:13', '45:37', '49:16', '4:126', '4:131', '4:132', '53:31', '57:1', '59:1', '59:24', '61:1', '62:1', '64:1']
2025-07-07 09:14:54,679 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 09:14:54,679 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:14:54,679 - INFO -   Matched verses: []
2025-07-07 09:14:55,680 - INFO - Processing question 80/881 (ID: 87) for qwen
2025-07-07 09:14:55,681 - INFO -   Ground truth verses: ['59:1', '57:1', '62:1', '45:37', '64:1', '4:132', '3:109', '4:131', '4:126', '42:4', '59:24', '3:129', '14:2', '22:64', '34:1', '53:31', '20:6', '42:53', '31:26', '30:27', '2:284', '16:49', '45:13', '49:16', '3:29', '10:68', '61:1']
2025-07-07 09:14:55,681 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:15:11,405 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:15:11,405 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:15:11,405 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:15:11,405 - INFO - Normalized predicted verses: []
2025-07-07 09:15:11,405 - INFO - Normalized ground truth verses: ['10:68', '14:2', '16:49', '20:6', '22:64', '2:284', '30:27', '31:26', '34:1', '3:109', '3:129', '3:29', '42:4', '42:53', '45:13', '45:37', '49:16', '4:126', '4:131', '4:132', '53:31', '57:1', '59:1', '59:24', '61:1', '62:1', '64:1']
2025-07-07 09:15:11,406 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:15:11,406 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:15:11,406 - INFO -   Matched verses: []
2025-07-07 09:15:11,416 - INFO - Saved data to results/qwen_checkpoint_79.json
2025-07-07 09:15:11,416 - INFO - Checkpoint saved for qwen at question 80
2025-07-07 09:15:11,416 - INFO -   Progress: 80/881
2025-07-07 09:15:11,416 - INFO -   Running averages - F1: 0.004, Precision: 0.003, Recall: 0.013
2025-07-07 09:15:12,417 - INFO - Processing question 81/881 (ID: 88) for qwen
2025-07-07 09:15:12,418 - INFO -   Ground truth verses: ['59:1', '57:1', '62:1', '45:37', '64:1', '4:132', '3:109', '4:131', '4:126', '42:4', '59:24', '3:129', '14:2', '22:64', '34:1', '53:31', '20:6', '42:53', '31:26', '30:27', '2:284', '16:49', '45:13', '49:16', '3:29', '10:68', '61:1']
2025-07-07 09:15:12,418 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:15:28,200 - WARNING - Found ```json but no closing ```, using content after ```json
2025-07-07 09:15:28,201 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 7 column 10 (char 162), falling back to regex parsing
2025-07-07 09:15:28,201 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 7 column 10 (char 162), falling back to regex parsing
2025-07-07 09:15:28,201 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 7 column 10 (char 162), falling back to regex parsing
2025-07-07 09:15:28,201 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:15:28,201 - INFO - Normalized predicted verses: []
2025-07-07 09:15:28,201 - INFO - Normalized ground truth verses: ['10:68', '14:2', '16:49', '20:6', '22:64', '2:284', '30:27', '31:26', '34:1', '3:109', '3:129', '3:29', '42:4', '42:53', '45:13', '45:37', '49:16', '4:126', '4:131', '4:132', '53:31', '57:1', '59:1', '59:24', '61:1', '62:1', '64:1']
2025-07-07 09:15:28,201 - INFO -   Response: 259 chars, Found 0 verses
2025-07-07 09:15:28,201 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:15:28,201 - INFO -   Matched verses: []
2025-07-07 09:15:29,202 - INFO - Processing question 82/881 (ID: 89) for qwen
2025-07-07 09:15:29,203 - INFO -   Ground truth verses: ['10:55', '28:13', '52:47', '43:78', '44:39']
2025-07-07 09:15:29,203 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:15:44,860 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:15:44,861 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:15:44,861 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:15:44,861 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:15:44,861 - INFO - Normalized predicted verses: []
2025-07-07 09:15:44,861 - INFO - Normalized ground truth verses: ['10:55', '28:13', '43:78', '44:39', '52:47']
2025-07-07 09:15:44,861 - INFO -   Response: 4391 chars, Found 0 verses
2025-07-07 09:15:44,861 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:15:44,861 - INFO -   Matched verses: []
2025-07-07 09:15:45,862 - INFO - Processing question 83/881 (ID: 90) for qwen
2025-07-07 09:15:45,862 - INFO -   Ground truth verses: ['10:55', '28:13', '52:47', '43:78', '44:39']
2025-07-07 09:15:45,862 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:16:01,585 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:16:01,586 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:16:01,586 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:16:01,586 - INFO - Normalized predicted verses: []
2025-07-07 09:16:01,586 - INFO - Normalized ground truth verses: ['10:55', '28:13', '43:78', '44:39', '52:47']
2025-07-07 09:16:01,586 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:16:01,586 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:16:01,586 - INFO -   Matched verses: []
2025-07-07 09:16:02,587 - INFO - Processing question 84/881 (ID: 91) for qwen
2025-07-07 09:16:02,587 - INFO -   Ground truth verses: ['10:55', '28:13', '52:47', '43:78', '44:39']
2025-07-07 09:16:02,587 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:16:18,338 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:16:18,338 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:16:18,338 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:16:18,338 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:16:18,338 - INFO - Normalized predicted verses: []
2025-07-07 09:16:18,338 - INFO - Normalized ground truth verses: ['10:55', '28:13', '43:78', '44:39', '52:47']
2025-07-07 09:16:18,338 - INFO -   Response: 4462 chars, Found 0 verses
2025-07-07 09:16:18,338 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:16:18,338 - INFO -   Matched verses: []
2025-07-07 09:16:19,340 - INFO - Processing question 85/881 (ID: 92) for qwen
2025-07-07 09:16:19,340 - INFO -   Ground truth verses: ['10:55', '28:13', '52:47', '43:78', '44:39']
2025-07-07 09:16:19,340 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:16:34,954 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:16:34,954 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:16:34,954 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:16:34,954 - INFO - Normalized predicted verses: []
2025-07-07 09:16:34,954 - INFO - Normalized ground truth verses: ['10:55', '28:13', '43:78', '44:39', '52:47']
2025-07-07 09:16:34,954 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:16:34,954 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:16:34,954 - INFO -   Matched verses: []
2025-07-07 09:16:34,965 - INFO - Saved data to results/qwen_checkpoint_84.json
2025-07-07 09:16:34,965 - INFO - Checkpoint saved for qwen at question 85
2025-07-07 09:16:34,965 - INFO -   Progress: 85/881
2025-07-07 09:16:34,965 - INFO -   Running averages - F1: 0.004, Precision: 0.002, Recall: 0.012
2025-07-07 09:16:35,966 - INFO - Processing question 86/881 (ID: 93) for qwen
2025-07-07 09:16:35,967 - INFO -   Ground truth verses: ['10:55', '28:13', '52:47', '43:78', '44:39']
2025-07-07 09:16:35,967 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:16:51,590 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:16:51,591 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:16:51,591 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:16:51,591 - INFO - Normalized predicted verses: []
2025-07-07 09:16:51,591 - INFO - Normalized ground truth verses: ['10:55', '28:13', '43:78', '44:39', '52:47']
2025-07-07 09:16:51,591 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:16:51,591 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:16:51,591 - INFO -   Matched verses: []
2025-07-07 09:16:52,592 - INFO - Processing question 87/881 (ID: 94) for qwen
2025-07-07 09:16:52,592 - INFO -   Ground truth verses: ['34:11', '23:51']
2025-07-07 09:16:52,592 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:17:08,203 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:17:08,204 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:17:08,204 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:17:08,204 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:17:08,204 - INFO - Normalized predicted verses: []
2025-07-07 09:17:08,204 - INFO - Normalized ground truth verses: ['23:51', '34:11']
2025-07-07 09:17:08,204 - INFO -   Response: 4003 chars, Found 0 verses
2025-07-07 09:17:08,204 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:17:08,204 - INFO -   Matched verses: []
2025-07-07 09:17:09,205 - INFO - Processing question 88/881 (ID: 95) for qwen
2025-07-07 09:17:09,205 - INFO -   Ground truth verses: ['34:11', '23:51']
2025-07-07 09:17:09,205 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:17:24,870 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:17:24,870 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:17:24,870 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:17:24,870 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:17:24,870 - INFO - Normalized predicted verses: []
2025-07-07 09:17:24,870 - INFO - Normalized ground truth verses: ['23:51', '34:11']
2025-07-07 09:17:24,870 - INFO -   Response: 4187 chars, Found 0 verses
2025-07-07 09:17:24,870 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:17:24,870 - INFO -   Matched verses: []
2025-07-07 09:17:25,871 - INFO - Processing question 89/881 (ID: 96) for qwen
2025-07-07 09:17:25,871 - INFO -   Ground truth verses: ['34:11', '23:51']
2025-07-07 09:17:25,871 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:17:41,613 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 09:17:41,614 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Ma'idah:4]' -> 'Al-Ma'idah:4'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Hujurat:12]' -> 'Al-Hujurat:12'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Tawbah:105]' -> 'Al-Tawbah:105'
2025-07-07 09:17:41,614 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Ma'idah:4]' -> 'Al-Ma'idah:4'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Ma'idah:4]' -> 'Al-Ma'idah:4'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Hujurat:12]' -> 'Al-Hujurat:12'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Hujurat:12]' -> 'Al-Hujurat:12'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Tawbah:105]' -> 'Al-Tawbah:105'
2025-07-07 09:17:41,614 - WARNING - Invalid colon format: '[Al-Tawbah:105]' -> 'Al-Tawbah:105'
2025-07-07 09:17:41,614 - INFO - Normalized predicted verses: ['2:255', '49:12', '5:4', '6:89', '9:105']
2025-07-07 09:17:41,614 - INFO - Normalized ground truth verses: ['23:51', '34:11']
2025-07-07 09:17:41,614 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 09:17:41,614 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:17:41,614 - INFO -   Matched verses: []
2025-07-07 09:17:42,616 - INFO - Processing question 90/881 (ID: 97) for qwen
2025-07-07 09:17:42,616 - INFO -   Ground truth verses: ['34:11', '23:51']
2025-07-07 09:17:42,616 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:17:58,366 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:17:58,366 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:17:58,366 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:17:58,367 - INFO - Normalized predicted verses: []
2025-07-07 09:17:58,367 - INFO - Normalized ground truth verses: ['23:51', '34:11']
2025-07-07 09:17:58,367 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:17:58,367 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:17:58,367 - INFO -   Matched verses: []
2025-07-07 09:17:58,378 - INFO - Saved data to results/qwen_checkpoint_89.json
2025-07-07 09:17:58,378 - INFO - Checkpoint saved for qwen at question 90
2025-07-07 09:17:58,378 - INFO -   Progress: 90/881
2025-07-07 09:17:58,378 - INFO -   Running averages - F1: 0.004, Precision: 0.002, Recall: 0.011
2025-07-07 09:17:59,379 - INFO - Processing question 91/881 (ID: 98) for qwen
2025-07-07 09:17:59,379 - INFO -   Ground truth verses: ['34:11', '23:51']
2025-07-07 09:17:59,379 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:18:14,963 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:18:14,963 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:18:14,963 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:18:14,963 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:18:14,963 - INFO - Normalized predicted verses: []
2025-07-07 09:18:14,963 - INFO - Normalized ground truth verses: ['23:51', '34:11']
2025-07-07 09:18:14,963 - INFO -   Response: 4122 chars, Found 0 verses
2025-07-07 09:18:14,963 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:18:14,963 - INFO -   Matched verses: []
2025-07-07 09:18:15,965 - INFO - Processing question 92/881 (ID: 99) for qwen
2025-07-07 09:18:15,965 - INFO -   Ground truth verses: ['2:189']
2025-07-07 09:18:15,965 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:18:31,657 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:18:31,657 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:18:31,657 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:18:31,658 - INFO - Normalized predicted verses: []
2025-07-07 09:18:31,658 - INFO - Normalized ground truth verses: ['2:189']
2025-07-07 09:18:31,658 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:18:31,658 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:18:31,658 - INFO -   Matched verses: []
2025-07-07 09:18:32,659 - INFO - Processing question 93/881 (ID: 100) for qwen
2025-07-07 09:18:32,659 - INFO -   Ground truth verses: ['2:189']
2025-07-07 09:18:32,659 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:18:48,301 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:18:48,301 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:18:48,301 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:18:48,301 - INFO - Normalized predicted verses: []
2025-07-07 09:18:48,301 - INFO - Normalized ground truth verses: ['2:189']
2025-07-07 09:18:48,301 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:18:48,301 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:18:48,301 - INFO -   Matched verses: []
2025-07-07 09:18:49,302 - INFO - Processing question 94/881 (ID: 101) for qwen
2025-07-07 09:18:49,303 - INFO -   Ground truth verses: ['2:189']
2025-07-07 09:18:49,303 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:19:04,985 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:19:04,985 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:19:04,985 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:19:04,985 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:19:04,985 - INFO - Normalized predicted verses: []
2025-07-07 09:19:04,985 - INFO - Normalized ground truth verses: ['2:189']
2025-07-07 09:19:04,985 - INFO -   Response: 4782 chars, Found 0 verses
2025-07-07 09:19:04,985 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:19:04,985 - INFO -   Matched verses: []
2025-07-07 09:19:05,986 - INFO - Processing question 95/881 (ID: 102) for qwen
2025-07-07 09:19:05,986 - INFO -   Ground truth verses: ['2:189']
2025-07-07 09:19:05,986 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:19:21,643 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:19:21,643 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:19:21,643 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:19:21,643 - INFO - Normalized predicted verses: []
2025-07-07 09:19:21,643 - INFO - Normalized ground truth verses: ['2:189']
2025-07-07 09:19:21,644 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:19:21,644 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:19:21,644 - INFO -   Matched verses: []
2025-07-07 09:19:21,655 - INFO - Saved data to results/qwen_checkpoint_94.json
2025-07-07 09:19:21,655 - INFO - Checkpoint saved for qwen at question 95
2025-07-07 09:19:21,655 - INFO -   Progress: 95/881
2025-07-07 09:19:21,655 - INFO -   Running averages - F1: 0.004, Precision: 0.002, Recall: 0.011
2025-07-07 09:19:22,657 - INFO - Processing question 96/881 (ID: 103) for qwen
2025-07-07 09:19:22,657 - INFO -   Ground truth verses: ['2:189']
2025-07-07 09:19:22,657 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:19:38,343 - INFO - Extracted JSON from code block: 764 characters
2025-07-07 09:19:38,344 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-An'am:148]' -> 'Al-An'am:148'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Tawbah:103]' -> 'Al-Tawbah:103'
2025-07-07 09:19:38,344 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-An'am:148]' -> 'Al-An'am:148'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-An'am:148]' -> 'Al-An'am:148'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Tawbah:103]' -> 'Al-Tawbah:103'
2025-07-07 09:19:38,344 - WARNING - Invalid colon format: '[Al-Tawbah:103]' -> 'Al-Tawbah:103'
2025-07-07 09:19:38,344 - INFO - Normalized predicted verses: ['2:255', '50:11', '5:11', '6:148', '9:103']
2025-07-07 09:19:38,344 - INFO - Normalized ground truth verses: ['2:189']
2025-07-07 09:19:38,344 - INFO -   Response: 764 chars, Found 5 verses
2025-07-07 09:19:38,344 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:19:38,344 - INFO -   Matched verses: []
2025-07-07 09:19:39,345 - INFO - Processing question 97/881 (ID: 104) for qwen
2025-07-07 09:19:39,346 - INFO -   Ground truth verses: ['75:25']
2025-07-07 09:19:39,346 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:19:54,981 - INFO - Extracted JSON from code block: 761 characters
2025-07-07 09:19:54,982 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:19:54,982 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:19:54,982 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:19:54,982 - INFO - Normalized predicted verses: ['15:72', '2:255', '5:104', '7:113']
2025-07-07 09:19:54,982 - INFO - Normalized ground truth verses: ['75:25']
2025-07-07 09:19:54,982 - INFO -   Response: 761 chars, Found 5 verses
2025-07-07 09:19:54,982 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:19:54,982 - INFO -   Matched verses: []
2025-07-07 09:19:55,983 - INFO - Processing question 98/881 (ID: 105) for qwen
2025-07-07 09:19:55,984 - INFO -   Ground truth verses: ['75:25']
2025-07-07 09:19:55,984 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:20:11,724 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:20:11,724 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:20:11,724 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:20:11,724 - INFO - Normalized predicted verses: []
2025-07-07 09:20:11,724 - INFO - Normalized ground truth verses: ['75:25']
2025-07-07 09:20:11,724 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:20:11,724 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:20:11,724 - INFO -   Matched verses: []
2025-07-07 09:20:12,725 - INFO - Processing question 99/881 (ID: 106) for qwen
2025-07-07 09:20:12,726 - INFO -   Ground truth verses: ['75:25']
2025-07-07 09:20:12,726 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:20:28,399 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:20:28,399 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:20:28,399 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:20:28,399 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:20:28,399 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:20:28,399 - WARNING - Invalid colon format: '[Al-Tawbah:11]' -> 'Al-Tawbah:11'
2025-07-07 09:20:28,400 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:20:28,400 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:20:28,400 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:20:28,400 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:20:28,400 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:20:28,400 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:20:28,400 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:20:28,400 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:20:28,400 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:20:28,400 - WARNING - Invalid colon format: '[Al-Tawbah:11]' -> 'Al-Tawbah:11'
2025-07-07 09:20:28,400 - WARNING - Invalid colon format: '[Al-Tawbah:11]' -> 'Al-Tawbah:11'
2025-07-07 09:20:28,400 - INFO - Normalized predicted verses: ['15:15', '2:255', '4:86', '5:10', '9:11']
2025-07-07 09:20:28,400 - INFO - Normalized ground truth verses: ['75:25']
2025-07-07 09:20:28,400 - INFO -   Response: 4984 chars, Found 5 verses
2025-07-07 09:20:28,400 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:20:28,400 - INFO -   Matched verses: []
2025-07-07 09:20:29,401 - INFO - Processing question 100/881 (ID: 107) for qwen
2025-07-07 09:20:29,401 - INFO -   Ground truth verses: ['75:25']
2025-07-07 09:20:29,401 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:20:45,120 - INFO - Extracted JSON from code block: 761 characters
2025-07-07 09:20:45,120 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:20:45,120 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:20:45,120 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:20:45,120 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:20:45,120 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:20:45,120 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:20:45,120 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:20:45,120 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:20:45,120 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:20:45,120 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:20:45,120 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:20:45,120 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:20:45,121 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:20:45,121 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:20:45,121 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:20:45,121 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:20:45,121 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:20:45,121 - INFO - Normalized predicted verses: ['15:72', '2:255', '55:55', '5:104', '7:113']
2025-07-07 09:20:45,121 - INFO - Normalized ground truth verses: ['75:25']
2025-07-07 09:20:45,121 - INFO -   Response: 761 chars, Found 5 verses
2025-07-07 09:20:45,121 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:20:45,121 - INFO -   Matched verses: []
2025-07-07 09:20:45,133 - INFO - Saved data to results/qwen_checkpoint_99.json
2025-07-07 09:20:45,133 - INFO - Checkpoint saved for qwen at question 100
2025-07-07 09:20:45,133 - INFO -   Progress: 100/881
2025-07-07 09:20:45,133 - INFO -   Running averages - F1: 0.003, Precision: 0.002, Recall: 0.010
2025-07-07 09:20:46,134 - INFO - Processing question 101/881 (ID: 108) for qwen
2025-07-07 09:20:46,135 - INFO -   Ground truth verses: ['75:25']
2025-07-07 09:20:46,135 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:21:01,721 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:21:01,721 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:21:01,721 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:21:01,721 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:21:01,721 - INFO - Normalized predicted verses: []
2025-07-07 09:21:01,721 - INFO - Normalized ground truth verses: ['75:25']
2025-07-07 09:21:01,721 - INFO -   Response: 4111 chars, Found 0 verses
2025-07-07 09:21:01,721 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:21:01,721 - INFO -   Matched verses: []
2025-07-07 09:21:02,722 - INFO - Processing question 102/881 (ID: 109) for qwen
2025-07-07 09:21:02,722 - INFO -   Ground truth verses: ['7:132']
2025-07-07 09:21:02,722 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:21:18,402 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 873), falling back to regex parsing
2025-07-07 09:21:18,402 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 873), falling back to regex parsing
2025-07-07 09:21:18,402 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 873), falling back to regex parsing
2025-07-07 09:21:18,402 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:21:18,402 - INFO - Normalized predicted verses: []
2025-07-07 09:21:18,402 - INFO - Normalized ground truth verses: ['7:132']
2025-07-07 09:21:18,402 - INFO -   Response: 3947 chars, Found 0 verses
2025-07-07 09:21:18,402 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:21:18,402 - INFO -   Matched verses: []
2025-07-07 09:21:19,403 - INFO - Processing question 103/881 (ID: 110) for qwen
2025-07-07 09:21:19,403 - INFO -   Ground truth verses: ['7:132']
2025-07-07 09:21:19,403 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:21:35,032 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 09:21:35,032 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Ankabut:29]' -> 'Al-Ankabut:29'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:21:35,032 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Ankabut:29]' -> 'Al-Ankabut:29'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Ankabut:29]' -> 'Al-Ankabut:29'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:21:35,032 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:21:35,033 - INFO - Normalized predicted verses: ['15:7', '29:29', '2:255', '55:10', '5:104']
2025-07-07 09:21:35,033 - INFO - Normalized ground truth verses: ['7:132']
2025-07-07 09:21:35,033 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 09:21:35,033 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:21:35,033 - INFO -   Matched verses: []
2025-07-07 09:21:36,034 - INFO - Processing question 104/881 (ID: 111) for qwen
2025-07-07 09:21:36,034 - INFO -   Ground truth verses: ['7:132']
2025-07-07 09:21:36,034 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:21:51,874 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:21:51,874 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:21:51,874 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:21:51,874 - INFO - Normalized predicted verses: []
2025-07-07 09:21:51,874 - INFO - Normalized ground truth verses: ['7:132']
2025-07-07 09:21:51,874 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:21:51,874 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:21:51,874 - INFO -   Matched verses: []
2025-07-07 09:21:52,875 - INFO - Processing question 105/881 (ID: 112) for qwen
2025-07-07 09:21:52,876 - INFO -   Ground truth verses: ['7:132']
2025-07-07 09:21:52,876 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:22:08,686 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 09:22:08,687 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Ankabut:29]' -> 'Al-Ankabut:29'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:22:08,687 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Ankabut:29]' -> 'Al-Ankabut:29'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Ankabut:29]' -> 'Al-Ankabut:29'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:22:08,687 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:22:08,687 - INFO - Normalized predicted verses: ['15:10', '29:29', '2:255', '50:10', '5:10']
2025-07-07 09:22:08,687 - INFO - Normalized ground truth verses: ['7:132']
2025-07-07 09:22:08,687 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 09:22:08,687 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:22:08,687 - INFO -   Matched verses: []
2025-07-07 09:22:08,700 - INFO - Saved data to results/qwen_checkpoint_104.json
2025-07-07 09:22:08,700 - INFO - Checkpoint saved for qwen at question 105
2025-07-07 09:22:08,700 - INFO -   Progress: 105/881
2025-07-07 09:22:08,700 - INFO -   Running averages - F1: 0.003, Precision: 0.002, Recall: 0.010
2025-07-07 09:22:09,701 - INFO - Processing question 106/881 (ID: 113) for qwen
2025-07-07 09:22:09,701 - INFO -   Ground truth verses: ['7:132']
2025-07-07 09:22:09,701 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:22:25,394 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:96]' -> 'Al-An'am:96'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:100]' -> 'Al-An'am:100'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:101]' -> 'Al-An'am:101'
2025-07-07 09:22:25,394 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:96]' -> 'Al-An'am:96'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:96]' -> 'Al-An'am:96'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:100]' -> 'Al-An'am:100'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:100]' -> 'Al-An'am:100'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:101]' -> 'Al-An'am:101'
2025-07-07 09:22:25,394 - WARNING - Invalid colon format: '[Al-An'am:101]' -> 'Al-An'am:101'
2025-07-07 09:22:25,394 - INFO - Normalized predicted verses: ['2:255', '4:100', '4:101', '4:89', '4:96']
2025-07-07 09:22:25,394 - INFO - Normalized ground truth verses: ['7:132']
2025-07-07 09:22:25,394 - INFO -   Response: 4674 chars, Found 5 verses
2025-07-07 09:22:25,394 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:22:25,394 - INFO -   Matched verses: []
2025-07-07 09:22:26,396 - INFO - Processing question 107/881 (ID: 115) for qwen
2025-07-07 09:22:26,396 - INFO -   Ground truth verses: ['68:5', '37:179', '37:175', '52:15', '51:21', '56:85', '69:38', '69:39']
2025-07-07 09:22:26,396 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:22:42,088 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 09:22:42,088 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:22:42,088 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Mu'minun:6]' -> 'Al-Mu'minun:6'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:22:42,089 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Mu'minun:6]' -> 'Al-Mu'minun:6'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Mu'minun:6]' -> 'Al-Mu'minun:6'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:22:42,089 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:22:42,089 - INFO - Normalized predicted verses: ['15:10', '23:6', '25:56', '2:255', '7:113']
2025-07-07 09:22:42,089 - INFO - Normalized ground truth verses: ['37:175', '37:179', '51:21', '52:15', '56:85', '68:5', '69:38', '69:39']
2025-07-07 09:22:42,089 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 09:22:42,089 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:22:42,089 - INFO -   Matched verses: []
2025-07-07 09:22:43,090 - INFO - Processing question 108/881 (ID: 116) for qwen
2025-07-07 09:22:43,090 - INFO -   Ground truth verses: ['68:5', '37:179', '37:175', '52:15', '51:21', '56:85', '69:38', '69:39']
2025-07-07 09:22:43,090 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:22:58,823 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:22:58,823 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:22:58,823 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:22:58,823 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:22:58,824 - INFO - Normalized predicted verses: []
2025-07-07 09:22:58,824 - INFO - Normalized ground truth verses: ['37:175', '37:179', '51:21', '52:15', '56:85', '68:5', '69:38', '69:39']
2025-07-07 09:22:58,824 - INFO -   Response: 3866 chars, Found 0 verses
2025-07-07 09:22:58,824 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:22:58,824 - INFO -   Matched verses: []
2025-07-07 09:22:59,825 - INFO - Processing question 109/881 (ID: 117) for qwen
2025-07-07 09:22:59,825 - INFO -   Ground truth verses: ['68:5', '37:179', '37:175', '52:15', '51:21', '56:85', '69:38', '69:39']
2025-07-07 09:22:59,825 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:23:15,624 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:23:15,624 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:23:15,624 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:23:15,624 - INFO - Normalized predicted verses: []
2025-07-07 09:23:15,624 - INFO - Normalized ground truth verses: ['37:175', '37:179', '51:21', '52:15', '56:85', '68:5', '69:38', '69:39']
2025-07-07 09:23:15,624 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:23:15,624 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:23:15,624 - INFO -   Matched verses: []
2025-07-07 09:23:16,626 - INFO - Processing question 110/881 (ID: 118) for qwen
2025-07-07 09:23:16,626 - INFO -   Ground truth verses: ['68:5', '37:179', '37:175', '52:15', '51:21', '56:85', '69:38', '69:39']
2025-07-07 09:23:16,626 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:23:32,321 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:23:32,321 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:23:32,321 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:23:32,321 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:23:32,321 - INFO - Normalized predicted verses: []
2025-07-07 09:23:32,321 - INFO - Normalized ground truth verses: ['37:175', '37:179', '51:21', '52:15', '56:85', '68:5', '69:38', '69:39']
2025-07-07 09:23:32,321 - INFO -   Response: 4292 chars, Found 0 verses
2025-07-07 09:23:32,321 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:23:32,321 - INFO -   Matched verses: []
2025-07-07 09:23:32,334 - INFO - Saved data to results/qwen_checkpoint_109.json
2025-07-07 09:23:32,334 - INFO - Checkpoint saved for qwen at question 110
2025-07-07 09:23:32,334 - INFO -   Progress: 110/881
2025-07-07 09:23:32,334 - INFO -   Running averages - F1: 0.003, Precision: 0.002, Recall: 0.009
2025-07-07 09:23:33,335 - INFO - Processing question 111/881 (ID: 119) for qwen
2025-07-07 09:23:33,336 - INFO -   Ground truth verses: ['3:121']
2025-07-07 09:23:33,336 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:23:49,048 - INFO - Extracted JSON from code block: 758 characters
2025-07-07 09:23:49,048 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-Hijr:53]' -> 'Al-Hijr:53'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-Maidah:52]' -> 'Al-Maidah:52'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-Tawbah:100]' -> 'Al-Tawbah:100'
2025-07-07 09:23:49,048 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-Hijr:53]' -> 'Al-Hijr:53'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-Hijr:53]' -> 'Al-Hijr:53'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-Maidah:52]' -> 'Al-Maidah:52'
2025-07-07 09:23:49,048 - WARNING - Invalid colon format: '[Al-Maidah:52]' -> 'Al-Maidah:52'
2025-07-07 09:23:49,049 - WARNING - Invalid colon format: '[Al-Tawbah:100]' -> 'Al-Tawbah:100'
2025-07-07 09:23:49,049 - WARNING - Invalid colon format: '[Al-Tawbah:100]' -> 'Al-Tawbah:100'
2025-07-07 09:23:49,049 - INFO - Normalized predicted verses: ['15:53', '2:255', '5:52', '6:84', '9:100']
2025-07-07 09:23:49,049 - INFO - Normalized ground truth verses: ['3:121']
2025-07-07 09:23:49,049 - INFO -   Response: 758 chars, Found 5 verses
2025-07-07 09:23:49,049 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:23:49,049 - INFO -   Matched verses: []
2025-07-07 09:23:50,050 - INFO - Processing question 112/881 (ID: 120) for qwen
2025-07-07 09:23:50,050 - INFO -   Ground truth verses: ['3:121']
2025-07-07 09:23:50,050 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:24:05,787 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 09:24:05,787 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:24:05,787 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:24:05,787 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:24:05,787 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:24:05,787 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:24:05,787 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:24:05,787 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:24:05,787 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:24:05,787 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:24:05,787 - INFO - Normalized ground truth verses: ['3:121']
2025-07-07 09:24:05,787 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 09:24:05,787 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:24:05,787 - INFO -   Matched verses: []
2025-07-07 09:24:06,788 - INFO - Processing question 113/881 (ID: 121) for qwen
2025-07-07 09:24:06,789 - INFO -   Ground truth verses: ['3:121']
2025-07-07 09:24:06,789 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:24:22,574 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:24:22,575 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:24:22,575 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:24:22,575 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:24:22,575 - INFO - Normalized predicted verses: []
2025-07-07 09:24:22,575 - INFO - Normalized ground truth verses: ['3:121']
2025-07-07 09:24:22,575 - INFO -   Response: 4472 chars, Found 0 verses
2025-07-07 09:24:22,575 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:24:22,575 - INFO -   Matched verses: []
2025-07-07 09:24:23,576 - INFO - Processing question 114/881 (ID: 122) for qwen
2025-07-07 09:24:23,576 - INFO -   Ground truth verses: ['3:121']
2025-07-07 09:24:23,576 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:24:39,231 - INFO - Extracted JSON from code block: 765 characters
2025-07-07 09:24:39,231 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Anfal:86]' -> 'Al-Anfal:86'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-An'am:124]' -> 'Al-An'am:124'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Mumtahanah:56]' -> 'Al-Mumtahanah:56'
2025-07-07 09:24:39,231 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Anfal:86]' -> 'Al-Anfal:86'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Anfal:86]' -> 'Al-Anfal:86'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-An'am:124]' -> 'Al-An'am:124'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-An'am:124]' -> 'Al-An'am:124'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Mumtahanah:56]' -> 'Al-Mumtahanah:56'
2025-07-07 09:24:39,231 - WARNING - Invalid colon format: '[Al-Mumtahanah:56]' -> 'Al-Mumtahanah:56'
2025-07-07 09:24:39,231 - INFO - Normalized predicted verses: ['2:255', '49:11', '4:86', '6:124']
2025-07-07 09:24:39,231 - INFO - Normalized ground truth verses: ['3:121']
2025-07-07 09:24:39,231 - INFO -   Response: 765 chars, Found 5 verses
2025-07-07 09:24:39,231 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:24:39,231 - INFO -   Matched verses: []
2025-07-07 09:24:40,233 - INFO - Processing question 115/881 (ID: 123) for qwen
2025-07-07 09:24:40,233 - INFO -   Ground truth verses: ['3:121']
2025-07-07 09:24:40,233 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:24:55,825 - INFO - Extracted JSON from code block: 20 characters
2025-07-07 09:24:55,825 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:24:55,825 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:24:55,825 - INFO - Normalized predicted verses: []
2025-07-07 09:24:55,825 - INFO - Normalized ground truth verses: ['3:121']
2025-07-07 09:24:55,826 - INFO -   Response: 20 chars, Found 0 verses
2025-07-07 09:24:55,826 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:24:55,826 - INFO -   Matched verses: []
2025-07-07 09:24:55,839 - INFO - Saved data to results/qwen_checkpoint_114.json
2025-07-07 09:24:55,839 - INFO - Checkpoint saved for qwen at question 115
2025-07-07 09:24:55,839 - INFO -   Progress: 115/881
2025-07-07 09:24:55,839 - INFO -   Running averages - F1: 0.003, Precision: 0.002, Recall: 0.009
2025-07-07 09:24:56,840 - INFO - Processing question 116/881 (ID: 124) for qwen
2025-07-07 09:24:56,840 - INFO -   Ground truth verses: ['39:39', '6:135', '11:93', '11:121']
2025-07-07 09:24:56,840 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:25:12,666 - INFO - Extracted JSON from code block: 757 characters
2025-07-07 09:25:12,667 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Tawbah:103]' -> 'Al-Tawbah:103'
2025-07-07 09:25:12,667 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Tawbah:103]' -> 'Al-Tawbah:103'
2025-07-07 09:25:12,667 - WARNING - Invalid colon format: '[Al-Tawbah:103]' -> 'Al-Tawbah:103'
2025-07-07 09:25:12,667 - INFO - Normalized predicted verses: ['15:6', '2:255', '4:89', '5:10', '9:103']
2025-07-07 09:25:12,667 - INFO - Normalized ground truth verses: ['11:121', '11:93', '39:39', '6:135']
2025-07-07 09:25:12,667 - INFO -   Response: 757 chars, Found 5 verses
2025-07-07 09:25:12,667 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:25:12,667 - INFO -   Matched verses: []
2025-07-07 09:25:13,668 - INFO - Processing question 117/881 (ID: 125) for qwen
2025-07-07 09:25:13,669 - INFO -   Ground truth verses: ['39:39', '6:135', '11:93', '11:121']
2025-07-07 09:25:13,669 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:25:29,268 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:25:29,268 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:25:29,268 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:25:29,268 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:25:29,268 - INFO - Normalized predicted verses: []
2025-07-07 09:25:29,268 - INFO - Normalized ground truth verses: ['11:121', '11:93', '39:39', '6:135']
2025-07-07 09:25:29,268 - INFO -   Response: 4568 chars, Found 0 verses
2025-07-07 09:25:29,268 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:25:29,268 - INFO -   Matched verses: []
2025-07-07 09:25:30,269 - INFO - Processing question 118/881 (ID: 126) for qwen
2025-07-07 09:25:30,270 - INFO -   Ground truth verses: ['39:39', '6:135', '11:93', '11:121']
2025-07-07 09:25:30,270 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:25:45,824 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:25:45,824 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:25:45,824 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:25:45,824 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:25:45,824 - INFO - Normalized predicted verses: []
2025-07-07 09:25:45,825 - INFO - Normalized ground truth verses: ['11:121', '11:93', '39:39', '6:135']
2025-07-07 09:25:45,825 - INFO -   Response: 4679 chars, Found 0 verses
2025-07-07 09:25:45,825 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:25:45,825 - INFO -   Matched verses: []
2025-07-07 09:25:46,826 - INFO - Processing question 119/881 (ID: 128) for qwen
2025-07-07 09:25:46,826 - INFO -   Ground truth verses: ['39:39', '6:135', '11:93', '11:121']
2025-07-07 09:25:46,826 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:26:02,316 - INFO - Extracted JSON from code block: 756 characters
2025-07-07 09:26:02,316 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:26:02,316 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:26:02,316 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:26:02,316 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:26:02,316 - WARNING - Invalid colon format: '[Al-Ma'idah:44]' -> 'Al-Ma'idah:44'
2025-07-07 09:26:02,316 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:26:02,316 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:26:02,316 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:26:02,316 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:26:02,316 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:26:02,316 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:26:02,316 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:26:02,317 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:26:02,317 - WARNING - Invalid colon format: '[Al-Ma'idah:44]' -> 'Al-Ma'idah:44'
2025-07-07 09:26:02,317 - WARNING - Invalid colon format: '[Al-Ma'idah:44]' -> 'Al-Ma'idah:44'
2025-07-07 09:26:02,317 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:26:02,317 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:26:02,317 - INFO - Normalized predicted verses: ['15:7', '2:255', '5:44', '6:89', '7:113']
2025-07-07 09:26:02,317 - INFO - Normalized ground truth verses: ['11:121', '11:93', '39:39', '6:135']
2025-07-07 09:26:02,317 - INFO -   Response: 756 chars, Found 5 verses
2025-07-07 09:26:02,317 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:26:02,317 - INFO -   Matched verses: []
2025-07-07 09:26:03,318 - INFO - Processing question 120/881 (ID: 129) for qwen
2025-07-07 09:26:03,318 - INFO -   Ground truth verses: ['26:213', '28:70', '23:117', '25:68', '15:96', '51:51', '28:88']
2025-07-07 09:26:03,318 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:26:19,095 - INFO - Extracted JSON from code block: 459 characters
2025-07-07 09:26:19,095 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:26:19,095 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:26:19,095 - INFO - Normalized predicted verses: []
2025-07-07 09:26:19,095 - INFO - Normalized ground truth verses: ['15:96', '23:117', '25:68', '26:213', '28:70', '28:88', '51:51']
2025-07-07 09:26:19,095 - INFO -   Response: 459 chars, Found 0 verses
2025-07-07 09:26:19,095 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:26:19,095 - INFO -   Matched verses: []
2025-07-07 09:26:19,110 - INFO - Saved data to results/qwen_checkpoint_119.json
2025-07-07 09:26:19,110 - INFO - Checkpoint saved for qwen at question 120
2025-07-07 09:26:19,110 - INFO -   Progress: 120/881
2025-07-07 09:26:19,110 - INFO -   Running averages - F1: 0.003, Precision: 0.002, Recall: 0.008
2025-07-07 09:26:20,111 - INFO - Processing question 121/881 (ID: 130) for qwen
2025-07-07 09:26:20,111 - INFO -   Ground truth verses: ['26:213', '28:70', '23:117', '25:68', '15:96', '51:51', '28:88']
2025-07-07 09:26:20,111 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:26:35,526 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:26:35,527 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:26:35,527 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:26:35,527 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:26:35,527 - INFO - Normalized predicted verses: []
2025-07-07 09:26:35,527 - INFO - Normalized ground truth verses: ['15:96', '23:117', '25:68', '26:213', '28:70', '28:88', '51:51']
2025-07-07 09:26:35,527 - INFO -   Response: 4067 chars, Found 0 verses
2025-07-07 09:26:35,527 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:26:35,527 - INFO -   Matched verses: []
2025-07-07 09:26:36,528 - INFO - Processing question 122/881 (ID: 131) for qwen
2025-07-07 09:26:36,528 - INFO -   Ground truth verses: ['26:213', '28:70', '23:117', '25:68', '15:96', '51:51', '28:88']
2025-07-07 09:26:36,528 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:26:51,968 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:26:51,968 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:26:51,968 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:26:51,968 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:26:51,968 - INFO - Normalized predicted verses: []
2025-07-07 09:26:51,968 - INFO - Normalized ground truth verses: ['15:96', '23:117', '25:68', '26:213', '28:70', '28:88', '51:51']
2025-07-07 09:26:51,968 - INFO -   Response: 4383 chars, Found 0 verses
2025-07-07 09:26:51,968 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:26:51,968 - INFO -   Matched verses: []
2025-07-07 09:26:52,969 - INFO - Processing question 123/881 (ID: 132) for qwen
2025-07-07 09:26:52,970 - INFO -   Ground truth verses: ['26:213', '28:70', '23:117', '25:68', '15:96', '51:51', '28:88']
2025-07-07 09:26:52,970 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:27:08,388 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 09:27:08,388 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:27:08,388 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:27:08,389 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:27:08,389 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:27:08,389 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:27:08,389 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:27:08,389 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:27:08,389 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:27:08,389 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:27:08,389 - INFO - Normalized ground truth verses: ['15:96', '23:117', '25:68', '26:213', '28:70', '28:88', '51:51']
2025-07-07 09:27:08,389 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 09:27:08,389 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:27:08,389 - INFO -   Matched verses: []
2025-07-07 09:27:09,390 - INFO - Processing question 124/881 (ID: 139) for qwen
2025-07-07 09:27:09,390 - INFO -   Ground truth verses: ['57:5', '3:109', '8:44']
2025-07-07 09:27:09,390 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:27:24,934 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:27:24,935 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:27:24,935 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:27:24,935 - INFO - Normalized predicted verses: []
2025-07-07 09:27:24,935 - INFO - Normalized ground truth verses: ['3:109', '57:5', '8:44']
2025-07-07 09:27:24,935 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:27:24,935 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:27:24,935 - INFO -   Matched verses: []
2025-07-07 09:27:25,936 - INFO - Processing question 125/881 (ID: 140) for qwen
2025-07-07 09:27:25,936 - INFO -   Ground truth verses: ['57:5', '3:109', '8:44']
2025-07-07 09:27:25,936 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:27:41,385 - INFO - Extracted JSON from code block: 756 characters
2025-07-07 09:27:41,385 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Hijr:52]' -> 'Al-Hijr:52'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Maidah:52]' -> 'Al-Maidah:52'
2025-07-07 09:27:41,385 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Hijr:52]' -> 'Al-Hijr:52'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Hijr:52]' -> 'Al-Hijr:52'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Maidah:52]' -> 'Al-Maidah:52'
2025-07-07 09:27:41,385 - WARNING - Invalid colon format: '[Al-Maidah:52]' -> 'Al-Maidah:52'
2025-07-07 09:27:41,386 - INFO - Normalized predicted verses: ['15:52', '2:255', '4:64', '55:55', '5:52']
2025-07-07 09:27:41,386 - INFO - Normalized ground truth verses: ['3:109', '57:5', '8:44']
2025-07-07 09:27:41,386 - INFO -   Response: 756 chars, Found 5 verses
2025-07-07 09:27:41,386 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:27:41,386 - INFO -   Matched verses: []
2025-07-07 09:27:41,400 - INFO - Saved data to results/qwen_checkpoint_124.json
2025-07-07 09:27:41,400 - INFO - Checkpoint saved for qwen at question 125
2025-07-07 09:27:41,400 - INFO -   Progress: 125/881
2025-07-07 09:27:41,400 - INFO -   Running averages - F1: 0.003, Precision: 0.002, Recall: 0.008
2025-07-07 09:27:42,401 - INFO - Processing question 126/881 (ID: 141) for qwen
2025-07-07 09:27:42,401 - INFO -   Ground truth verses: ['57:5', '3:109', '8:44']
2025-07-07 09:27:42,401 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:27:58,000 - INFO - Extracted JSON from code block: 753 characters
2025-07-07 09:27:58,000 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:27:58,000 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:27:58,000 - WARNING - Invalid colon format: '[Al-Hijr:53]' -> 'Al-Hijr:53'
2025-07-07 09:27:58,000 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:27:58,000 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:27:58,000 - WARNING - Invalid colon format: '[Al-Zumar:52]' -> 'Al-Zumar:52'
2025-07-07 09:27:58,000 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:27:58,000 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:27:58,000 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:27:58,001 - WARNING - Invalid colon format: '[Al-Hijr:53]' -> 'Al-Hijr:53'
2025-07-07 09:27:58,001 - WARNING - Invalid colon format: '[Al-Hijr:53]' -> 'Al-Hijr:53'
2025-07-07 09:27:58,001 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:27:58,001 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:27:58,001 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:27:58,001 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:27:58,001 - WARNING - Invalid colon format: '[Al-Zumar:52]' -> 'Al-Zumar:52'
2025-07-07 09:27:58,001 - WARNING - Invalid colon format: '[Al-Zumar:52]' -> 'Al-Zumar:52'
2025-07-07 09:27:58,001 - INFO - Normalized predicted verses: ['15:53', '39:52', '5:10', '6:84']
2025-07-07 09:27:58,001 - INFO - Normalized ground truth verses: ['3:109', '57:5', '8:44']
2025-07-07 09:27:58,001 - INFO -   Response: 753 chars, Found 5 verses
2025-07-07 09:27:58,001 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:27:58,001 - INFO -   Matched verses: []
2025-07-07 09:27:59,002 - INFO - Processing question 127/881 (ID: 142) for qwen
2025-07-07 09:27:59,002 - INFO -   Ground truth verses: ['57:5', '3:109', '8:44']
2025-07-07 09:27:59,002 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:28:14,508 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:28:14,508 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:28:14,508 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:28:14,508 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:28:14,508 - INFO - Normalized predicted verses: []
2025-07-07 09:28:14,508 - INFO - Normalized ground truth verses: ['3:109', '57:5', '8:44']
2025-07-07 09:28:14,508 - INFO -   Response: 5417 chars, Found 0 verses
2025-07-07 09:28:14,508 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:28:14,508 - INFO -   Matched verses: []
2025-07-07 09:28:15,509 - INFO - Processing question 128/881 (ID: 143) for qwen
2025-07-07 09:28:15,509 - INFO -   Ground truth verses: ['57:5', '3:109', '8:44']
2025-07-07 09:28:15,509 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:28:24,129 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 09:28:24,129 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:28:24,129 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:28:24,129 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:28:24,129 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:28:24,129 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:28:24,129 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:28:24,129 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:28:24,129 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:28:24,130 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:28:24,130 - INFO - Normalized ground truth verses: ['3:109', '57:5', '8:44']
2025-07-07 09:28:24,130 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 09:28:24,130 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:28:24,130 - INFO -   Matched verses: []
2025-07-07 09:28:25,131 - INFO - Processing question 129/881 (ID: 144) for qwen
2025-07-07 09:28:25,131 - INFO -   Ground truth verses: ['56:93', '26:101', '78:25', '56:42', '70:10']
2025-07-07 09:28:25,131 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:28:40,675 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:28:40,675 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:28:40,675 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:28:40,675 - INFO - Normalized predicted verses: []
2025-07-07 09:28:40,675 - INFO - Normalized ground truth verses: ['26:101', '56:42', '56:93', '70:10', '78:25']
2025-07-07 09:28:40,675 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:28:40,675 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:28:40,675 - INFO -   Matched verses: []
2025-07-07 09:28:41,676 - INFO - Processing question 130/881 (ID: 145) for qwen
2025-07-07 09:28:41,677 - INFO -   Ground truth verses: ['56:93', '26:101', '78:25', '56:42', '70:10']
2025-07-07 09:28:41,677 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:28:57,145 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:28:57,145 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:28:57,145 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:28:57,145 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:28:57,145 - INFO - Normalized predicted verses: []
2025-07-07 09:28:57,145 - INFO - Normalized ground truth verses: ['26:101', '56:42', '56:93', '70:10', '78:25']
2025-07-07 09:28:57,145 - INFO -   Response: 2119 chars, Found 0 verses
2025-07-07 09:28:57,145 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:28:57,145 - INFO -   Matched verses: []
2025-07-07 09:28:57,160 - INFO - Saved data to results/qwen_checkpoint_129.json
2025-07-07 09:28:57,160 - INFO - Checkpoint saved for qwen at question 130
2025-07-07 09:28:57,160 - INFO -   Progress: 130/881
2025-07-07 09:28:57,160 - INFO -   Running averages - F1: 0.003, Precision: 0.002, Recall: 0.008
2025-07-07 09:28:58,161 - INFO - Processing question 131/881 (ID: 146) for qwen
2025-07-07 09:28:58,161 - INFO -   Ground truth verses: ['56:93', '26:101', '78:25', '56:42', '70:10']
2025-07-07 09:28:58,161 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:29:13,664 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:29:13,664 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:29:13,664 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:29:13,664 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:29:13,664 - INFO - Normalized predicted verses: []
2025-07-07 09:29:13,664 - INFO - Normalized ground truth verses: ['26:101', '56:42', '56:93', '70:10', '78:25']
2025-07-07 09:29:13,664 - INFO -   Response: 4120 chars, Found 0 verses
2025-07-07 09:29:13,664 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:29:13,664 - INFO -   Matched verses: []
2025-07-07 09:29:14,665 - INFO - Processing question 132/881 (ID: 147) for qwen
2025-07-07 09:29:14,665 - INFO -   Ground truth verses: ['56:93', '26:101', '78:25', '56:42', '70:10']
2025-07-07 09:29:14,665 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:29:30,172 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:29:30,172 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:29:30,172 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:29:30,172 - INFO - Normalized predicted verses: []
2025-07-07 09:29:30,172 - INFO - Normalized ground truth verses: ['26:101', '56:42', '56:93', '70:10', '78:25']
2025-07-07 09:29:30,172 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:29:30,172 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:29:30,172 - INFO -   Matched verses: []
2025-07-07 09:29:31,174 - INFO - Processing question 133/881 (ID: 148) for qwen
2025-07-07 09:29:31,174 - INFO -   Ground truth verses: ['56:93', '26:101', '78:25', '56:42', '70:10']
2025-07-07 09:29:31,174 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:29:46,685 - INFO - Extracted JSON from code block: 763 characters
2025-07-07 09:29:46,685 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:29:46,685 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:29:46,685 - WARNING - Invalid colon format: '[Al-An'am:164]' -> 'Al-An'am:164'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Ma'idah:18]' -> 'Al-Ma'idah:18'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Zumar:10]' -> 'Al-Zumar:10'
2025-07-07 09:29:46,686 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-An'am:164]' -> 'Al-An'am:164'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-An'am:164]' -> 'Al-An'am:164'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Ma'idah:18]' -> 'Al-Ma'idah:18'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Ma'idah:18]' -> 'Al-Ma'idah:18'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Zumar:10]' -> 'Al-Zumar:10'
2025-07-07 09:29:46,686 - WARNING - Invalid colon format: '[Al-Zumar:10]' -> 'Al-Zumar:10'
2025-07-07 09:29:46,686 - INFO - Normalized predicted verses: ['2:255', '39:10', '50:11', '5:18']
2025-07-07 09:29:46,686 - INFO - Normalized ground truth verses: ['26:101', '56:42', '56:93', '70:10', '78:25']
2025-07-07 09:29:46,686 - INFO -   Response: 763 chars, Found 5 verses
2025-07-07 09:29:46,686 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:29:46,686 - INFO -   Matched verses: []
2025-07-07 09:29:47,687 - INFO - Processing question 134/881 (ID: 149) for qwen
2025-07-07 09:29:47,687 - INFO -   Ground truth verses: ['6:160', '27:89', '27:90', '28:84']
2025-07-07 09:29:47,687 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:30:03,272 - INFO - Extracted JSON from code block: 101 characters
2025-07-07 09:30:03,273 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:30:03,273 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:30:03,273 - INFO - Normalized predicted verses: []
2025-07-07 09:30:03,273 - INFO - Normalized ground truth verses: ['27:89', '27:90', '28:84', '6:160']
2025-07-07 09:30:03,273 - INFO -   Response: 101 chars, Found 0 verses
2025-07-07 09:30:03,273 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:30:03,273 - INFO -   Matched verses: []
2025-07-07 09:30:04,274 - INFO - Processing question 135/881 (ID: 150) for qwen
2025-07-07 09:30:04,274 - INFO -   Ground truth verses: ['6:160', '27:89', '27:90', '28:84']
2025-07-07 09:30:04,274 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:30:19,853 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:30:19,853 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:30:19,853 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:30:19,853 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:30:19,853 - INFO - Normalized predicted verses: []
2025-07-07 09:30:19,853 - INFO - Normalized ground truth verses: ['27:89', '27:90', '28:84', '6:160']
2025-07-07 09:30:19,853 - INFO -   Response: 3880 chars, Found 0 verses
2025-07-07 09:30:19,853 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:30:19,853 - INFO -   Matched verses: []
2025-07-07 09:30:19,868 - INFO - Saved data to results/qwen_checkpoint_134.json
2025-07-07 09:30:19,868 - INFO - Checkpoint saved for qwen at question 135
2025-07-07 09:30:19,868 - INFO -   Progress: 135/881
2025-07-07 09:30:19,868 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.007
2025-07-07 09:30:20,869 - INFO - Processing question 136/881 (ID: 151) for qwen
2025-07-07 09:30:20,869 - INFO -   Ground truth verses: ['6:160', '27:89', '27:90', '28:84']
2025-07-07 09:30:20,869 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:30:36,308 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:30:36,308 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:30:36,308 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:30:36,308 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:30:36,308 - INFO - Normalized predicted verses: []
2025-07-07 09:30:36,308 - INFO - Normalized ground truth verses: ['27:89', '27:90', '28:84', '6:160']
2025-07-07 09:30:36,308 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 09:30:36,308 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:30:36,308 - INFO -   Matched verses: []
2025-07-07 09:30:37,310 - INFO - Processing question 137/881 (ID: 152) for qwen
2025-07-07 09:30:37,310 - INFO -   Ground truth verses: ['6:160', '27:89', '27:90', '28:84']
2025-07-07 09:30:37,310 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:30:52,946 - INFO - Extracted JSON from code block: 758 characters
2025-07-07 09:30:52,946 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:30:52,946 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:30:52,946 - WARNING - Invalid colon format: '[Al-An'am:164]' -> 'Al-An'am:164'
2025-07-07 09:30:52,946 - WARNING - Invalid colon format: '[Al-Ma'idah:115]' -> 'Al-Ma'idah:115'
2025-07-07 09:30:52,946 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:30:52,946 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:30:52,946 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:30:52,946 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:30:52,947 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:30:52,947 - WARNING - Invalid colon format: '[Al-An'am:164]' -> 'Al-An'am:164'
2025-07-07 09:30:52,947 - WARNING - Invalid colon format: '[Al-An'am:164]' -> 'Al-An'am:164'
2025-07-07 09:30:52,947 - WARNING - Invalid colon format: '[Al-Ma'idah:115]' -> 'Al-Ma'idah:115'
2025-07-07 09:30:52,947 - WARNING - Invalid colon format: '[Al-Ma'idah:115]' -> 'Al-Ma'idah:115'
2025-07-07 09:30:52,947 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:30:52,947 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:30:52,947 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:30:52,947 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:30:52,947 - INFO - Normalized predicted verses: ['15:7', '2:255', '57:5', '5:115']
2025-07-07 09:30:52,947 - INFO - Normalized ground truth verses: ['27:89', '27:90', '28:84', '6:160']
2025-07-07 09:30:52,947 - INFO -   Response: 758 chars, Found 5 verses
2025-07-07 09:30:52,947 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:30:52,947 - INFO -   Matched verses: []
2025-07-07 09:30:53,948 - INFO - Processing question 138/881 (ID: 153) for qwen
2025-07-07 09:30:53,948 - INFO -   Ground truth verses: ['6:160', '27:89', '27:90', '28:84']
2025-07-07 09:30:53,948 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:31:09,487 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:31:09,487 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:31:09,487 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:31:09,487 - INFO - Normalized predicted verses: []
2025-07-07 09:31:09,487 - INFO - Normalized ground truth verses: ['27:89', '27:90', '28:84', '6:160']
2025-07-07 09:31:09,487 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:31:09,487 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:31:09,487 - INFO -   Matched verses: []
2025-07-07 09:31:10,488 - INFO - Processing question 139/881 (ID: 154) for qwen
2025-07-07 09:31:10,489 - INFO -   Ground truth verses: ['16:65', '13:12', '29:63', '45:5', '2:164', '30:24']
2025-07-07 09:31:10,489 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:31:26,085 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:31:26,085 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:31:26,085 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:31:26,085 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:31:26,085 - INFO - Normalized predicted verses: []
2025-07-07 09:31:26,085 - INFO - Normalized ground truth verses: ['13:12', '16:65', '29:63', '2:164', '30:24', '45:5']
2025-07-07 09:31:26,085 - INFO -   Response: 4420 chars, Found 0 verses
2025-07-07 09:31:26,085 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:31:26,085 - INFO -   Matched verses: []
2025-07-07 09:31:27,087 - INFO - Processing question 140/881 (ID: 155) for qwen
2025-07-07 09:31:27,087 - INFO -   Ground truth verses: ['16:65', '13:12', '29:63', '45:5', '2:164', '30:24']
2025-07-07 09:31:27,087 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:31:42,660 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:31:42,661 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:31:42,661 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:31:42,661 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:31:42,661 - INFO - Normalized predicted verses: []
2025-07-07 09:31:42,661 - INFO - Normalized ground truth verses: ['13:12', '16:65', '29:63', '2:164', '30:24', '45:5']
2025-07-07 09:31:42,661 - INFO -   Response: 5396 chars, Found 0 verses
2025-07-07 09:31:42,661 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:31:42,661 - INFO -   Matched verses: []
2025-07-07 09:31:42,676 - INFO - Saved data to results/qwen_checkpoint_139.json
2025-07-07 09:31:42,676 - INFO - Checkpoint saved for qwen at question 140
2025-07-07 09:31:42,676 - INFO -   Progress: 140/881
2025-07-07 09:31:42,676 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.007
2025-07-07 09:31:43,677 - INFO - Processing question 141/881 (ID: 156) for qwen
2025-07-07 09:31:43,678 - INFO -   Ground truth verses: ['16:65', '13:12', '29:63', '45:5', '2:164', '30:24']
2025-07-07 09:31:43,678 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:31:59,207 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:31:59,207 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:31:59,207 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:31:59,208 - INFO - Normalized predicted verses: []
2025-07-07 09:31:59,208 - INFO - Normalized ground truth verses: ['13:12', '16:65', '29:63', '2:164', '30:24', '45:5']
2025-07-07 09:31:59,208 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:31:59,208 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:31:59,208 - INFO -   Matched verses: []
2025-07-07 09:32:00,209 - INFO - Processing question 142/881 (ID: 157) for qwen
2025-07-07 09:32:00,209 - INFO -   Ground truth verses: ['16:65', '13:12', '29:63', '45:5', '2:164', '30:24']
2025-07-07 09:32:00,209 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:32:15,699 - INFO - Extracted JSON from code block: 322 characters
2025-07-07 09:32:15,700 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:32:15,700 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:32:15,700 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:32:15,700 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:32:15,700 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:32:15,700 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:32:15,700 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:32:15,700 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:32:15,700 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:32:15,700 - INFO - Normalized ground truth verses: ['13:12', '16:65', '29:63', '2:164', '30:24', '45:5']
2025-07-07 09:32:15,700 - INFO -   Response: 322 chars, Found 2 verses
2025-07-07 09:32:15,700 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:32:15,700 - INFO -   Matched verses: []
2025-07-07 09:32:16,701 - INFO - Processing question 143/881 (ID: 158) for qwen
2025-07-07 09:32:16,702 - INFO -   Ground truth verses: ['16:65', '13:12', '29:63', '45:5', '2:164', '30:24']
2025-07-07 09:32:16,702 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:32:32,242 - INFO - Extracted JSON from code block: 739 characters
2025-07-07 09:32:32,242 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:32:32,242 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:32:32,242 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:32:32,242 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:32:32,242 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:32:32,242 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:32:32,242 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:32:32,243 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:32:32,243 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:32:32,243 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:32:32,243 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:32:32,243 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:32:32,243 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:32:32,243 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:32:32,243 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:32:32,243 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:32:32,243 - INFO - Normalized predicted verses: ['6:140', '6:141', '6:145', '6:146', '6:147']
2025-07-07 09:32:32,243 - INFO - Normalized ground truth verses: ['13:12', '16:65', '29:63', '2:164', '30:24', '45:5']
2025-07-07 09:32:32,243 - INFO -   Response: 739 chars, Found 5 verses
2025-07-07 09:32:32,243 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:32:32,243 - INFO -   Matched verses: []
2025-07-07 09:32:33,244 - INFO - Processing question 144/881 (ID: 159) for qwen
2025-07-07 09:32:33,244 - INFO -   Ground truth verses: ['9:72', '9:89', '4:13', '85:11', '20:76', '64:9', '5:119', '48:5', '5:85', '4:57', '57:12', '4:122', '22:14', '3:136', '61:12']
2025-07-07 09:32:33,244 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:32:48,752 - INFO - Extracted JSON from code block: 313 characters
2025-07-07 09:32:48,752 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:32:48,753 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:32:48,753 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:32:48,753 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 09:32:48,753 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 09:32:48,753 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:32:48,753 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:32:48,753 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:32:48,753 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:32:48,753 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:32:48,753 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 09:32:48,753 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 09:32:48,753 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 09:32:48,753 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 09:32:48,753 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:32:48,753 - INFO - Normalized ground truth verses: ['20:76', '22:14', '3:136', '48:5', '4:122', '4:13', '4:57', '57:12', '5:119', '5:85', '61:12', '64:9', '85:11', '9:72', '9:89']
2025-07-07 09:32:48,753 - INFO -   Response: 313 chars, Found 2 verses
2025-07-07 09:32:48,753 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:32:48,753 - INFO -   Matched verses: []
2025-07-07 09:32:49,754 - INFO - Processing question 145/881 (ID: 160) for qwen
2025-07-07 09:32:49,754 - INFO -   Ground truth verses: ['9:72', '9:89', '4:13', '85:11', '20:76', '64:9', '5:119', '48:5', '5:85', '4:57', '57:12', '4:122', '22:14', '3:136', '61:12']
2025-07-07 09:32:49,754 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:33:05,217 - INFO - Extracted JSON from code block: 128 characters
2025-07-07 09:33:05,217 - WARNING - Failed to parse JSON response: No JSON object found, falling back to regex parsing
2025-07-07 09:33:05,217 - WARNING - Failed to parse JSON response: No JSON object found, falling back to regex parsing
2025-07-07 09:33:05,217 - WARNING - Failed to parse JSON response: No JSON object found, falling back to regex parsing
2025-07-07 09:33:05,217 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:33:05,217 - INFO - Normalized predicted verses: []
2025-07-07 09:33:05,217 - INFO - Normalized ground truth verses: ['20:76', '22:14', '3:136', '48:5', '4:122', '4:13', '4:57', '57:12', '5:119', '5:85', '61:12', '64:9', '85:11', '9:72', '9:89']
2025-07-07 09:33:05,217 - INFO -   Response: 128 chars, Found 0 verses
2025-07-07 09:33:05,217 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:33:05,217 - INFO -   Matched verses: []
2025-07-07 09:33:05,233 - INFO - Saved data to results/qwen_checkpoint_144.json
2025-07-07 09:33:05,233 - INFO - Checkpoint saved for qwen at question 145
2025-07-07 09:33:05,233 - INFO -   Progress: 145/881
2025-07-07 09:33:05,233 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.007
2025-07-07 09:33:06,234 - INFO - Processing question 146/881 (ID: 161) for qwen
2025-07-07 09:33:06,234 - INFO -   Ground truth verses: ['9:72', '9:89', '4:13', '85:11', '20:76', '64:9', '5:119', '48:5', '5:85', '4:57', '57:12', '4:122', '22:14', '3:136', '61:12']
2025-07-07 09:33:06,234 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:33:21,827 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Ma'idah:45]' -> 'Al-Ma'idah:45'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Hajj:11]' -> 'Al-Hajj:11'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Furqan:70]' -> 'Al-Furqan:70'
2025-07-07 09:33:21,827 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Ma'idah:45]' -> 'Al-Ma'idah:45'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Ma'idah:45]' -> 'Al-Ma'idah:45'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Hajj:11]' -> 'Al-Hajj:11'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Hajj:11]' -> 'Al-Hajj:11'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Furqan:70]' -> 'Al-Furqan:70'
2025-07-07 09:33:21,827 - WARNING - Invalid colon format: '[Al-Furqan:70]' -> 'Al-Furqan:70'
2025-07-07 09:33:21,828 - INFO - Normalized predicted verses: ['22:11', '25:70', '2:255', '5:45', '6:89']
2025-07-07 09:33:21,828 - INFO - Normalized ground truth verses: ['20:76', '22:14', '3:136', '48:5', '4:122', '4:13', '4:57', '57:12', '5:119', '5:85', '61:12', '64:9', '85:11', '9:72', '9:89']
2025-07-07 09:33:21,828 - INFO -   Response: 4191 chars, Found 5 verses
2025-07-07 09:33:21,828 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:33:21,828 - INFO -   Matched verses: []
2025-07-07 09:33:22,829 - INFO - Processing question 147/881 (ID: 162) for qwen
2025-07-07 09:33:22,829 - INFO -   Ground truth verses: ['9:72', '9:89', '4:13', '85:11', '20:76', '64:9', '5:119', '48:5', '5:85', '4:57', '57:12', '4:122', '22:14', '3:136', '61:12']
2025-07-07 09:33:22,829 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:33:38,333 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:33:38,333 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:33:38,333 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:33:38,333 - INFO - Normalized predicted verses: []
2025-07-07 09:33:38,333 - INFO - Normalized ground truth verses: ['20:76', '22:14', '3:136', '48:5', '4:122', '4:13', '4:57', '57:12', '5:119', '5:85', '61:12', '64:9', '85:11', '9:72', '9:89']
2025-07-07 09:33:38,333 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:33:38,333 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:33:38,333 - INFO -   Matched verses: []
2025-07-07 09:33:39,334 - INFO - Processing question 148/881 (ID: 163) for qwen
2025-07-07 09:33:39,335 - INFO -   Ground truth verses: ['9:72', '9:89', '4:13', '85:11', '20:76', '64:9', '5:119', '48:5', '5:85', '4:57', '57:12', '4:122', '22:14', '3:136', '61:12']
2025-07-07 09:33:39,335 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:33:54,832 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 877), falling back to regex parsing
2025-07-07 09:33:54,833 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 877), falling back to regex parsing
2025-07-07 09:33:54,833 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 877), falling back to regex parsing
2025-07-07 09:33:54,833 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:33:54,833 - INFO - Normalized predicted verses: []
2025-07-07 09:33:54,833 - INFO - Normalized ground truth verses: ['20:76', '22:14', '3:136', '48:5', '4:122', '4:13', '4:57', '57:12', '5:119', '5:85', '61:12', '64:9', '85:11', '9:72', '9:89']
2025-07-07 09:33:54,833 - INFO -   Response: 4549 chars, Found 0 verses
2025-07-07 09:33:54,833 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:33:54,833 - INFO -   Matched verses: []
2025-07-07 09:33:55,834 - INFO - Processing question 149/881 (ID: 164) for qwen
2025-07-07 09:33:55,834 - INFO -   Ground truth verses: ['21:67', '16:73', '21:98', '29:17']
2025-07-07 09:33:55,834 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:34:11,318 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:34:11,318 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:34:11,318 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:34:11,318 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:34:11,318 - INFO - Normalized predicted verses: []
2025-07-07 09:34:11,318 - INFO - Normalized ground truth verses: ['16:73', '21:67', '21:98', '29:17']
2025-07-07 09:34:11,319 - INFO -   Response: 4328 chars, Found 0 verses
2025-07-07 09:34:11,319 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:34:11,319 - INFO -   Matched verses: []
2025-07-07 09:34:12,320 - INFO - Processing question 150/881 (ID: 165) for qwen
2025-07-07 09:34:12,320 - INFO -   Ground truth verses: ['21:67', '16:73', '21:98', '29:17']
2025-07-07 09:34:12,320 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:34:27,845 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:34:27,845 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:34:27,845 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:34:27,845 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:34:27,845 - INFO - Normalized predicted verses: []
2025-07-07 09:34:27,845 - INFO - Normalized ground truth verses: ['16:73', '21:67', '21:98', '29:17']
2025-07-07 09:34:27,845 - INFO -   Response: 4101 chars, Found 0 verses
2025-07-07 09:34:27,845 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:34:27,845 - INFO -   Matched verses: []
2025-07-07 09:34:27,861 - INFO - Saved data to results/qwen_checkpoint_149.json
2025-07-07 09:34:27,861 - INFO - Checkpoint saved for qwen at question 150
2025-07-07 09:34:27,861 - INFO -   Progress: 150/881
2025-07-07 09:34:27,861 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.007
2025-07-07 09:34:28,862 - INFO - Processing question 151/881 (ID: 166) for qwen
2025-07-07 09:34:28,862 - INFO -   Ground truth verses: ['21:67', '16:73', '21:98', '29:17']
2025-07-07 09:34:28,862 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:34:44,249 - INFO - Extracted JSON from code block: 313 characters
2025-07-07 09:34:44,249 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:34:44,249 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:34:44,249 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:34:44,249 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 09:34:44,249 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 09:34:44,249 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:34:44,249 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:34:44,249 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:34:44,249 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:34:44,249 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:34:44,249 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 09:34:44,249 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 09:34:44,249 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 09:34:44,249 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 09:34:44,249 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:34:44,249 - INFO - Normalized ground truth verses: ['16:73', '21:67', '21:98', '29:17']
2025-07-07 09:34:44,249 - INFO -   Response: 313 chars, Found 2 verses
2025-07-07 09:34:44,249 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:34:44,249 - INFO -   Matched verses: []
2025-07-07 09:34:45,251 - INFO - Processing question 152/881 (ID: 167) for qwen
2025-07-07 09:34:45,251 - INFO -   Ground truth verses: ['21:67', '16:73', '21:98', '29:17']
2025-07-07 09:34:45,251 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:35:00,647 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:35:00,647 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:35:00,648 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:35:00,648 - INFO - Normalized predicted verses: []
2025-07-07 09:35:00,648 - INFO - Normalized ground truth verses: ['16:73', '21:67', '21:98', '29:17']
2025-07-07 09:35:00,648 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:35:00,648 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:35:00,648 - INFO -   Matched verses: []
2025-07-07 09:35:01,649 - INFO - Processing question 153/881 (ID: 168) for qwen
2025-07-07 09:35:01,649 - INFO -   Ground truth verses: ['21:67', '16:73', '21:98', '29:17']
2025-07-07 09:35:01,649 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:35:17,433 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 09:35:17,433 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-An'am:103]' -> 'Al-An'am:103'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-An'am:104]' -> 'Al-An'am:104'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-An'am:105]' -> 'Al-An'am:105'
2025-07-07 09:35:17,433 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-An'am:103]' -> 'Al-An'am:103'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-An'am:103]' -> 'Al-An'am:103'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-An'am:104]' -> 'Al-An'am:104'
2025-07-07 09:35:17,433 - WARNING - Invalid colon format: '[Al-An'am:104]' -> 'Al-An'am:104'
2025-07-07 09:35:17,434 - WARNING - Invalid colon format: '[Al-An'am:105]' -> 'Al-An'am:105'
2025-07-07 09:35:17,434 - WARNING - Invalid colon format: '[Al-An'am:105]' -> 'Al-An'am:105'
2025-07-07 09:35:17,434 - INFO - Normalized predicted verses: ['2:255', '6:103', '6:104', '6:105', '6:84']
2025-07-07 09:35:17,434 - INFO - Normalized ground truth verses: ['16:73', '21:67', '21:98', '29:17']
2025-07-07 09:35:17,434 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 09:35:17,434 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:35:17,434 - INFO -   Matched verses: []
2025-07-07 09:35:18,435 - INFO - Processing question 154/881 (ID: 169) for qwen
2025-07-07 09:35:18,435 - INFO -   Ground truth verses: ['3:183']
2025-07-07 09:35:18,435 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:35:34,082 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:35:34,082 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:35:34,082 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:35:34,082 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:35:34,082 - INFO - Normalized predicted verses: []
2025-07-07 09:35:34,082 - INFO - Normalized ground truth verses: ['3:183']
2025-07-07 09:35:34,082 - INFO -   Response: 5101 chars, Found 0 verses
2025-07-07 09:35:34,082 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:35:34,082 - INFO -   Matched verses: []
2025-07-07 09:35:35,084 - INFO - Processing question 155/881 (ID: 170) for qwen
2025-07-07 09:35:35,084 - INFO -   Ground truth verses: ['3:183']
2025-07-07 09:35:35,084 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:35:50,715 - INFO - Extracted JSON from code block: 759 characters
2025-07-07 09:35:50,716 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-An'am:148]' -> 'Al-An'am:148'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:35:50,716 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-An'am:148]' -> 'Al-An'am:148'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-An'am:148]' -> 'Al-An'am:148'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Hijr:72]' -> 'Al-Hijr:72'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:35:50,716 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:35:50,716 - INFO - Normalized predicted verses: ['15:72', '2:255', '55:55', '5:52', '6:148']
2025-07-07 09:35:50,716 - INFO - Normalized ground truth verses: ['3:183']
2025-07-07 09:35:50,716 - INFO -   Response: 759 chars, Found 5 verses
2025-07-07 09:35:50,716 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:35:50,716 - INFO -   Matched verses: []
2025-07-07 09:35:50,732 - INFO - Saved data to results/qwen_checkpoint_154.json
2025-07-07 09:35:50,732 - INFO - Checkpoint saved for qwen at question 155
2025-07-07 09:35:50,732 - INFO -   Progress: 155/881
2025-07-07 09:35:50,732 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.006
2025-07-07 09:35:51,734 - INFO - Processing question 156/881 (ID: 171) for qwen
2025-07-07 09:35:51,734 - INFO -   Ground truth verses: ['3:183']
2025-07-07 09:35:51,734 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:36:07,287 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:36:07,287 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:36:07,288 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:36:07,288 - INFO - Normalized predicted verses: []
2025-07-07 09:36:07,288 - INFO - Normalized ground truth verses: ['3:183']
2025-07-07 09:36:07,288 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:36:07,288 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:36:07,288 - INFO -   Matched verses: []
2025-07-07 09:36:08,289 - INFO - Processing question 157/881 (ID: 172) for qwen
2025-07-07 09:36:08,289 - INFO -   Ground truth verses: ['3:183']
2025-07-07 09:36:08,289 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:36:23,961 - INFO - Extracted JSON from code block: 749 characters
2025-07-07 09:36:23,961 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 09:36:23,961 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 09:36:23,961 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 09:36:23,961 - INFO - Normalized predicted verses: ['6:84', '6:85', '6:86', '6:87', '6:88']
2025-07-07 09:36:23,961 - INFO - Normalized ground truth verses: ['3:183']
2025-07-07 09:36:23,961 - INFO -   Response: 749 chars, Found 5 verses
2025-07-07 09:36:23,961 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:36:23,961 - INFO -   Matched verses: []
2025-07-07 09:36:24,962 - INFO - Processing question 158/881 (ID: 173) for qwen
2025-07-07 09:36:24,963 - INFO -   Ground truth verses: ['3:183']
2025-07-07 09:36:24,963 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:36:40,555 - INFO - Extracted JSON from code block: 754 characters
2025-07-07 09:36:40,555 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:36:40,555 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:36:40,555 - WARNING - Invalid colon format: '[Al-Ma'idah:64]' -> 'Al-Ma'idah:64'
2025-07-07 09:36:40,555 - WARNING - Invalid colon format: '[Al-Hijr:56]' -> 'Al-Hijr:56'
2025-07-07 09:36:40,555 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:36:40,555 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:36:40,555 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:36:40,555 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:36:40,555 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:36:40,555 - WARNING - Invalid colon format: '[Al-Ma'idah:64]' -> 'Al-Ma'idah:64'
2025-07-07 09:36:40,556 - WARNING - Invalid colon format: '[Al-Ma'idah:64]' -> 'Al-Ma'idah:64'
2025-07-07 09:36:40,556 - WARNING - Invalid colon format: '[Al-Hijr:56]' -> 'Al-Hijr:56'
2025-07-07 09:36:40,556 - WARNING - Invalid colon format: '[Al-Hijr:56]' -> 'Al-Hijr:56'
2025-07-07 09:36:40,556 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:36:40,556 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:36:40,556 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:36:40,556 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:36:40,556 - INFO - Normalized predicted verses: ['15:56', '25:56', '4:89', '55:55', '5:64']
2025-07-07 09:36:40,556 - INFO - Normalized ground truth verses: ['3:183']
2025-07-07 09:36:40,556 - INFO -   Response: 754 chars, Found 5 verses
2025-07-07 09:36:40,556 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:36:40,556 - INFO -   Matched verses: []
2025-07-07 09:36:41,557 - INFO - Processing question 159/881 (ID: 174) for qwen
2025-07-07 09:36:41,557 - INFO -   Ground truth verses: ['2:204']
2025-07-07 09:36:41,557 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:36:57,140 - INFO - Extracted JSON from code block: 753 characters
2025-07-07 09:36:57,141 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[At-Tawbah:124]' -> 'At-Tawbah:124'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Qaf:12]' -> 'Qaf:12'
2025-07-07 09:36:57,141 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[At-Tawbah:124]' -> 'At-Tawbah:124'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[At-Tawbah:124]' -> 'At-Tawbah:124'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Qaf:12]' -> 'Qaf:12'
2025-07-07 09:36:57,141 - WARNING - Invalid colon format: '[Qaf:12]' -> 'Qaf:12'
2025-07-07 09:36:57,141 - INFO - Normalized predicted verses: ['15:10', '2:255', '50:12', '6:84', '9:124']
2025-07-07 09:36:57,141 - INFO - Normalized ground truth verses: ['2:204']
2025-07-07 09:36:57,141 - INFO -   Response: 753 chars, Found 5 verses
2025-07-07 09:36:57,141 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:36:57,141 - INFO -   Matched verses: []
2025-07-07 09:36:58,142 - INFO - Processing question 160/881 (ID: 175) for qwen
2025-07-07 09:36:58,142 - INFO -   Ground truth verses: ['2:204']
2025-07-07 09:36:58,143 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:37:13,839 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 09:37:13,839 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:37:13,839 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:37:13,839 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:37:13,839 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-Hujurat:15]' -> 'Al-Hujurat:15'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-Qamar:19]' -> 'Al-Qamar:19'
2025-07-07 09:37:13,840 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-Hujurat:15]' -> 'Al-Hujurat:15'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-Hujurat:15]' -> 'Al-Hujurat:15'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-Qamar:19]' -> 'Al-Qamar:19'
2025-07-07 09:37:13,840 - WARNING - Invalid colon format: '[Al-Qamar:19]' -> 'Al-Qamar:19'
2025-07-07 09:37:13,840 - INFO - Normalized predicted verses: ['2:255', '49:15', '4:82', '55:19', '5:16']
2025-07-07 09:37:13,840 - INFO - Normalized ground truth verses: ['2:204']
2025-07-07 09:37:13,840 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 09:37:13,840 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:37:13,840 - INFO -   Matched verses: []
2025-07-07 09:37:13,858 - INFO - Saved data to results/qwen_checkpoint_159.json
2025-07-07 09:37:13,858 - INFO - Checkpoint saved for qwen at question 160
2025-07-07 09:37:13,858 - INFO -   Progress: 160/881
2025-07-07 09:37:13,858 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.006
2025-07-07 09:37:14,859 - INFO - Processing question 161/881 (ID: 176) for qwen
2025-07-07 09:37:14,859 - INFO -   Ground truth verses: ['2:204']
2025-07-07 09:37:14,859 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:37:30,941 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:37:30,942 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:37:30,942 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:37:30,942 - INFO - Normalized predicted verses: []
2025-07-07 09:37:30,942 - INFO - Normalized ground truth verses: ['2:204']
2025-07-07 09:37:30,942 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:37:30,942 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:37:30,942 - INFO -   Matched verses: []
2025-07-07 09:37:31,943 - INFO - Processing question 162/881 (ID: 177) for qwen
2025-07-07 09:37:31,943 - INFO -   Ground truth verses: ['2:204']
2025-07-07 09:37:31,943 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:37:47,825 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 09:37:47,825 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Hujurat:15]' -> 'Al-Hujurat:15'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Tawbah:10]' -> 'Al-Tawbah:10'
2025-07-07 09:37:47,825 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Hujurat:15]' -> 'Al-Hujurat:15'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Hujurat:15]' -> 'Al-Hujurat:15'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Tawbah:10]' -> 'Al-Tawbah:10'
2025-07-07 09:37:47,825 - WARNING - Invalid colon format: '[Al-Tawbah:10]' -> 'Al-Tawbah:10'
2025-07-07 09:37:47,825 - INFO - Normalized predicted verses: ['2:255', '49:15', '4:82', '5:16', '9:10']
2025-07-07 09:37:47,825 - INFO - Normalized ground truth verses: ['2:204']
2025-07-07 09:37:47,825 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 09:37:47,825 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:37:47,826 - INFO -   Matched verses: []
2025-07-07 09:37:48,827 - INFO - Processing question 163/881 (ID: 178) for qwen
2025-07-07 09:37:48,827 - INFO -   Ground truth verses: ['2:204']
2025-07-07 09:37:48,827 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:38:04,640 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:38:04,640 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:38:04,640 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:38:04,640 - INFO - Normalized predicted verses: []
2025-07-07 09:38:04,640 - INFO - Normalized ground truth verses: ['2:204']
2025-07-07 09:38:04,640 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:38:04,640 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:38:04,640 - INFO -   Matched verses: []
2025-07-07 09:38:05,641 - INFO - Processing question 164/881 (ID: 179) for qwen
2025-07-07 09:38:05,642 - INFO -   Ground truth verses: ['2:96']
2025-07-07 09:38:05,642 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:38:21,561 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:38:21,561 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:38:21,561 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:38:21,561 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:38:21,561 - INFO - Normalized predicted verses: []
2025-07-07 09:38:21,561 - INFO - Normalized ground truth verses: ['2:96']
2025-07-07 09:38:21,561 - INFO -   Response: 4540 chars, Found 0 verses
2025-07-07 09:38:21,561 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:38:21,561 - INFO -   Matched verses: []
2025-07-07 09:38:22,562 - INFO - Processing question 165/881 (ID: 180) for qwen
2025-07-07 09:38:22,562 - INFO -   Ground truth verses: ['2:96']
2025-07-07 09:38:22,562 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:38:38,442 - INFO - Extracted JSON from code block: 743 characters
2025-07-07 09:38:38,443 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Qaf:6]' -> 'Qaf:6'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Yunus:10]' -> 'Yunus:10'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Sad:11]' -> 'Sad:11'
2025-07-07 09:38:38,443 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Qaf:6]' -> 'Qaf:6'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Qaf:6]' -> 'Qaf:6'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Yunus:10]' -> 'Yunus:10'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Yunus:10]' -> 'Yunus:10'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Sad:11]' -> 'Sad:11'
2025-07-07 09:38:38,443 - WARNING - Invalid colon format: '[Sad:11]' -> 'Sad:11'
2025-07-07 09:38:38,443 - INFO - Normalized predicted verses: ['10:10', '2:255', '38:11', '46:6', '7:113']
2025-07-07 09:38:38,443 - INFO - Normalized ground truth verses: ['2:96']
2025-07-07 09:38:38,443 - INFO -   Response: 743 chars, Found 5 verses
2025-07-07 09:38:38,443 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:38:38,443 - INFO -   Matched verses: []
2025-07-07 09:38:38,461 - INFO - Saved data to results/qwen_checkpoint_164.json
2025-07-07 09:38:38,461 - INFO - Checkpoint saved for qwen at question 165
2025-07-07 09:38:38,461 - INFO -   Progress: 165/881
2025-07-07 09:38:38,461 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.006
2025-07-07 09:38:39,463 - INFO - Processing question 166/881 (ID: 181) for qwen
2025-07-07 09:38:39,463 - INFO -   Ground truth verses: ['2:96']
2025-07-07 09:38:39,463 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:38:55,264 - INFO - Extracted JSON from code block: 724 characters
2025-07-07 09:38:55,264 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:38:55,264 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[2]' -> '2'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[6]' -> '6'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[5]' -> '5'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Hajj]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Hajj]' -> 'Al-Hajj'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[22]' -> '22'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Tawbah]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Tawbah]' -> 'Al-Tawbah'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[9]' -> '9'
2025-07-07 09:38:55,265 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Hajj]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Hajj]' -> 'Al-Hajj'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Hajj]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Hajj]' -> 'Al-Hajj'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Tawbah]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Tawbah]' -> 'Al-Tawbah'
2025-07-07 09:38:55,265 - WARNING - Could not normalize verse reference: '[Al-Tawbah]'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[Al-Tawbah]' -> 'Al-Tawbah'
2025-07-07 09:38:55,265 - WARNING - Unrecognized verse format: '[2]' -> '2'
2025-07-07 09:38:55,266 - WARNING - Unrecognized verse format: '[2]' -> '2'
2025-07-07 09:38:55,266 - WARNING - Unrecognized verse format: '[6]' -> '6'
2025-07-07 09:38:55,266 - WARNING - Unrecognized verse format: '[6]' -> '6'
2025-07-07 09:38:55,266 - WARNING - Unrecognized verse format: '[5]' -> '5'
2025-07-07 09:38:55,266 - WARNING - Unrecognized verse format: '[5]' -> '5'
2025-07-07 09:38:55,266 - WARNING - Unrecognized verse format: '[22]' -> '22'
2025-07-07 09:38:55,266 - WARNING - Unrecognized verse format: '[22]' -> '22'
2025-07-07 09:38:55,266 - WARNING - Unrecognized verse format: '[9]' -> '9'
2025-07-07 09:38:55,266 - WARNING - Unrecognized verse format: '[9]' -> '9'
2025-07-07 09:38:55,266 - INFO - Normalized predicted verses: []
2025-07-07 09:38:55,266 - INFO - Normalized ground truth verses: ['2:96']
2025-07-07 09:38:55,266 - INFO -   Response: 724 chars, Found 5 verses
2025-07-07 09:38:55,266 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:38:55,266 - INFO -   Matched verses: []
2025-07-07 09:38:56,267 - INFO - Processing question 167/881 (ID: 182) for qwen
2025-07-07 09:38:56,267 - INFO -   Ground truth verses: ['2:96']
2025-07-07 09:38:56,267 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:39:12,246 - INFO - Extracted JSON from code block: 756 characters
2025-07-07 09:39:12,246 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 15 column 13 (char 384), falling back to regex parsing
2025-07-07 09:39:12,246 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 15 column 13 (char 384), falling back to regex parsing
2025-07-07 09:39:12,246 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 15 column 13 (char 384), falling back to regex parsing
2025-07-07 09:39:12,246 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:39:12,246 - INFO - Normalized predicted verses: []
2025-07-07 09:39:12,246 - INFO - Normalized ground truth verses: ['2:96']
2025-07-07 09:39:12,246 - INFO -   Response: 756 chars, Found 0 verses
2025-07-07 09:39:12,246 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:39:12,246 - INFO -   Matched verses: []
2025-07-07 09:39:13,247 - INFO - Processing question 168/881 (ID: 183) for qwen
2025-07-07 09:39:13,248 - INFO -   Ground truth verses: ['2:96']
2025-07-07 09:39:13,248 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:39:29,112 - INFO - Extracted JSON from code block: 101 characters
2025-07-07 09:39:29,112 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:39:29,112 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:39:29,113 - INFO - Normalized predicted verses: []
2025-07-07 09:39:29,113 - INFO - Normalized ground truth verses: ['2:96']
2025-07-07 09:39:29,113 - INFO -   Response: 101 chars, Found 0 verses
2025-07-07 09:39:29,113 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:39:29,113 - INFO -   Matched verses: []
2025-07-07 09:39:30,114 - INFO - Processing question 169/881 (ID: 184) for qwen
2025-07-07 09:39:30,114 - INFO -   Ground truth verses: ['84:16', '81:15', '69:38', '75:2']
2025-07-07 09:39:30,114 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:39:46,001 - INFO - Extracted JSON from code block: 746 characters
2025-07-07 09:39:46,002 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Qaf:1]' -> 'Qaf:1'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Yunus:10]' -> 'Yunus:10'
2025-07-07 09:39:46,002 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Qaf:1]' -> 'Qaf:1'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Qaf:1]' -> 'Qaf:1'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Yunus:10]' -> 'Yunus:10'
2025-07-07 09:39:46,002 - WARNING - Invalid colon format: '[Yunus:10]' -> 'Yunus:10'
2025-07-07 09:39:46,002 - INFO - Normalized predicted verses: ['10:10', '15:15', '2:255', '50:1', '6:84']
2025-07-07 09:39:46,002 - INFO - Normalized ground truth verses: ['69:38', '75:2', '81:15', '84:16']
2025-07-07 09:39:46,002 - INFO -   Response: 746 chars, Found 5 verses
2025-07-07 09:39:46,002 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:39:46,002 - INFO -   Matched verses: []
2025-07-07 09:39:47,004 - INFO - Processing question 170/881 (ID: 185) for qwen
2025-07-07 09:39:47,004 - INFO -   Ground truth verses: ['84:16', '81:15', '69:38', '75:2']
2025-07-07 09:39:47,004 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:40:02,878 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:40:02,878 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:40:02,878 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:40:02,878 - INFO - Normalized predicted verses: []
2025-07-07 09:40:02,878 - INFO - Normalized ground truth verses: ['69:38', '75:2', '81:15', '84:16']
2025-07-07 09:40:02,878 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:40:02,878 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:40:02,878 - INFO -   Matched verses: []
2025-07-07 09:40:02,895 - INFO - Saved data to results/qwen_checkpoint_169.json
2025-07-07 09:40:02,895 - INFO - Checkpoint saved for qwen at question 170
2025-07-07 09:40:02,895 - INFO -   Progress: 170/881
2025-07-07 09:40:02,895 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.006
2025-07-07 09:40:03,897 - INFO - Processing question 171/881 (ID: 186) for qwen
2025-07-07 09:40:03,897 - INFO -   Ground truth verses: ['84:16', '81:15', '69:38', '75:2']
2025-07-07 09:40:03,897 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:40:19,848 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:40:19,848 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:40:19,848 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:40:19,848 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:40:19,848 - INFO - Normalized predicted verses: []
2025-07-07 09:40:19,849 - INFO - Normalized ground truth verses: ['69:38', '75:2', '81:15', '84:16']
2025-07-07 09:40:19,849 - INFO -   Response: 5000 chars, Found 0 verses
2025-07-07 09:40:19,849 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:40:19,849 - INFO -   Matched verses: []
2025-07-07 09:40:20,850 - INFO - Processing question 172/881 (ID: 187) for qwen
2025-07-07 09:40:20,850 - INFO -   Ground truth verses: ['84:16', '81:15', '69:38', '75:2']
2025-07-07 09:40:20,850 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:40:36,724 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:40:36,724 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:40:36,724 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:40:36,724 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:40:36,725 - INFO - Normalized predicted verses: []
2025-07-07 09:40:36,725 - INFO - Normalized ground truth verses: ['69:38', '75:2', '81:15', '84:16']
2025-07-07 09:40:36,725 - INFO -   Response: 4367 chars, Found 0 verses
2025-07-07 09:40:36,725 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:40:36,725 - INFO -   Matched verses: []
2025-07-07 09:40:37,726 - INFO - Processing question 173/881 (ID: 189) for qwen
2025-07-07 09:40:37,726 - INFO -   Ground truth verses: ['16:91', '13:20']
2025-07-07 09:40:37,726 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:40:53,491 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:40:53,491 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:40:53,491 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:40:53,491 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:40:53,491 - INFO - Normalized predicted verses: []
2025-07-07 09:40:53,491 - INFO - Normalized ground truth verses: ['13:20', '16:91']
2025-07-07 09:40:53,491 - INFO -   Response: 4990 chars, Found 0 verses
2025-07-07 09:40:53,491 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:40:53,491 - INFO -   Matched verses: []
2025-07-07 09:40:54,493 - INFO - Processing question 174/881 (ID: 190) for qwen
2025-07-07 09:40:54,493 - INFO -   Ground truth verses: ['16:91', '13:20']
2025-07-07 09:40:54,493 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:41:10,385 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:41:10,385 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:41:10,385 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:41:10,385 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:41:10,385 - INFO - Normalized predicted verses: []
2025-07-07 09:41:10,385 - INFO - Normalized ground truth verses: ['13:20', '16:91']
2025-07-07 09:41:10,385 - INFO -   Response: 2997 chars, Found 0 verses
2025-07-07 09:41:10,385 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:41:10,385 - INFO -   Matched verses: []
2025-07-07 09:41:11,386 - INFO - Processing question 175/881 (ID: 192) for qwen
2025-07-07 09:41:11,387 - INFO -   Ground truth verses: ['16:91', '13:20']
2025-07-07 09:41:11,387 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:41:27,270 - INFO - Extracted JSON from code block: 101 characters
2025-07-07 09:41:27,270 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:41:27,270 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:41:27,270 - INFO - Normalized predicted verses: []
2025-07-07 09:41:27,270 - INFO - Normalized ground truth verses: ['13:20', '16:91']
2025-07-07 09:41:27,270 - INFO -   Response: 101 chars, Found 0 verses
2025-07-07 09:41:27,270 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:41:27,270 - INFO -   Matched verses: []
2025-07-07 09:41:27,289 - INFO - Saved data to results/qwen_checkpoint_174.json
2025-07-07 09:41:27,289 - INFO - Checkpoint saved for qwen at question 175
2025-07-07 09:41:27,289 - INFO -   Progress: 175/881
2025-07-07 09:41:27,289 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.006
2025-07-07 09:41:28,290 - INFO - Processing question 176/881 (ID: 194) for qwen
2025-07-07 09:41:28,290 - INFO -   Ground truth verses: ['20:120']
2025-07-07 09:41:28,290 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:41:44,003 - INFO - Extracted JSON from code block: 758 characters
2025-07-07 09:41:44,004 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-Ma'idah:4]' -> 'Al-Ma'idah:4'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-'Isra':69]' -> 'Al-'Isra':69'
2025-07-07 09:41:44,004 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-Ma'idah:4]' -> 'Al-Ma'idah:4'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-Ma'idah:4]' -> 'Al-Ma'idah:4'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-'Isra':69]' -> 'Al-'Isra':69'
2025-07-07 09:41:44,004 - WARNING - Invalid colon format: '[Al-'Isra':69]' -> 'Al-'Isra':69'
2025-07-07 09:41:44,004 - INFO - Normalized predicted verses: ['17:69', '2:255', '5:4', '6:86', '7:113']
2025-07-07 09:41:44,004 - INFO - Normalized ground truth verses: ['20:120']
2025-07-07 09:41:44,004 - INFO -   Response: 758 chars, Found 5 verses
2025-07-07 09:41:44,004 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:41:44,004 - INFO -   Matched verses: []
2025-07-07 09:41:45,005 - INFO - Processing question 177/881 (ID: 195) for qwen
2025-07-07 09:41:45,006 - INFO -   Ground truth verses: ['20:120']
2025-07-07 09:41:45,006 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:42:00,883 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:42:00,883 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:42:00,883 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:42:00,883 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:42:00,883 - INFO - Normalized predicted verses: []
2025-07-07 09:42:00,883 - INFO - Normalized ground truth verses: ['20:120']
2025-07-07 09:42:00,883 - INFO -   Response: 3523 chars, Found 0 verses
2025-07-07 09:42:00,883 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:42:00,884 - INFO -   Matched verses: []
2025-07-07 09:42:01,885 - INFO - Processing question 178/881 (ID: 196) for qwen
2025-07-07 09:42:01,885 - INFO -   Ground truth verses: ['20:120']
2025-07-07 09:42:01,885 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:42:17,790 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:42:17,790 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:42:17,790 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:42:17,790 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:42:17,790 - INFO - Normalized predicted verses: []
2025-07-07 09:42:17,790 - INFO - Normalized ground truth verses: ['20:120']
2025-07-07 09:42:17,790 - INFO -   Response: 5000 chars, Found 0 verses
2025-07-07 09:42:17,790 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:42:17,790 - INFO -   Matched verses: []
2025-07-07 09:42:18,791 - INFO - Processing question 179/881 (ID: 197) for qwen
2025-07-07 09:42:18,791 - INFO -   Ground truth verses: ['20:120']
2025-07-07 09:42:18,791 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:42:34,564 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:42:34,565 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:42:34,565 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:42:34,565 - INFO - Normalized predicted verses: []
2025-07-07 09:42:34,565 - INFO - Normalized ground truth verses: ['20:120']
2025-07-07 09:42:34,565 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:42:34,565 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:42:34,565 - INFO -   Matched verses: []
2025-07-07 09:42:35,566 - INFO - Processing question 180/881 (ID: 198) for qwen
2025-07-07 09:42:35,566 - INFO -   Ground truth verses: ['20:120']
2025-07-07 09:42:35,566 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:42:51,397 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:42:51,397 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:42:51,397 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:42:51,397 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:42:51,397 - INFO - Normalized predicted verses: []
2025-07-07 09:42:51,397 - INFO - Normalized ground truth verses: ['20:120']
2025-07-07 09:42:51,397 - INFO -   Response: 4990 chars, Found 0 verses
2025-07-07 09:42:51,397 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:42:51,397 - INFO -   Matched verses: []
2025-07-07 09:42:51,415 - INFO - Saved data to results/qwen_checkpoint_179.json
2025-07-07 09:42:51,415 - INFO - Checkpoint saved for qwen at question 180
2025-07-07 09:42:51,415 - INFO -   Progress: 180/881
2025-07-07 09:42:51,415 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.006
2025-07-07 09:42:52,417 - INFO - Processing question 181/881 (ID: 199) for qwen
2025-07-07 09:42:52,417 - INFO -   Ground truth verses: ['5:4']
2025-07-07 09:42:52,417 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:43:08,304 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:43:08,304 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:43:08,304 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:43:08,304 - INFO - Normalized predicted verses: []
2025-07-07 09:43:08,304 - INFO - Normalized ground truth verses: ['5:4']
2025-07-07 09:43:08,304 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:43:08,304 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:43:08,304 - INFO -   Matched verses: []
2025-07-07 09:43:09,305 - INFO - Processing question 182/881 (ID: 200) for qwen
2025-07-07 09:43:09,306 - INFO -   Ground truth verses: ['5:4']
2025-07-07 09:43:09,306 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:43:25,080 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:43:25,080 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:43:25,080 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:43:25,080 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:43:25,080 - INFO - Normalized predicted verses: []
2025-07-07 09:43:25,080 - INFO - Normalized ground truth verses: ['5:4']
2025-07-07 09:43:25,080 - INFO -   Response: 4045 chars, Found 0 verses
2025-07-07 09:43:25,080 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:43:25,080 - INFO -   Matched verses: []
2025-07-07 09:43:26,082 - INFO - Processing question 183/881 (ID: 201) for qwen
2025-07-07 09:43:26,082 - INFO -   Ground truth verses: ['5:4']
2025-07-07 09:43:26,082 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:43:41,820 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:43:41,820 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:43:41,820 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:43:41,820 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:43:41,820 - INFO - Normalized predicted verses: []
2025-07-07 09:43:41,820 - INFO - Normalized ground truth verses: ['5:4']
2025-07-07 09:43:41,820 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 09:43:41,820 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:43:41,820 - INFO -   Matched verses: []
2025-07-07 09:43:42,821 - INFO - Processing question 184/881 (ID: 202) for qwen
2025-07-07 09:43:42,822 - INFO -   Ground truth verses: ['5:4']
2025-07-07 09:43:42,822 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:43:58,560 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:43:58,560 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:43:58,560 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:43:58,560 - INFO - Normalized predicted verses: []
2025-07-07 09:43:58,560 - INFO - Normalized ground truth verses: ['5:4']
2025-07-07 09:43:58,560 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:43:58,560 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:43:58,560 - INFO -   Matched verses: []
2025-07-07 09:43:59,562 - INFO - Processing question 185/881 (ID: 203) for qwen
2025-07-07 09:43:59,562 - INFO -   Ground truth verses: ['5:4']
2025-07-07 09:43:59,562 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:44:15,246 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 09:44:15,246 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:44:15,246 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:44:15,246 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:44:15,246 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:44:15,246 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:44:15,246 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:44:15,246 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:44:15,246 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:44:15,247 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:44:15,247 - INFO - Normalized ground truth verses: ['5:4']
2025-07-07 09:44:15,247 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 09:44:15,247 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:44:15,247 - INFO -   Matched verses: []
2025-07-07 09:44:15,265 - INFO - Saved data to results/qwen_checkpoint_184.json
2025-07-07 09:44:15,265 - INFO - Checkpoint saved for qwen at question 185
2025-07-07 09:44:15,265 - INFO -   Progress: 185/881
2025-07-07 09:44:15,265 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.005
2025-07-07 09:44:16,267 - INFO - Processing question 186/881 (ID: 204) for qwen
2025-07-07 09:44:16,267 - INFO -   Ground truth verses: ['56:38', '69:45', '74:39', '20:17']
2025-07-07 09:44:16,267 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:44:32,163 - INFO - Extracted JSON from code block: 751 characters
2025-07-07 09:44:32,163 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:44:32,163 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:44:32,163 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Zumar:55]' -> 'Al-Zumar:55'
2025-07-07 09:44:32,164 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Zumar:55]' -> 'Al-Zumar:55'
2025-07-07 09:44:32,164 - WARNING - Invalid colon format: '[Al-Zumar:55]' -> 'Al-Zumar:55'
2025-07-07 09:44:32,164 - INFO - Normalized predicted verses: ['15:10', '39:55', '50:5', '5:52', '6:84']
2025-07-07 09:44:32,164 - INFO - Normalized ground truth verses: ['20:17', '56:38', '69:45', '74:39']
2025-07-07 09:44:32,164 - INFO -   Response: 751 chars, Found 5 verses
2025-07-07 09:44:32,164 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:44:32,164 - INFO -   Matched verses: []
2025-07-07 09:44:33,165 - INFO - Processing question 187/881 (ID: 205) for qwen
2025-07-07 09:44:33,165 - INFO -   Ground truth verses: ['56:38', '69:45', '74:39', '20:17']
2025-07-07 09:44:33,166 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:44:49,024 - INFO - Extracted JSON from code block: 756 characters
2025-07-07 09:44:49,025 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Hijr:53]' -> 'Al-Hijr:53'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:44:49,025 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Hijr:53]' -> 'Al-Hijr:53'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Hijr:53]' -> 'Al-Hijr:53'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:44:49,025 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:44:49,025 - INFO - Normalized predicted verses: ['15:53', '2:255', '2:64', '5:52', '5:55']
2025-07-07 09:44:49,025 - INFO - Normalized ground truth verses: ['20:17', '56:38', '69:45', '74:39']
2025-07-07 09:44:49,025 - INFO -   Response: 756 chars, Found 5 verses
2025-07-07 09:44:49,025 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:44:49,025 - INFO -   Matched verses: []
2025-07-07 09:44:50,026 - INFO - Processing question 188/881 (ID: 206) for qwen
2025-07-07 09:44:50,027 - INFO -   Ground truth verses: ['56:38', '69:45', '74:39', '20:17']
2025-07-07 09:44:50,027 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:45:05,812 - INFO - Extracted JSON from code block: 748 characters
2025-07-07 09:45:05,812 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:45:05,812 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:45:05,812 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:45:05,812 - WARNING - Invalid colon format: '[Yunus:56]' -> 'Yunus:56'
2025-07-07 09:45:05,812 - WARNING - Invalid colon format: '[Ta-Ha:20]' -> 'Ta-Ha:20'
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Sad:11]' -> 'Sad:11'
2025-07-07 09:45:05,813 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Yunus:56]' -> 'Yunus:56'
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Yunus:56]' -> 'Yunus:56'
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Ta-Ha:20]' -> 'Ta-Ha:20'
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Ta-Ha:20]' -> 'Ta-Ha:20'
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Sad:11]' -> 'Sad:11'
2025-07-07 09:45:05,813 - WARNING - Invalid colon format: '[Sad:11]' -> 'Sad:11'
2025-07-07 09:45:05,813 - INFO - Normalized predicted verses: ['10:56', '20:20', '2:255', '38:11', '7:113']
2025-07-07 09:45:05,813 - INFO - Normalized ground truth verses: ['20:17', '56:38', '69:45', '74:39']
2025-07-07 09:45:05,813 - INFO -   Response: 748 chars, Found 5 verses
2025-07-07 09:45:05,813 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:45:05,813 - INFO -   Matched verses: []
2025-07-07 09:45:06,814 - INFO - Processing question 189/881 (ID: 207) for qwen
2025-07-07 09:45:06,814 - INFO -   Ground truth verses: ['56:38', '69:45', '74:39', '20:17']
2025-07-07 09:45:06,814 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:45:22,764 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:45:22,765 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:45:22,765 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:45:22,765 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:45:22,765 - INFO - Normalized predicted verses: []
2025-07-07 09:45:22,765 - INFO - Normalized ground truth verses: ['20:17', '56:38', '69:45', '74:39']
2025-07-07 09:45:22,765 - INFO -   Response: 3502 chars, Found 0 verses
2025-07-07 09:45:22,765 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:45:22,765 - INFO -   Matched verses: []
2025-07-07 09:45:23,766 - INFO - Processing question 190/881 (ID: 208) for qwen
2025-07-07 09:45:23,766 - INFO -   Ground truth verses: ['56:38', '69:45', '74:39', '20:17']
2025-07-07 09:45:23,766 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:45:39,552 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:45:39,552 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:45:39,552 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:45:39,552 - INFO - Normalized predicted verses: []
2025-07-07 09:45:39,552 - INFO - Normalized ground truth verses: ['20:17', '56:38', '69:45', '74:39']
2025-07-07 09:45:39,552 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:45:39,552 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:45:39,552 - INFO -   Matched verses: []
2025-07-07 09:45:39,573 - INFO - Saved data to results/qwen_checkpoint_189.json
2025-07-07 09:45:39,573 - INFO - Checkpoint saved for qwen at question 190
2025-07-07 09:45:39,573 - INFO -   Progress: 190/881
2025-07-07 09:45:39,573 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.005
2025-07-07 09:45:40,574 - INFO - Processing question 191/881 (ID: 209) for qwen
2025-07-07 09:45:40,574 - INFO -   Ground truth verses: ['33:37']
2025-07-07 09:45:40,574 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:45:56,492 - INFO - Extracted JSON from code block: 759 characters
2025-07-07 09:45:56,492 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[Al-Ma'idah:55]' -> 'Al-Ma'idah:55'
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[At-Tawbah:120]' -> 'At-Tawbah:120'
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 09:45:56,492 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[Al-Ma'idah:55]' -> 'Al-Ma'idah:55'
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[Al-Ma'idah:55]' -> 'Al-Ma'idah:55'
2025-07-07 09:45:56,492 - WARNING - Invalid colon format: '[At-Tawbah:120]' -> 'At-Tawbah:120'
2025-07-07 09:45:56,493 - WARNING - Invalid colon format: '[At-Tawbah:120]' -> 'At-Tawbah:120'
2025-07-07 09:45:56,493 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 09:45:56,493 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 09:45:56,493 - INFO - Normalized predicted verses: ['15:79', '2:255', '5:55', '6:84', '9:120']
2025-07-07 09:45:56,493 - INFO - Normalized ground truth verses: ['33:37']
2025-07-07 09:45:56,493 - INFO -   Response: 759 chars, Found 5 verses
2025-07-07 09:45:56,493 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:45:56,493 - INFO -   Matched verses: []
2025-07-07 09:45:57,494 - INFO - Processing question 192/881 (ID: 210) for qwen
2025-07-07 09:45:57,494 - INFO -   Ground truth verses: ['33:37']
2025-07-07 09:45:57,494 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:46:13,256 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:46:13,256 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:46:13,256 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:46:13,256 - INFO - Normalized predicted verses: []
2025-07-07 09:46:13,256 - INFO - Normalized ground truth verses: ['33:37']
2025-07-07 09:46:13,256 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:46:13,256 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:46:13,256 - INFO -   Matched verses: []
2025-07-07 09:46:14,258 - INFO - Processing question 193/881 (ID: 211) for qwen
2025-07-07 09:46:14,258 - INFO -   Ground truth verses: ['33:37']
2025-07-07 09:46:14,258 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:46:30,138 - INFO - Extracted JSON from code block: 757 characters
2025-07-07 09:46:30,139 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[An-Nur:35]' -> 'An-Nur:35'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[At-Tawbah:107]' -> 'At-Tawbah:107'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[Ar-Rum:11]' -> 'Ar-Rum:11'
2025-07-07 09:46:30,139 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[An-Nur:35]' -> 'An-Nur:35'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[An-Nur:35]' -> 'An-Nur:35'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[At-Tawbah:107]' -> 'At-Tawbah:107'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[At-Tawbah:107]' -> 'At-Tawbah:107'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[Ar-Rum:11]' -> 'Ar-Rum:11'
2025-07-07 09:46:30,139 - WARNING - Invalid colon format: '[Ar-Rum:11]' -> 'Ar-Rum:11'
2025-07-07 09:46:30,139 - INFO - Normalized predicted verses: ['24:35', '2:255', '30:11', '7:113', '9:107']
2025-07-07 09:46:30,139 - INFO - Normalized ground truth verses: ['33:37']
2025-07-07 09:46:30,139 - INFO -   Response: 757 chars, Found 5 verses
2025-07-07 09:46:30,139 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:46:30,139 - INFO -   Matched verses: []
2025-07-07 09:46:31,140 - INFO - Processing question 194/881 (ID: 212) for qwen
2025-07-07 09:46:31,141 - INFO -   Ground truth verses: ['33:37']
2025-07-07 09:46:31,141 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:46:46,952 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:46:46,952 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:46:46,952 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:46:46,952 - INFO - Normalized predicted verses: []
2025-07-07 09:46:46,952 - INFO - Normalized ground truth verses: ['33:37']
2025-07-07 09:46:46,952 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:46:46,952 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:46:46,952 - INFO -   Matched verses: []
2025-07-07 09:46:47,953 - INFO - Processing question 195/881 (ID: 213) for qwen
2025-07-07 09:46:47,954 - INFO -   Ground truth verses: ['33:37']
2025-07-07 09:46:47,954 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:47:03,721 - INFO - Extracted JSON from code block: 1383 characters
2025-07-07 09:47:03,721 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:47:03,721 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:47:03,721 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:47:03,721 - WARNING - Unrecognized verse format: '[2]' -> '2'
2025-07-07 09:47:03,721 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:47:03,721 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:47:03,721 - WARNING - Unrecognized verse format: '[6]' -> '6'
2025-07-07 09:47:03,721 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:47:03,721 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:47:03,721 - WARNING - Unrecognized verse format: '[5]' -> '5'
2025-07-07 09:47:03,721 - WARNING - Could not normalize verse reference: '[At-Tawbah]'
2025-07-07 09:47:03,721 - WARNING - Unrecognized verse format: '[At-Tawbah]' -> 'At-Tawbah'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[9]' -> '9'
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[An-Nur]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[An-Nur]' -> 'An-Nur'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[24]' -> '24'
2025-07-07 09:47:03,722 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[At-Tawbah]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[At-Tawbah]' -> 'At-Tawbah'
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[At-Tawbah]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[At-Tawbah]' -> 'At-Tawbah'
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[An-Nur]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[An-Nur]' -> 'An-Nur'
2025-07-07 09:47:03,722 - WARNING - Could not normalize verse reference: '[An-Nur]'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[An-Nur]' -> 'An-Nur'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[2]' -> '2'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[2]' -> '2'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[6]' -> '6'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[6]' -> '6'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[5]' -> '5'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[5]' -> '5'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[9]' -> '9'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[9]' -> '9'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[24]' -> '24'
2025-07-07 09:47:03,722 - WARNING - Unrecognized verse format: '[24]' -> '24'
2025-07-07 09:47:03,722 - INFO - Normalized predicted verses: []
2025-07-07 09:47:03,722 - INFO - Normalized ground truth verses: ['33:37']
2025-07-07 09:47:03,722 - INFO -   Response: 1383 chars, Found 5 verses
2025-07-07 09:47:03,723 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:47:03,723 - INFO -   Matched verses: []
2025-07-07 09:47:03,742 - INFO - Saved data to results/qwen_checkpoint_194.json
2025-07-07 09:47:03,742 - INFO - Checkpoint saved for qwen at question 195
2025-07-07 09:47:03,742 - INFO -   Progress: 195/881
2025-07-07 09:47:03,742 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.005
2025-07-07 09:47:04,743 - INFO - Processing question 196/881 (ID: 214) for qwen
2025-07-07 09:47:04,744 - INFO -   Ground truth verses: ['33:17', '4:173', '4:123']
2025-07-07 09:47:04,744 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:47:20,690 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:47:20,691 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:47:20,691 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:47:20,691 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:47:20,691 - INFO - Normalized predicted verses: []
2025-07-07 09:47:20,691 - INFO - Normalized ground truth verses: ['33:17', '4:123', '4:173']
2025-07-07 09:47:20,691 - INFO -   Response: 3910 chars, Found 0 verses
2025-07-07 09:47:20,691 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:47:20,691 - INFO -   Matched verses: []
2025-07-07 09:47:21,692 - INFO - Processing question 197/881 (ID: 215) for qwen
2025-07-07 09:47:21,692 - INFO -   Ground truth verses: ['33:17', '4:173', '4:123']
2025-07-07 09:47:21,692 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:47:37,456 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 15 column 17 (char 440), falling back to regex parsing
2025-07-07 09:47:37,456 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 15 column 17 (char 440), falling back to regex parsing
2025-07-07 09:47:37,456 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 15 column 17 (char 440), falling back to regex parsing
2025-07-07 09:47:37,456 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:47:37,456 - INFO - Normalized predicted verses: []
2025-07-07 09:47:37,456 - INFO - Normalized ground truth verses: ['33:17', '4:123', '4:173']
2025-07-07 09:47:37,456 - INFO -   Response: 4760 chars, Found 0 verses
2025-07-07 09:47:37,456 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:47:37,456 - INFO -   Matched verses: []
2025-07-07 09:47:38,457 - INFO - Processing question 198/881 (ID: 216) for qwen
2025-07-07 09:47:38,457 - INFO -   Ground truth verses: ['33:17', '4:173', '4:123']
2025-07-07 09:47:38,457 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:47:54,194 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:47:54,194 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:47:54,195 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:47:54,195 - INFO - Normalized predicted verses: []
2025-07-07 09:47:54,195 - INFO - Normalized ground truth verses: ['33:17', '4:123', '4:173']
2025-07-07 09:47:54,195 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:47:54,195 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:47:54,195 - INFO -   Matched verses: []
2025-07-07 09:47:55,196 - INFO - Processing question 199/881 (ID: 217) for qwen
2025-07-07 09:47:55,196 - INFO -   Ground truth verses: ['33:17', '4:173', '4:123']
2025-07-07 09:47:55,196 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:48:11,126 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:48:11,127 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:48:11,127 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:48:11,127 - INFO - Normalized predicted verses: []
2025-07-07 09:48:11,127 - INFO - Normalized ground truth verses: ['33:17', '4:123', '4:173']
2025-07-07 09:48:11,127 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:48:11,127 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:48:11,127 - INFO -   Matched verses: []
2025-07-07 09:48:12,128 - INFO - Processing question 200/881 (ID: 218) for qwen
2025-07-07 09:48:12,128 - INFO -   Ground truth verses: ['33:17', '4:173', '4:123']
2025-07-07 09:48:12,128 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:48:27,911 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:48:27,911 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:48:27,911 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:48:27,911 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:48:27,911 - INFO - Normalized predicted verses: []
2025-07-07 09:48:27,911 - INFO - Normalized ground truth verses: ['33:17', '4:123', '4:173']
2025-07-07 09:48:27,911 - INFO -   Response: 4539 chars, Found 0 verses
2025-07-07 09:48:27,911 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:48:27,911 - INFO -   Matched verses: []
2025-07-07 09:48:27,931 - INFO - Saved data to results/qwen_checkpoint_199.json
2025-07-07 09:48:27,931 - INFO - Checkpoint saved for qwen at question 200
2025-07-07 09:48:27,931 - INFO -   Progress: 200/881
2025-07-07 09:48:27,931 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.005
2025-07-07 09:48:28,932 - INFO - Processing question 201/881 (ID: 219) for qwen
2025-07-07 09:48:28,932 - INFO -   Ground truth verses: ['32:12']
2025-07-07 09:48:28,932 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:48:44,636 - INFO - Extracted JSON from code block: 757 characters
2025-07-07 09:48:44,636 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:48:44,636 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:48:44,636 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Hijr:74]' -> 'Al-Hijr:74'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 09:48:44,637 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Hijr:74]' -> 'Al-Hijr:74'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Hijr:74]' -> 'Al-Hijr:74'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 09:48:44,637 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 09:48:44,637 - INFO - Normalized predicted verses: ['15:74', '2:255', '57:12', '5:11', '6:84']
2025-07-07 09:48:44,637 - INFO - Normalized ground truth verses: ['32:12']
2025-07-07 09:48:44,637 - INFO -   Response: 757 chars, Found 5 verses
2025-07-07 09:48:44,637 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:48:44,637 - INFO -   Matched verses: []
2025-07-07 09:48:45,638 - INFO - Processing question 202/881 (ID: 220) for qwen
2025-07-07 09:48:45,638 - INFO -   Ground truth verses: ['32:12']
2025-07-07 09:48:45,639 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:49:01,587 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:49:01,587 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:49:01,587 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:49:01,588 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:49:01,588 - INFO - Normalized predicted verses: []
2025-07-07 09:49:01,588 - INFO - Normalized ground truth verses: ['32:12']
2025-07-07 09:49:01,588 - INFO -   Response: 4359 chars, Found 0 verses
2025-07-07 09:49:01,588 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:49:01,588 - INFO -   Matched verses: []
2025-07-07 09:49:02,589 - INFO - Processing question 203/881 (ID: 221) for qwen
2025-07-07 09:49:02,589 - INFO -   Ground truth verses: ['32:12']
2025-07-07 09:49:02,589 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:49:18,389 - INFO - Extracted JSON from code block: 759 characters
2025-07-07 09:49:18,389 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:49:18,389 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:49:18,389 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:49:18,390 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:49:18,390 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:49:18,390 - INFO - Normalized predicted verses: ['15:15', '2:255', '4:86', '50:10', '5:104']
2025-07-07 09:49:18,390 - INFO - Normalized ground truth verses: ['32:12']
2025-07-07 09:49:18,390 - INFO -   Response: 759 chars, Found 5 verses
2025-07-07 09:49:18,390 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:49:18,390 - INFO -   Matched verses: []
2025-07-07 09:49:19,391 - INFO - Processing question 204/881 (ID: 222) for qwen
2025-07-07 09:49:19,391 - INFO -   Ground truth verses: ['32:12']
2025-07-07 09:49:19,391 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:49:35,045 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 876), falling back to regex parsing
2025-07-07 09:49:35,045 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 876), falling back to regex parsing
2025-07-07 09:49:35,045 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 876), falling back to regex parsing
2025-07-07 09:49:35,045 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:49:35,045 - INFO - Normalized predicted verses: []
2025-07-07 09:49:35,045 - INFO - Normalized ground truth verses: ['32:12']
2025-07-07 09:49:35,045 - INFO -   Response: 4518 chars, Found 0 verses
2025-07-07 09:49:35,045 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:49:35,045 - INFO -   Matched verses: []
2025-07-07 09:49:36,047 - INFO - Processing question 205/881 (ID: 223) for qwen
2025-07-07 09:49:36,047 - INFO -   Ground truth verses: ['32:12']
2025-07-07 09:49:36,047 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:49:51,791 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:49:51,791 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:49:51,791 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:49:51,791 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:49:51,791 - INFO - Normalized predicted verses: []
2025-07-07 09:49:51,791 - INFO - Normalized ground truth verses: ['32:12']
2025-07-07 09:49:51,791 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 09:49:51,791 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:49:51,791 - INFO -   Matched verses: []
2025-07-07 09:49:51,811 - INFO - Saved data to results/qwen_checkpoint_204.json
2025-07-07 09:49:51,811 - INFO - Checkpoint saved for qwen at question 205
2025-07-07 09:49:51,811 - INFO -   Progress: 205/881
2025-07-07 09:49:51,811 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.005
2025-07-07 09:49:52,812 - INFO - Processing question 206/881 (ID: 224) for qwen
2025-07-07 09:49:52,812 - INFO -   Ground truth verses: ['84:20', '57:8']
2025-07-07 09:49:52,812 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:50:08,543 - INFO - Extracted JSON from code block: 752 characters
2025-07-07 09:50:08,543 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Qamar:19]' -> 'Al-Qamar:19'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:50:08,543 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Qamar:19]' -> 'Al-Qamar:19'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Qamar:19]' -> 'Al-Qamar:19'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:50:08,543 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 09:50:08,543 - INFO - Normalized predicted verses: ['15:7', '25:56', '4:82', '57:19', '5:16']
2025-07-07 09:50:08,543 - INFO - Normalized ground truth verses: ['57:8', '84:20']
2025-07-07 09:50:08,543 - INFO -   Response: 752 chars, Found 5 verses
2025-07-07 09:50:08,543 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:50:08,544 - INFO -   Matched verses: []
2025-07-07 09:50:09,545 - INFO - Processing question 207/881 (ID: 225) for qwen
2025-07-07 09:50:09,545 - INFO -   Ground truth verses: ['84:20', '57:8']
2025-07-07 09:50:09,545 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:50:25,362 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:50:25,362 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:50:25,362 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:50:25,362 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:50:25,362 - INFO - Normalized predicted verses: []
2025-07-07 09:50:25,362 - INFO - Normalized ground truth verses: ['57:8', '84:20']
2025-07-07 09:50:25,362 - INFO -   Response: 4785 chars, Found 0 verses
2025-07-07 09:50:25,362 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:50:25,362 - INFO -   Matched verses: []
2025-07-07 09:50:26,363 - INFO - Processing question 208/881 (ID: 226) for qwen
2025-07-07 09:50:26,364 - INFO -   Ground truth verses: ['84:20', '57:8']
2025-07-07 09:50:26,364 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:50:42,005 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:50:42,006 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:50:42,006 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:50:42,006 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:50:42,006 - INFO - Normalized predicted verses: []
2025-07-07 09:50:42,006 - INFO - Normalized ground truth verses: ['57:8', '84:20']
2025-07-07 09:50:42,006 - INFO -   Response: 5073 chars, Found 0 verses
2025-07-07 09:50:42,006 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:50:42,006 - INFO -   Matched verses: []
2025-07-07 09:50:43,007 - INFO - Processing question 209/881 (ID: 227) for qwen
2025-07-07 09:50:43,007 - INFO -   Ground truth verses: ['84:20', '57:8']
2025-07-07 09:50:43,007 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:50:58,732 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 09:50:58,732 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Ma'idah:4]' -> 'Al-Ma'idah:4'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Tawbah:106]' -> 'Al-Tawbah:106'
2025-07-07 09:50:58,732 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Ma'idah:4]' -> 'Al-Ma'idah:4'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Ma'idah:4]' -> 'Al-Ma'idah:4'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Tawbah:106]' -> 'Al-Tawbah:106'
2025-07-07 09:50:58,732 - WARNING - Invalid colon format: '[Al-Tawbah:106]' -> 'Al-Tawbah:106'
2025-07-07 09:50:58,733 - INFO - Normalized predicted verses: ['2:255', '49:11', '5:4', '6:89', '9:106']
2025-07-07 09:50:58,733 - INFO - Normalized ground truth verses: ['57:8', '84:20']
2025-07-07 09:50:58,733 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 09:50:58,733 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:50:58,733 - INFO -   Matched verses: []
2025-07-07 09:50:59,734 - INFO - Processing question 210/881 (ID: 228) for qwen
2025-07-07 09:50:59,734 - INFO -   Ground truth verses: ['84:20', '57:8']
2025-07-07 09:50:59,734 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:51:15,590 - INFO - Extracted JSON from code block: 746 characters
2025-07-07 09:51:15,590 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:51:15,590 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:51:15,590 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:51:15,590 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:51:15,590 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:51:15,590 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:51:15,590 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:51:15,590 - WARNING - Could not normalize verse reference: '[At-Tawbah]'
2025-07-07 09:51:15,590 - WARNING - Unrecognized verse format: '[At-Tawbah]' -> 'At-Tawbah'
2025-07-07 09:51:15,590 - WARNING - Could not normalize verse reference: '[Al-Hujurat]'
2025-07-07 09:51:15,590 - WARNING - Unrecognized verse format: '[Al-Hujurat]' -> 'Al-Hujurat'
2025-07-07 09:51:15,590 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:51:15,591 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:51:15,591 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:51:15,591 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:51:15,591 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:51:15,591 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:51:15,591 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:51:15,591 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:51:15,591 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:51:15,591 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:51:15,591 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:51:15,591 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:51:15,591 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:51:15,591 - WARNING - Could not normalize verse reference: '[At-Tawbah]'
2025-07-07 09:51:15,591 - WARNING - Unrecognized verse format: '[At-Tawbah]' -> 'At-Tawbah'
2025-07-07 09:51:15,591 - WARNING - Could not normalize verse reference: '[At-Tawbah]'
2025-07-07 09:51:15,591 - WARNING - Unrecognized verse format: '[At-Tawbah]' -> 'At-Tawbah'
2025-07-07 09:51:15,591 - WARNING - Could not normalize verse reference: '[Al-Hujurat]'
2025-07-07 09:51:15,591 - WARNING - Unrecognized verse format: '[Al-Hujurat]' -> 'Al-Hujurat'
2025-07-07 09:51:15,591 - WARNING - Could not normalize verse reference: '[Al-Hujurat]'
2025-07-07 09:51:15,591 - WARNING - Unrecognized verse format: '[Al-Hujurat]' -> 'Al-Hujurat'
2025-07-07 09:51:15,591 - INFO - Normalized predicted verses: ['2:255', '49:13', '5:48', '6:108', '9:113']
2025-07-07 09:51:15,591 - INFO - Normalized ground truth verses: ['57:8', '84:20']
2025-07-07 09:51:15,591 - INFO -   Response: 746 chars, Found 5 verses
2025-07-07 09:51:15,591 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:51:15,591 - INFO -   Matched verses: []
2025-07-07 09:51:15,611 - INFO - Saved data to results/qwen_checkpoint_209.json
2025-07-07 09:51:15,611 - INFO - Checkpoint saved for qwen at question 210
2025-07-07 09:51:15,611 - INFO -   Progress: 210/881
2025-07-07 09:51:15,611 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.005
2025-07-07 09:51:16,612 - INFO - Processing question 211/881 (ID: 229) for qwen
2025-07-07 09:51:16,612 - INFO -   Ground truth verses: ['9:123']
2025-07-07 09:51:16,613 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:51:32,222 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 881), falling back to regex parsing
2025-07-07 09:51:32,222 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 881), falling back to regex parsing
2025-07-07 09:51:32,222 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 881), falling back to regex parsing
2025-07-07 09:51:32,222 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:51:32,222 - INFO - Normalized predicted verses: []
2025-07-07 09:51:32,222 - INFO - Normalized ground truth verses: ['9:123']
2025-07-07 09:51:32,222 - INFO -   Response: 3485 chars, Found 0 verses
2025-07-07 09:51:32,222 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:51:32,222 - INFO -   Matched verses: []
2025-07-07 09:51:33,223 - INFO - Processing question 212/881 (ID: 230) for qwen
2025-07-07 09:51:33,224 - INFO -   Ground truth verses: ['9:123']
2025-07-07 09:51:33,224 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:51:49,016 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 09:51:49,017 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:51:49,017 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:51:49,017 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:51:49,017 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:51:49,017 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:51:49,017 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:51:49,017 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:51:49,017 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:51:49,017 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:51:49,017 - INFO - Normalized ground truth verses: ['9:123']
2025-07-07 09:51:49,017 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 09:51:49,017 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:51:49,017 - INFO -   Matched verses: []
2025-07-07 09:51:50,018 - INFO - Processing question 213/881 (ID: 231) for qwen
2025-07-07 09:51:50,019 - INFO -   Ground truth verses: ['9:123']
2025-07-07 09:51:50,019 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:52:05,740 - INFO - Extracted JSON from code block: 752 characters
2025-07-07 09:52:05,740 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:52:05,740 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:52:05,740 - WARNING - Invalid colon format: '[Al-Ma'idah:56]' -> 'Al-Ma'idah:56'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Qalam:1]' -> 'Al-Qalam:1'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Tawbah:9]' -> 'Al-Tawbah:9'
2025-07-07 09:52:05,741 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Ma'idah:56]' -> 'Al-Ma'idah:56'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Ma'idah:56]' -> 'Al-Ma'idah:56'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Qalam:1]' -> 'Al-Qalam:1'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Qalam:1]' -> 'Al-Qalam:1'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Tawbah:9]' -> 'Al-Tawbah:9'
2025-07-07 09:52:05,741 - WARNING - Invalid colon format: '[Al-Tawbah:9]' -> 'Al-Tawbah:9'
2025-07-07 09:52:05,741 - INFO - Normalized predicted verses: ['49:11', '4:89', '5:56', '68:1', '9:9']
2025-07-07 09:52:05,741 - INFO - Normalized ground truth verses: ['9:123']
2025-07-07 09:52:05,741 - INFO -   Response: 752 chars, Found 5 verses
2025-07-07 09:52:05,741 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:52:05,741 - INFO -   Matched verses: []
2025-07-07 09:52:06,742 - INFO - Processing question 214/881 (ID: 232) for qwen
2025-07-07 09:52:06,742 - INFO -   Ground truth verses: ['9:123']
2025-07-07 09:52:06,742 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:52:22,495 - INFO - Extracted JSON from code block: 101 characters
2025-07-07 09:52:22,495 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:52:22,495 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:52:22,495 - INFO - Normalized predicted verses: []
2025-07-07 09:52:22,495 - INFO - Normalized ground truth verses: ['9:123']
2025-07-07 09:52:22,495 - INFO -   Response: 101 chars, Found 0 verses
2025-07-07 09:52:22,495 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:52:22,495 - INFO -   Matched verses: []
2025-07-07 09:52:23,496 - INFO - Processing question 215/881 (ID: 233) for qwen
2025-07-07 09:52:23,496 - INFO -   Ground truth verses: ['9:123']
2025-07-07 09:52:23,496 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:52:39,275 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:52:39,275 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:52:39,275 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:52:39,275 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:52:39,275 - INFO - Normalized predicted verses: []
2025-07-07 09:52:39,275 - INFO - Normalized ground truth verses: ['9:123']
2025-07-07 09:52:39,275 - INFO -   Response: 4582 chars, Found 0 verses
2025-07-07 09:52:39,275 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:52:39,275 - INFO -   Matched verses: []
2025-07-07 09:52:39,295 - INFO - Saved data to results/qwen_checkpoint_214.json
2025-07-07 09:52:39,295 - INFO - Checkpoint saved for qwen at question 215
2025-07-07 09:52:39,295 - INFO -   Progress: 215/881
2025-07-07 09:52:39,295 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.005
2025-07-07 09:52:40,296 - INFO - Processing question 216/881 (ID: 234) for qwen
2025-07-07 09:52:40,296 - INFO -   Ground truth verses: ['42:46', '42:44']
2025-07-07 09:52:40,296 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:52:56,023 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:52:56,023 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:52:56,023 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:52:56,023 - INFO - Normalized predicted verses: []
2025-07-07 09:52:56,023 - INFO - Normalized ground truth verses: ['42:44', '42:46']
2025-07-07 09:52:56,023 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:52:56,023 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:52:56,023 - INFO -   Matched verses: []
2025-07-07 09:52:57,024 - INFO - Processing question 217/881 (ID: 235) for qwen
2025-07-07 09:52:57,024 - INFO -   Ground truth verses: ['42:46', '42:44']
2025-07-07 09:52:57,024 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:53:12,889 - INFO - Extracted JSON from code block: 58 characters
2025-07-07 09:53:12,889 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:53:12,890 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:53:12,890 - INFO - Normalized predicted verses: []
2025-07-07 09:53:12,890 - INFO - Normalized ground truth verses: ['42:44', '42:46']
2025-07-07 09:53:12,890 - INFO -   Response: 58 chars, Found 0 verses
2025-07-07 09:53:12,890 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:53:12,890 - INFO -   Matched verses: []
2025-07-07 09:53:13,891 - INFO - Processing question 218/881 (ID: 236) for qwen
2025-07-07 09:53:13,891 - INFO -   Ground truth verses: ['42:46', '42:44']
2025-07-07 09:53:13,891 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:53:29,816 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:53:29,816 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:53:29,816 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:53:29,816 - INFO - Normalized predicted verses: []
2025-07-07 09:53:29,816 - INFO - Normalized ground truth verses: ['42:44', '42:46']
2025-07-07 09:53:29,816 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:53:29,816 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:53:29,816 - INFO -   Matched verses: []
2025-07-07 09:53:30,817 - INFO - Processing question 219/881 (ID: 237) for qwen
2025-07-07 09:53:30,817 - INFO -   Ground truth verses: ['42:46', '42:44']
2025-07-07 09:53:30,817 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:53:46,646 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:53:46,646 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:53:46,646 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 09:53:46,646 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:53:46,646 - INFO - Normalized predicted verses: []
2025-07-07 09:53:46,646 - INFO - Normalized ground truth verses: ['42:44', '42:46']
2025-07-07 09:53:46,646 - INFO -   Response: 4294 chars, Found 0 verses
2025-07-07 09:53:46,646 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:53:46,646 - INFO -   Matched verses: []
2025-07-07 09:53:47,647 - INFO - Processing question 220/881 (ID: 238) for qwen
2025-07-07 09:53:47,647 - INFO -   Ground truth verses: ['42:46', '42:44']
2025-07-07 09:53:47,647 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:54:03,396 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 09:54:03,396 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:54:03,396 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:54:03,396 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:54:03,396 - WARNING - Invalid colon format: '[Al-An'am:108]' -> 'Al-An'am:108'
2025-07-07 09:54:03,396 - WARNING - Invalid colon format: '[Al-An'am:109]' -> 'Al-An'am:109'
2025-07-07 09:54:03,396 - WARNING - Invalid colon format: '[Al-An'am:110]' -> 'Al-An'am:110'
2025-07-07 09:54:03,397 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:54:03,397 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:54:03,397 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:54:03,397 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:54:03,397 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:54:03,397 - WARNING - Invalid colon format: '[Al-An'am:108]' -> 'Al-An'am:108'
2025-07-07 09:54:03,397 - WARNING - Invalid colon format: '[Al-An'am:108]' -> 'Al-An'am:108'
2025-07-07 09:54:03,397 - WARNING - Invalid colon format: '[Al-An'am:109]' -> 'Al-An'am:109'
2025-07-07 09:54:03,397 - WARNING - Invalid colon format: '[Al-An'am:109]' -> 'Al-An'am:109'
2025-07-07 09:54:03,397 - WARNING - Invalid colon format: '[Al-An'am:110]' -> 'Al-An'am:110'
2025-07-07 09:54:03,397 - WARNING - Invalid colon format: '[Al-An'am:110]' -> 'Al-An'am:110'
2025-07-07 09:54:03,397 - INFO - Normalized predicted verses: ['2:255', '6:108', '6:109', '6:110', '6:84']
2025-07-07 09:54:03,397 - INFO - Normalized ground truth verses: ['42:44', '42:46']
2025-07-07 09:54:03,397 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 09:54:03,397 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:54:03,397 - INFO -   Matched verses: []
2025-07-07 09:54:03,421 - INFO - Saved data to results/qwen_checkpoint_219.json
2025-07-07 09:54:03,421 - INFO - Checkpoint saved for qwen at question 220
2025-07-07 09:54:03,421 - INFO -   Progress: 220/881
2025-07-07 09:54:03,421 - INFO -   Running averages - F1: 0.002, Precision: 0.001, Recall: 0.005
2025-07-07 09:54:04,422 - INFO - Processing question 221/881 (ID: 239) for qwen
2025-07-07 09:54:04,422 - INFO -   Ground truth verses: ['100:11']
2025-07-07 09:54:04,422 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:54:20,331 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:54:20,331 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:54:20,331 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:54:20,331 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:54:20,331 - INFO - Normalized predicted verses: []
2025-07-07 09:54:20,331 - INFO - Normalized ground truth verses: ['100:11']
2025-07-07 09:54:20,331 - INFO -   Response: 4361 chars, Found 0 verses
2025-07-07 09:54:20,331 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:54:20,331 - INFO -   Matched verses: []
2025-07-07 09:54:21,333 - INFO - Processing question 222/881 (ID: 240) for qwen
2025-07-07 09:54:21,333 - INFO -   Ground truth verses: ['100:11']
2025-07-07 09:54:21,333 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:54:37,206 - INFO - Extracted JSON from code block: 741 characters
2025-07-07 09:54:37,206 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Hijr]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Hijr]' -> 'Al-Hijr'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Qamar]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Qamar]' -> 'Al-Qamar'
2025-07-07 09:54:37,207 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Hijr]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Hijr]' -> 'Al-Hijr'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Hijr]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Hijr]' -> 'Al-Hijr'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Qamar]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Qamar]' -> 'Al-Qamar'
2025-07-07 09:54:37,207 - WARNING - Could not normalize verse reference: '[Al-Qamar]'
2025-07-07 09:54:37,207 - WARNING - Unrecognized verse format: '[Al-Qamar]' -> 'Al-Qamar'
2025-07-07 09:54:37,208 - INFO - Normalized predicted verses: ['15:44', '2:255', '50:39', '5:13', '6:54']
2025-07-07 09:54:37,208 - INFO - Normalized ground truth verses: ['100:11']
2025-07-07 09:54:37,208 - INFO -   Response: 741 chars, Found 5 verses
2025-07-07 09:54:37,208 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:54:37,208 - INFO -   Matched verses: []
2025-07-07 09:54:38,209 - INFO - Processing question 223/881 (ID: 241) for qwen
2025-07-07 09:54:38,209 - INFO -   Ground truth verses: ['100:11']
2025-07-07 09:54:38,209 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:54:53,908 - INFO - Extracted JSON from code block: 211 characters
2025-07-07 09:54:53,909 - WARNING - Failed to parse JSON response: Extra data: line 4 column 6 (char 134), falling back to regex parsing
2025-07-07 09:54:53,909 - WARNING - Failed to parse JSON response: Extra data: line 4 column 6 (char 134), falling back to regex parsing
2025-07-07 09:54:53,909 - WARNING - Failed to parse JSON response: Extra data: line 4 column 6 (char 134), falling back to regex parsing
2025-07-07 09:54:53,909 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:54:53,909 - INFO - Normalized predicted verses: []
2025-07-07 09:54:53,909 - INFO - Normalized ground truth verses: ['100:11']
2025-07-07 09:54:53,909 - INFO -   Response: 211 chars, Found 0 verses
2025-07-07 09:54:53,909 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:54:53,909 - INFO -   Matched verses: []
2025-07-07 09:54:54,910 - INFO - Processing question 224/881 (ID: 242) for qwen
2025-07-07 09:54:54,910 - INFO -   Ground truth verses: ['100:11']
2025-07-07 09:54:54,910 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:55:10,765 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:55:10,765 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:55:10,765 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:55:10,765 - INFO - Normalized predicted verses: []
2025-07-07 09:55:10,765 - INFO - Normalized ground truth verses: ['100:11']
2025-07-07 09:55:10,765 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:55:10,765 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:55:10,765 - INFO -   Matched verses: []
2025-07-07 09:55:11,766 - INFO - Processing question 225/881 (ID: 243) for qwen
2025-07-07 09:55:11,766 - INFO -   Ground truth verses: ['100:11']
2025-07-07 09:55:11,766 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:55:27,523 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 09:55:27,523 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:55:27,523 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:55:27,523 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:55:27,523 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:55:27,523 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:55:27,523 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:55:27,523 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:55:27,523 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:55:27,524 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:55:27,524 - INFO - Normalized ground truth verses: ['100:11']
2025-07-07 09:55:27,524 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 09:55:27,524 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:55:27,524 - INFO -   Matched verses: []
2025-07-07 09:55:27,544 - INFO - Saved data to results/qwen_checkpoint_224.json
2025-07-07 09:55:27,544 - INFO - Checkpoint saved for qwen at question 225
2025-07-07 09:55:27,544 - INFO -   Progress: 225/881
2025-07-07 09:55:27,544 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 09:55:28,546 - INFO - Processing question 226/881 (ID: 244) for qwen
2025-07-07 09:55:28,546 - INFO -   Ground truth verses: ['7:82', '29:24', '29:29', '27:56']
2025-07-07 09:55:28,546 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:55:44,169 - INFO - Extracted JSON from code block: 759 characters
2025-07-07 09:55:44,169 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-An'am:104]' -> 'Al-An'am:104'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-Hijr:45]' -> 'Al-Hijr:45'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:55:44,169 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-An'am:104]' -> 'Al-An'am:104'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-An'am:104]' -> 'Al-An'am:104'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-Hijr:45]' -> 'Al-Hijr:45'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-Hijr:45]' -> 'Al-Hijr:45'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:55:44,169 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 09:55:44,169 - INFO - Normalized predicted verses: ['15:45', '2:255', '54:10', '6:104', '7:113']
2025-07-07 09:55:44,169 - INFO - Normalized ground truth verses: ['27:56', '29:24', '29:29', '7:82']
2025-07-07 09:55:44,169 - INFO -   Response: 759 chars, Found 5 verses
2025-07-07 09:55:44,169 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:55:44,169 - INFO -   Matched verses: []
2025-07-07 09:55:45,171 - INFO - Processing question 227/881 (ID: 245) for qwen
2025-07-07 09:55:45,171 - INFO -   Ground truth verses: ['7:82', '29:24', '29:29', '27:56']
2025-07-07 09:55:45,171 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:56:01,077 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:56:01,077 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:56:01,077 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:56:01,077 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:56:01,078 - INFO - Normalized predicted verses: []
2025-07-07 09:56:01,078 - INFO - Normalized ground truth verses: ['27:56', '29:24', '29:29', '7:82']
2025-07-07 09:56:01,078 - INFO -   Response: 3433 chars, Found 0 verses
2025-07-07 09:56:01,078 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:56:01,078 - INFO -   Matched verses: []
2025-07-07 09:56:02,079 - INFO - Processing question 228/881 (ID: 246) for qwen
2025-07-07 09:56:02,079 - INFO -   Ground truth verses: ['56:53', '37:66']
2025-07-07 09:56:02,079 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:56:04,892 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 09:56:04,895 - INFO - Loaded 6236 verses for validation
2025-07-07 09:56:04,896 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 09:56:04,896 - INFO - Models to test: gemma
2025-07-07 09:56:04,896 - INFO - Questions limit: All
2025-07-07 09:56:04,896 - INFO - Using device: cuda
2025-07-07 09:56:05,801 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 09:56:05,801 - INFO - 
============================================================
2025-07-07 09:56:05,801 - INFO - Starting evaluation for GEMMA
2025-07-07 09:56:05,801 - INFO - ============================================================
2025-07-07 09:56:05,801 - INFO - Starting benchmark for GEMMA
2025-07-07 09:56:05,801 - INFO - Using device: cuda
2025-07-07 09:56:05,801 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 09:56:05,802 - INFO - Processing 881 questions for gemma (starting from 1)
2025-07-07 09:56:05,802 - INFO - Processing question 1/881 (ID: 0) for gemma
2025-07-07 09:56:05,802 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 09:56:05,803 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:05,803 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:06,036 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99b5-72b4bd69242750dd060e6f73;17179cd1-6830-47aa-a5b5-f664c28b267b)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:06,037 - ERROR - Failed to load model gemma
2025-07-07 09:56:06,037 - WARNING -   Failed to get response for question 0
2025-07-07 09:56:07,038 - INFO - Processing question 2/881 (ID: 1) for gemma
2025-07-07 09:56:07,038 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 09:56:07,038 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:07,038 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:07,234 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99b7-57fa64754bb7afb864afe079;f262ad18-d1ac-4f6e-9ebf-62aecb6ca414)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:07,235 - ERROR - Failed to load model gemma
2025-07-07 09:56:07,235 - WARNING -   Failed to get response for question 1
2025-07-07 09:56:08,236 - INFO - Processing question 3/881 (ID: 2) for gemma
2025-07-07 09:56:08,236 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 09:56:08,236 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:08,236 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:08,424 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99b8-47cf3f957c5552640eb40548;6ef15e71-0445-4771-a850-eb8131b1fb5b)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:08,425 - ERROR - Failed to load model gemma
2025-07-07 09:56:08,425 - WARNING -   Failed to get response for question 2
2025-07-07 09:56:09,426 - INFO - Processing question 4/881 (ID: 3) for gemma
2025-07-07 09:56:09,426 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 09:56:09,426 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:09,426 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:09,617 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99b9-3628e12b109a10cb4bbcfeb9;7f042924-917d-42f0-9044-e7fe1d2a685d)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:09,617 - ERROR - Failed to load model gemma
2025-07-07 09:56:09,617 - WARNING -   Failed to get response for question 3
2025-07-07 09:56:10,618 - INFO - Processing question 5/881 (ID: 4) for gemma
2025-07-07 09:56:10,619 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 09:56:10,619 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:10,619 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:10,809 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99ba-5abfc43f450ea0eb3f2d416a;35cca2c5-2efe-4abc-849e-6296f28bcc4d)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:10,809 - ERROR - Failed to load model gemma
2025-07-07 09:56:10,809 - WARNING -   Failed to get response for question 4
2025-07-07 09:56:10,813 - INFO - Saved data to results/gemma_checkpoint_4.json
2025-07-07 09:56:10,813 - INFO - Checkpoint saved for gemma at question 5
2025-07-07 09:56:10,813 - INFO -   Progress: 5/881
2025-07-07 09:56:10,813 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:56:11,814 - INFO - Processing question 6/881 (ID: 5) for gemma
2025-07-07 09:56:11,814 - INFO -   Ground truth verses: ['6:43']
2025-07-07 09:56:11,814 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:11,814 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:12,007 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99bb-6a9bf99039816a327cde7a8a;cc16971b-55e0-49e6-9404-d1319df9fb21)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:12,007 - ERROR - Failed to load model gemma
2025-07-07 09:56:12,007 - WARNING -   Failed to get response for question 5
2025-07-07 09:56:13,008 - INFO - Processing question 7/881 (ID: 6) for gemma
2025-07-07 09:56:13,008 - INFO -   Ground truth verses: ['6:43']
2025-07-07 09:56:13,008 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:13,008 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:13,365 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99bd-1c4182f265702e7f484f4cfa;bf12ca22-d1ac-4134-a314-a945c2b5a23a)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:13,365 - ERROR - Failed to load model gemma
2025-07-07 09:56:13,365 - WARNING -   Failed to get response for question 6
2025-07-07 09:56:14,367 - INFO - Processing question 8/881 (ID: 7) for gemma
2025-07-07 09:56:14,367 - INFO -   Ground truth verses: ['6:43']
2025-07-07 09:56:14,367 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:14,367 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:14,563 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99be-3210d8d82f6abdc7122e067b;7f53bc39-796e-4725-a21f-f9f56965312d)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:14,563 - ERROR - Failed to load model gemma
2025-07-07 09:56:14,563 - WARNING -   Failed to get response for question 7
2025-07-07 09:56:15,564 - INFO - Processing question 9/881 (ID: 8) for gemma
2025-07-07 09:56:15,564 - INFO -   Ground truth verses: ['6:43']
2025-07-07 09:56:15,564 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:15,565 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:15,778 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99bf-70363483713e28da2f018dc9;f7dc01a1-1fab-4621-b089-22d317e80668)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:15,778 - ERROR - Failed to load model gemma
2025-07-07 09:56:15,778 - WARNING -   Failed to get response for question 8
2025-07-07 09:56:16,779 - INFO - Processing question 10/881 (ID: 9) for gemma
2025-07-07 09:56:16,779 - INFO -   Ground truth verses: ['6:43']
2025-07-07 09:56:16,779 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:16,779 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:16,968 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99c0-3811b48d6dbd8da21520b8e6;b4aab7d1-4751-4b9b-b9a7-76c1bab12a86)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:16,968 - ERROR - Failed to load model gemma
2025-07-07 09:56:16,968 - WARNING -   Failed to get response for question 9
2025-07-07 09:56:16,971 - INFO - Saved data to results/gemma_checkpoint_9.json
2025-07-07 09:56:16,972 - INFO - Checkpoint saved for gemma at question 10
2025-07-07 09:56:16,972 - INFO -   Progress: 10/881
2025-07-07 09:56:16,972 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:56:17,831 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:56:17,831 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:56:17,831 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:56:17,831 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:56:17,831 - INFO - Normalized predicted verses: []
2025-07-07 09:56:17,832 - INFO - Normalized ground truth verses: ['37:66', '56:53']
2025-07-07 09:56:17,832 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 09:56:17,832 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:56:17,832 - INFO -   Matched verses: []
2025-07-07 09:56:17,973 - INFO - Processing question 11/881 (ID: 10) for gemma
2025-07-07 09:56:17,973 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 09:56:17,973 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:17,973 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:18,167 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99c2-09d52f440f5f1c940fa24cef;a89cdc9a-7e77-4366-a7d1-b2fe979e97a4)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:18,168 - ERROR - Failed to load model gemma
2025-07-07 09:56:18,168 - WARNING -   Failed to get response for question 10
2025-07-07 09:56:18,833 - INFO - Processing question 229/881 (ID: 247) for qwen
2025-07-07 09:56:18,833 - INFO -   Ground truth verses: ['56:53', '37:66']
2025-07-07 09:56:18,833 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:56:19,169 - INFO - Processing question 12/881 (ID: 11) for gemma
2025-07-07 09:56:19,169 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 09:56:19,169 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:19,169 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:19,363 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99c3-2026d3e4627017793b5dddd7;f2ecc601-db87-430f-b66a-82b8dbf73b3c)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:19,363 - ERROR - Failed to load model gemma
2025-07-07 09:56:19,363 - WARNING -   Failed to get response for question 11
2025-07-07 09:56:20,365 - INFO - Processing question 13/881 (ID: 12) for gemma
2025-07-07 09:56:20,365 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 09:56:20,365 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:20,365 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:20,559 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99c4-6610862c2bede46627202830;d3ccb4e2-f751-4228-b9e6-93d428cfdb7f)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:20,559 - ERROR - Failed to load model gemma
2025-07-07 09:56:20,559 - WARNING -   Failed to get response for question 12
2025-07-07 09:56:21,560 - INFO - Processing question 14/881 (ID: 13) for gemma
2025-07-07 09:56:21,560 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 09:56:21,560 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:21,560 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:21,755 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99c5-6a437aba26cf6c120d3700a2;b4c1b3d2-ad9c-43fd-a3f3-5a5db8293598)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:21,755 - ERROR - Failed to load model gemma
2025-07-07 09:56:21,755 - WARNING -   Failed to get response for question 13
2025-07-07 09:56:22,756 - INFO - Processing question 15/881 (ID: 14) for gemma
2025-07-07 09:56:22,756 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 09:56:22,756 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:22,756 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:22,954 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99c6-1f9557492dce9f9712001593;5ad6bf0b-8c40-46cf-ad8e-c5906271837d)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:22,954 - ERROR - Failed to load model gemma
2025-07-07 09:56:22,954 - WARNING -   Failed to get response for question 14
2025-07-07 09:56:22,958 - INFO - Saved data to results/gemma_checkpoint_14.json
2025-07-07 09:56:22,958 - INFO - Checkpoint saved for gemma at question 15
2025-07-07 09:56:22,958 - INFO -   Progress: 15/881
2025-07-07 09:56:22,958 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:56:23,959 - INFO - Processing question 16/881 (ID: 15) for gemma
2025-07-07 09:56:23,959 - INFO -   Ground truth verses: ['52:44']
2025-07-07 09:56:23,959 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:23,959 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:24,148 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99c8-347985a12b120691259c2a22;9b46d300-cb60-4824-820e-4ae976a2db13)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:24,148 - ERROR - Failed to load model gemma
2025-07-07 09:56:24,148 - WARNING -   Failed to get response for question 15
2025-07-07 09:56:25,149 - INFO - Processing question 17/881 (ID: 16) for gemma
2025-07-07 09:56:25,149 - INFO -   Ground truth verses: ['52:44']
2025-07-07 09:56:25,150 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:25,150 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:25,342 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99c9-20613a7d0be694df27ba3b71;40378979-9040-4e32-af9c-e405c291e748)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:25,342 - ERROR - Failed to load model gemma
2025-07-07 09:56:25,342 - WARNING -   Failed to get response for question 16
2025-07-07 09:56:26,344 - INFO - Processing question 18/881 (ID: 17) for gemma
2025-07-07 09:56:26,344 - INFO -   Ground truth verses: ['52:44']
2025-07-07 09:56:26,344 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:26,344 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:26,536 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99ca-44ed707763d2e93063f295a7;7479778f-94e4-43a0-9983-269234a3f3fa)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:26,536 - ERROR - Failed to load model gemma
2025-07-07 09:56:26,536 - WARNING -   Failed to get response for question 17
2025-07-07 09:56:27,538 - INFO - Processing question 19/881 (ID: 18) for gemma
2025-07-07 09:56:27,538 - INFO -   Ground truth verses: ['52:44']
2025-07-07 09:56:27,538 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:27,538 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:27,728 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99cb-64ba786c309cb7996efa4e3a;27b1f87b-2907-4d29-aea4-9648b6de970d)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:27,729 - ERROR - Failed to load model gemma
2025-07-07 09:56:27,729 - WARNING -   Failed to get response for question 18
2025-07-07 09:56:28,730 - INFO - Processing question 20/881 (ID: 19) for gemma
2025-07-07 09:56:28,730 - INFO -   Ground truth verses: ['52:44']
2025-07-07 09:56:28,730 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:28,730 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:28,918 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99cc-530cf33c2820fe15761d18d0;5b83bc86-32ae-4b21-bb5a-b039a791fe3a)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:28,918 - ERROR - Failed to load model gemma
2025-07-07 09:56:28,918 - WARNING -   Failed to get response for question 19
2025-07-07 09:56:28,923 - INFO - Saved data to results/gemma_checkpoint_19.json
2025-07-07 09:56:28,923 - INFO - Checkpoint saved for gemma at question 20
2025-07-07 09:56:28,923 - INFO -   Progress: 20/881
2025-07-07 09:56:28,923 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:56:29,924 - INFO - Processing question 21/881 (ID: 20) for gemma
2025-07-07 09:56:29,924 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 09:56:29,924 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:29,924 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:30,116 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99ce-4d83454e484566ce29a0bc7c;181b66d7-1f25-412e-a452-314e71d263cc)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:30,116 - ERROR - Failed to load model gemma
2025-07-07 09:56:30,116 - WARNING -   Failed to get response for question 20
2025-07-07 09:56:31,117 - INFO - Processing question 22/881 (ID: 21) for gemma
2025-07-07 09:56:31,118 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 09:56:31,118 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:31,118 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:31,468 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99cf-785e1b5732983b1f7f43f9b9;b6dae2f3-3c35-452f-83b5-68167d5a7ce4)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:31,468 - ERROR - Failed to load model gemma
2025-07-07 09:56:31,468 - WARNING -   Failed to get response for question 21
2025-07-07 09:56:32,469 - INFO - Processing question 23/881 (ID: 22) for gemma
2025-07-07 09:56:32,469 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 09:56:32,469 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:32,469 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:32,659 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99d0-33c7187c3b3b4fd962a6a04a;bbad3f2b-a12d-4db5-8896-2b2a0c482d2c)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:32,659 - ERROR - Failed to load model gemma
2025-07-07 09:56:32,659 - WARNING -   Failed to get response for question 22
2025-07-07 09:56:33,660 - INFO - Processing question 24/881 (ID: 23) for gemma
2025-07-07 09:56:33,660 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 09:56:33,660 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:33,660 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:33,852 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99d1-562939cd04086a1e29fac46e;a2c2b02b-b814-4b35-ab61-05f6cea78d1e)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:33,853 - ERROR - Failed to load model gemma
2025-07-07 09:56:33,853 - WARNING -   Failed to get response for question 23
2025-07-07 09:56:34,563 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:56:34,566 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:56:34,566 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:56:34,566 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:56:34,566 - INFO - Normalized predicted verses: []
2025-07-07 09:56:34,566 - INFO - Normalized ground truth verses: ['37:66', '56:53']
2025-07-07 09:56:34,566 - INFO -   Response: 4073 chars, Found 0 verses
2025-07-07 09:56:34,566 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:56:34,566 - INFO -   Matched verses: []
2025-07-07 09:56:34,854 - INFO - Processing question 25/881 (ID: 24) for gemma
2025-07-07 09:56:34,854 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 09:56:34,854 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:34,854 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:35,045 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99d2-40fbf76e59c9a9e6075c61cb;5a778828-9c02-4e1b-9bd6-0594895f7cb3)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:35,045 - ERROR - Failed to load model gemma
2025-07-07 09:56:35,045 - WARNING -   Failed to get response for question 24
2025-07-07 09:56:35,048 - INFO - Saved data to results/gemma_checkpoint_24.json
2025-07-07 09:56:35,048 - INFO - Checkpoint saved for gemma at question 25
2025-07-07 09:56:35,048 - INFO -   Progress: 25/881
2025-07-07 09:56:35,049 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:56:35,567 - INFO - Processing question 230/881 (ID: 249) for qwen
2025-07-07 09:56:35,568 - INFO -   Ground truth verses: ['56:53', '37:66']
2025-07-07 09:56:35,568 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:56:36,050 - INFO - Processing question 26/881 (ID: 30) for gemma
2025-07-07 09:56:36,050 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 09:56:36,050 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:36,050 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:36,244 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99d4-13f17ed24de61ac90521ae52;d4b94fe9-ac28-46d7-9e61-e42d01a30bcf)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:36,244 - ERROR - Failed to load model gemma
2025-07-07 09:56:36,244 - WARNING -   Failed to get response for question 30
2025-07-07 09:56:37,246 - INFO - Processing question 27/881 (ID: 31) for gemma
2025-07-07 09:56:37,246 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 09:56:37,246 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:37,246 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:37,436 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99d5-2237896c6c665a7e4202f855;e920647a-e3d7-497c-93c1-5d2854d3a155)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:37,436 - ERROR - Failed to load model gemma
2025-07-07 09:56:37,436 - WARNING -   Failed to get response for question 31
2025-07-07 09:56:38,437 - INFO - Processing question 28/881 (ID: 32) for gemma
2025-07-07 09:56:38,437 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 09:56:38,437 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:38,437 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:38,628 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99d6-006d1d9f588b2f6930a49a93;063e39f7-53b3-4878-94d2-f82b1dfeaec5)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:38,629 - ERROR - Failed to load model gemma
2025-07-07 09:56:38,629 - WARNING -   Failed to get response for question 32
2025-07-07 09:56:39,630 - INFO - Processing question 29/881 (ID: 33) for gemma
2025-07-07 09:56:39,630 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 09:56:39,630 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:39,630 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:39,823 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99d7-2ab42faf61f1ab3a42cf280c;865de99f-32bb-46e6-8b4b-4a9d19ddba75)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:39,823 - ERROR - Failed to load model gemma
2025-07-07 09:56:39,823 - WARNING -   Failed to get response for question 33
2025-07-07 09:56:40,824 - INFO - Processing question 30/881 (ID: 34) for gemma
2025-07-07 09:56:40,824 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 09:56:40,824 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:56:40,824 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:56:41,014 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b99d8-6f4baaac6e79a3bb7977fb00;8f3f0933-73cb-46a8-afee-9e94d7225c4b)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:56:41,014 - ERROR - Failed to load model gemma
2025-07-07 09:56:41,014 - WARNING -   Failed to get response for question 34
2025-07-07 09:56:41,018 - INFO - Saved data to results/gemma_checkpoint_29.json
2025-07-07 09:56:41,018 - INFO - Checkpoint saved for gemma at question 30
2025-07-07 09:56:41,018 - INFO -   Progress: 30/881
2025-07-07 09:56:41,018 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:56:51,119 - INFO - Extracted JSON from code block: 754 characters
2025-07-07 09:56:51,119 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 15 column 13 (char 384), falling back to regex parsing
2025-07-07 09:56:51,119 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 15 column 13 (char 384), falling back to regex parsing
2025-07-07 09:56:51,119 - WARNING - Failed to parse JSON response: Expecting ',' delimiter: line 15 column 13 (char 384), falling back to regex parsing
2025-07-07 09:56:51,119 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:56:51,119 - INFO - Normalized predicted verses: []
2025-07-07 09:56:51,119 - INFO - Normalized ground truth verses: ['37:66', '56:53']
2025-07-07 09:56:51,119 - INFO -   Response: 754 chars, Found 0 verses
2025-07-07 09:56:51,119 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:56:51,119 - INFO -   Matched verses: []
2025-07-07 09:56:51,140 - INFO - Saved data to results/qwen_checkpoint_229.json
2025-07-07 09:56:51,141 - INFO - Checkpoint saved for qwen at question 230
2025-07-07 09:56:51,141 - INFO -   Progress: 230/881
2025-07-07 09:56:51,141 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 09:56:52,142 - INFO - Processing question 231/881 (ID: 250) for qwen
2025-07-07 09:56:52,142 - INFO -   Ground truth verses: ['56:53', '37:66']
2025-07-07 09:56:52,142 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:57:07,852 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 09:57:07,852 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 09:57:07,852 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 09:57:07,852 - INFO - Normalized predicted verses: []
2025-07-07 09:57:07,852 - INFO - Normalized ground truth verses: ['37:66', '56:53']
2025-07-07 09:57:07,852 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 09:57:07,852 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:57:07,852 - INFO -   Matched verses: []
2025-07-07 09:57:08,853 - INFO - Processing question 232/881 (ID: 256) for qwen
2025-07-07 09:57:08,853 - INFO -   Ground truth verses: ['17:30', '34:39', '39:52', '30:37', '42:12', '13:26', '29:62', '30:6', '40:57', '34:28', '34:36']
2025-07-07 09:57:08,853 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:57:24,549 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:57:24,549 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:57:24,549 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:57:24,549 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:57:24,549 - INFO - Normalized predicted verses: []
2025-07-07 09:57:24,549 - INFO - Normalized ground truth verses: ['13:26', '17:30', '29:62', '30:37', '30:6', '34:28', '34:36', '34:39', '39:52', '40:57', '42:12']
2025-07-07 09:57:24,549 - INFO -   Response: 4416 chars, Found 0 verses
2025-07-07 09:57:24,549 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:57:24,549 - INFO -   Matched verses: []
2025-07-07 09:57:25,551 - INFO - Processing question 233/881 (ID: 257) for qwen
2025-07-07 09:57:25,551 - INFO -   Ground truth verses: ['17:30', '34:39', '39:52', '30:37', '42:12', '13:26', '29:62', '30:6', '40:57', '34:28', '34:36']
2025-07-07 09:57:25,551 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:57:34,269 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 09:57:34,272 - INFO - Loaded 6236 verses for validation
2025-07-07 09:57:34,272 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 09:57:34,273 - INFO - Models to test: gemma
2025-07-07 09:57:34,273 - INFO - Questions limit: All
2025-07-07 09:57:34,273 - INFO - Using device: cuda
2025-07-07 09:57:34,458 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 09:57:34,458 - INFO - 
============================================================
2025-07-07 09:57:34,458 - INFO - Starting evaluation for GEMMA
2025-07-07 09:57:34,458 - INFO - ============================================================
2025-07-07 09:57:34,458 - INFO - Starting benchmark for GEMMA
2025-07-07 09:57:34,458 - INFO - Using device: cuda
2025-07-07 09:57:34,458 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 09:57:34,460 - INFO - Loaded checkpoint for gemma: 30 questions completed
2025-07-07 09:57:34,460 - INFO - Processing 851 questions for gemma (starting from 31)
2025-07-07 09:57:34,460 - INFO - Processing question 31/881 (ID: 35) for gemma
2025-07-07 09:57:34,460 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 09:57:34,460 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:57:34,460 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:57:34,672 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9a0e-2296b5ef7536dce52014afe5;21ec2322-fb62-4f9b-ae7d-a8f201d28211)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:57:34,672 - ERROR - Failed to load model gemma
2025-07-07 09:57:34,672 - WARNING -   Failed to get response for question 35
2025-07-07 09:57:35,673 - INFO - Processing question 32/881 (ID: 36) for gemma
2025-07-07 09:57:35,673 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 09:57:35,673 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:57:35,673 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:57:35,864 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9a0f-2d6181a50f93fdb52d13ae06;a0248ec1-6c38-43ae-9fdc-1bd292a7a7a2)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:57:35,864 - ERROR - Failed to load model gemma
2025-07-07 09:57:35,864 - WARNING -   Failed to get response for question 36
2025-07-07 09:57:36,865 - INFO - Processing question 33/881 (ID: 37) for gemma
2025-07-07 09:57:36,865 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 09:57:36,866 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:57:36,866 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:57:37,060 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9a11-0aac1c30236c406a188dd1da;e21f8d12-5ad0-4898-84d6-cf2d13bc1701)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:57:37,060 - ERROR - Failed to load model gemma
2025-07-07 09:57:37,060 - WARNING -   Failed to get response for question 37
2025-07-07 09:57:38,061 - INFO - Processing question 34/881 (ID: 38) for gemma
2025-07-07 09:57:38,061 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 09:57:38,061 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:57:38,061 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:57:38,285 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9a12-728f641740adfa7d0a341ffd;dff99db6-5379-4fe5-a798-a18e0cfe8b57)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:57:38,285 - ERROR - Failed to load model gemma
2025-07-07 09:57:38,285 - WARNING -   Failed to get response for question 38
2025-07-07 09:57:39,286 - INFO - Processing question 35/881 (ID: 39) for gemma
2025-07-07 09:57:39,287 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 09:57:39,287 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:57:39,287 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:57:39,478 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9a13-7799e1fc67a437c06fc43944;5dc66aa6-04bd-4c1b-b3b1-47369e45c4db)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:57:39,478 - ERROR - Failed to load model gemma
2025-07-07 09:57:39,478 - WARNING -   Failed to get response for question 39
2025-07-07 09:57:39,483 - INFO - Saved data to results/gemma_checkpoint_34.json
2025-07-07 09:57:39,483 - INFO - Checkpoint saved for gemma at question 35
2025-07-07 09:57:39,483 - INFO -   Progress: 35/881
2025-07-07 09:57:39,483 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:57:40,484 - INFO - Processing question 36/881 (ID: 40) for gemma
2025-07-07 09:57:40,485 - INFO -   Ground truth verses: ['73:11']
2025-07-07 09:57:40,485 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:57:40,485 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:57:40,677 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9a14-73bf6bbc092031c444bddf54;604c1a50-2ef2-401f-8426-12657e126cda)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:57:40,677 - ERROR - Failed to load model gemma
2025-07-07 09:57:40,677 - WARNING -   Failed to get response for question 40
2025-07-07 09:57:41,231 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 09:57:41,231 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 09:57:41,231 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:57:41,231 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:57:41,231 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 09:57:41,231 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:57:41,231 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:57:41,231 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:57:41,231 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:57:41,231 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 09:57:41,231 - INFO - Normalized ground truth verses: ['13:26', '17:30', '29:62', '30:37', '30:6', '34:28', '34:36', '34:39', '39:52', '40:57', '42:12']
2025-07-07 09:57:41,231 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 09:57:41,231 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:57:41,232 - INFO -   Matched verses: []
2025-07-07 09:57:41,678 - INFO - Processing question 37/881 (ID: 41) for gemma
2025-07-07 09:57:41,678 - INFO -   Ground truth verses: ['73:11']
2025-07-07 09:57:41,678 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:57:41,679 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:57:41,870 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9a15-7a57db2c15559988478cb93e;c6fbee9f-68d1-44b5-8ce6-f44e3faa7cd3)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:57:41,870 - ERROR - Failed to load model gemma
2025-07-07 09:57:41,870 - WARNING -   Failed to get response for question 41
2025-07-07 09:57:42,233 - INFO - Processing question 234/881 (ID: 258) for qwen
2025-07-07 09:57:42,233 - INFO -   Ground truth verses: ['17:30', '34:39', '39:52', '30:37', '42:12', '13:26', '29:62', '30:6', '40:57', '34:28', '34:36']
2025-07-07 09:57:42,233 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:57:42,871 - INFO - Processing question 38/881 (ID: 42) for gemma
2025-07-07 09:57:42,871 - INFO -   Ground truth verses: ['73:11']
2025-07-07 09:57:42,871 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:57:42,871 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:57:43,067 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9a17-1060d1f13880082937fb8e7a;09903ac4-311f-45c9-b1b3-d16fd09e2ff2)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:57:43,067 - ERROR - Failed to load model gemma
2025-07-07 09:57:43,067 - WARNING -   Failed to get response for question 42
2025-07-07 09:57:44,068 - INFO - Processing question 39/881 (ID: 43) for gemma
2025-07-07 09:57:44,068 - INFO -   Ground truth verses: ['73:11']
2025-07-07 09:57:44,068 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 09:57:44,068 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 09:57:44,262 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9a18-7968c0626c0e9c832996c018;531f4a2f-daa6-42cd-a9c5-f46135dcad84)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 09:57:44,262 - ERROR - Failed to load model gemma
2025-07-07 09:57:44,262 - WARNING -   Failed to get response for question 43
2025-07-07 09:57:57,661 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:57:57,662 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:57:57,662 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:57:57,662 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:57:57,662 - INFO - Normalized predicted verses: []
2025-07-07 09:57:57,662 - INFO - Normalized ground truth verses: ['13:26', '17:30', '29:62', '30:37', '30:6', '34:28', '34:36', '34:39', '39:52', '40:57', '42:12']
2025-07-07 09:57:57,662 - INFO -   Response: 4186 chars, Found 0 verses
2025-07-07 09:57:57,662 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:57:57,662 - INFO -   Matched verses: []
2025-07-07 09:57:58,663 - INFO - Processing question 235/881 (ID: 259) for qwen
2025-07-07 09:57:58,663 - INFO -   Ground truth verses: ['17:30', '34:39', '39:52', '30:37', '42:12', '13:26', '29:62', '30:6', '40:57', '34:28', '34:36']
2025-07-07 09:57:58,663 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:58:14,384 - INFO - Extracted JSON from code block: 753 characters
2025-07-07 09:58:14,384 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:58:14,384 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Hijr:6]' -> 'Al-Hijr:6'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:58:14,384 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:58:14,384 - INFO - Normalized predicted verses: ['15:6', '2:255', '4:85', '57:5', '5:10']
2025-07-07 09:58:14,384 - INFO - Normalized ground truth verses: ['13:26', '17:30', '29:62', '30:37', '30:6', '34:28', '34:36', '34:39', '39:52', '40:57', '42:12']
2025-07-07 09:58:14,385 - INFO -   Response: 753 chars, Found 5 verses
2025-07-07 09:58:14,385 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:58:14,385 - INFO -   Matched verses: []
2025-07-07 09:58:14,406 - INFO - Saved data to results/qwen_checkpoint_234.json
2025-07-07 09:58:14,406 - INFO - Checkpoint saved for qwen at question 235
2025-07-07 09:58:14,406 - INFO -   Progress: 235/881
2025-07-07 09:58:14,406 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 09:58:15,407 - INFO - Processing question 236/881 (ID: 260) for qwen
2025-07-07 09:58:15,407 - INFO -   Ground truth verses: ['17:30', '34:39', '39:52', '30:37', '42:12', '13:26', '29:62', '30:6', '40:57', '34:28', '34:36']
2025-07-07 09:58:15,408 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:58:31,319 - INFO - Extracted JSON from code block: 757 characters
2025-07-07 09:58:31,319 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:58:31,319 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Ma'idah:11]' -> 'Al-Ma'idah:11'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:58:31,319 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 09:58:31,319 - INFO - Normalized predicted verses: ['15:15', '2:255', '57:5', '5:11', '7:113']
2025-07-07 09:58:31,319 - INFO - Normalized ground truth verses: ['13:26', '17:30', '29:62', '30:37', '30:6', '34:28', '34:36', '34:39', '39:52', '40:57', '42:12']
2025-07-07 09:58:31,319 - INFO -   Response: 757 chars, Found 5 verses
2025-07-07 09:58:31,319 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:58:31,319 - INFO -   Matched verses: []
2025-07-07 09:58:32,321 - INFO - Processing question 237/881 (ID: 261) for qwen
2025-07-07 09:58:32,321 - INFO -   Ground truth verses: ['19:3', '26:10']
2025-07-07 09:58:32,321 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:58:48,191 - INFO - Extracted JSON from code block: 749 characters
2025-07-07 09:58:48,191 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 09:58:48,191 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 09:58:48,191 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 09:58:48,191 - INFO - Normalized predicted verses: ['6:84', '6:85', '6:86', '6:87', '6:88']
2025-07-07 09:58:48,191 - INFO - Normalized ground truth verses: ['19:3', '26:10']
2025-07-07 09:58:48,192 - INFO -   Response: 749 chars, Found 5 verses
2025-07-07 09:58:48,192 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:58:48,192 - INFO -   Matched verses: []
2025-07-07 09:58:49,193 - INFO - Processing question 238/881 (ID: 262) for qwen
2025-07-07 09:58:49,193 - INFO -   Ground truth verses: ['19:3', '26:10']
2025-07-07 09:58:49,193 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:59:05,023 - INFO - Extracted JSON from code block: 757 characters
2025-07-07 09:59:05,023 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Hijr:52]' -> 'Al-Hijr:52'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:59:05,023 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Ma'idah:52]' -> 'Al-Ma'idah:52'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Hijr:52]' -> 'Al-Hijr:52'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Hijr:52]' -> 'Al-Hijr:52'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:59:05,023 - WARNING - Invalid colon format: '[Al-Qamar:55]' -> 'Al-Qamar:55'
2025-07-07 09:59:05,023 - INFO - Normalized predicted verses: ['15:52', '2:255', '55:55', '5:52', '6:84']
2025-07-07 09:59:05,023 - INFO - Normalized ground truth verses: ['19:3', '26:10']
2025-07-07 09:59:05,023 - INFO -   Response: 757 chars, Found 5 verses
2025-07-07 09:59:05,023 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:59:05,023 - INFO -   Matched verses: []
2025-07-07 09:59:06,025 - INFO - Processing question 239/881 (ID: 263) for qwen
2025-07-07 09:59:06,025 - INFO -   Ground truth verses: ['19:3', '26:10']
2025-07-07 09:59:06,025 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:59:21,717 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:59:21,718 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:59:21,718 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 09:59:21,718 - INFO - Regex fallback created 0 detailed verses
2025-07-07 09:59:21,718 - INFO - Normalized predicted verses: []
2025-07-07 09:59:21,718 - INFO - Normalized ground truth verses: ['19:3', '26:10']
2025-07-07 09:59:21,718 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 09:59:21,718 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:59:21,718 - INFO -   Matched verses: []
2025-07-07 09:59:22,719 - INFO - Processing question 240/881 (ID: 264) for qwen
2025-07-07 09:59:22,719 - INFO -   Ground truth verses: ['19:3', '26:10']
2025-07-07 09:59:22,719 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:59:38,507 - INFO - Extracted JSON from code block: 749 characters
2025-07-07 09:59:38,507 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:59:38,507 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:59:38,507 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:59:38,507 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 09:59:38,508 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 09:59:38,508 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 09:59:38,508 - INFO - Normalized predicted verses: ['6:84', '6:85', '6:86', '6:87', '6:88']
2025-07-07 09:59:38,508 - INFO - Normalized ground truth verses: ['19:3', '26:10']
2025-07-07 09:59:38,508 - INFO -   Response: 749 chars, Found 5 verses
2025-07-07 09:59:38,508 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:59:38,508 - INFO -   Matched verses: []
2025-07-07 09:59:38,529 - INFO - Saved data to results/qwen_checkpoint_239.json
2025-07-07 09:59:38,529 - INFO - Checkpoint saved for qwen at question 240
2025-07-07 09:59:38,529 - INFO -   Progress: 240/881
2025-07-07 09:59:38,529 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 09:59:39,530 - INFO - Processing question 241/881 (ID: 265) for qwen
2025-07-07 09:59:39,530 - INFO -   Ground truth verses: ['19:3', '26:10']
2025-07-07 09:59:39,530 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 09:59:55,326 - INFO - Extracted JSON from code block: 760 characters
2025-07-07 09:59:55,326 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 09:59:55,326 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:59:55,326 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:59:55,326 - WARNING - Invalid colon format: '[Al-An'am:165]' -> 'Al-An'am:165'
2025-07-07 09:59:55,326 - WARNING - Invalid colon format: '[Al-An'am:166]' -> 'Al-An'am:166'
2025-07-07 09:59:55,326 - WARNING - Invalid colon format: '[Al-An'am:167]' -> 'Al-An'am:167'
2025-07-07 09:59:55,327 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 09:59:55,327 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:59:55,327 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 09:59:55,327 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:59:55,327 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 09:59:55,327 - WARNING - Invalid colon format: '[Al-An'am:165]' -> 'Al-An'am:165'
2025-07-07 09:59:55,327 - WARNING - Invalid colon format: '[Al-An'am:165]' -> 'Al-An'am:165'
2025-07-07 09:59:55,327 - WARNING - Invalid colon format: '[Al-An'am:166]' -> 'Al-An'am:166'
2025-07-07 09:59:55,327 - WARNING - Invalid colon format: '[Al-An'am:166]' -> 'Al-An'am:166'
2025-07-07 09:59:55,327 - WARNING - Invalid colon format: '[Al-An'am:167]' -> 'Al-An'am:167'
2025-07-07 09:59:55,327 - WARNING - Invalid colon format: '[Al-An'am:167]' -> 'Al-An'am:167'
2025-07-07 09:59:55,327 - INFO - Normalized predicted verses: ['2:255', '4:165', '4:166', '4:167', '4:89']
2025-07-07 09:59:55,327 - INFO - Normalized ground truth verses: ['19:3', '26:10']
2025-07-07 09:59:55,327 - INFO -   Response: 760 chars, Found 5 verses
2025-07-07 09:59:55,327 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 09:59:55,327 - INFO -   Matched verses: []
2025-07-07 09:59:56,328 - INFO - Processing question 242/881 (ID: 266) for qwen
2025-07-07 09:59:56,328 - INFO -   Ground truth verses: ['98:8', '9:100', '9:89', '4:13', '64:9', '4:122', '20:76', '5:85', '9:72', '4:57', '85:11', '48:5', '57:12', '3:136', '58:22', '61:12', '14:23', '3:198', '3:15', '5:119']
2025-07-07 09:59:56,328 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:00:12,051 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:00:12,051 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:00:12,051 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:00:12,051 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:00:12,051 - INFO - Normalized predicted verses: []
2025-07-07 10:00:12,051 - INFO - Normalized ground truth verses: ['14:23', '20:76', '3:136', '3:15', '3:198', '48:5', '4:122', '4:13', '4:57', '57:12', '58:22', '5:119', '5:85', '61:12', '64:9', '85:11', '98:8', '9:100', '9:72', '9:89']
2025-07-07 10:00:12,051 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 10:00:12,051 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:00:12,051 - INFO -   Matched verses: []
2025-07-07 10:00:13,053 - INFO - Processing question 243/881 (ID: 267) for qwen
2025-07-07 10:00:13,053 - INFO -   Ground truth verses: ['98:8', '9:100', '9:89', '4:13', '64:9', '4:122', '20:76', '5:85', '9:72', '4:57', '85:11', '48:5', '57:12', '3:136', '58:22', '61:12', '14:23', '3:198', '3:15', '5:119']
2025-07-07 10:00:13,053 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:00:29,012 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:00:29,012 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:00:29,012 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:00:29,012 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:00:29,012 - INFO - Normalized predicted verses: []
2025-07-07 10:00:29,012 - INFO - Normalized ground truth verses: ['14:23', '20:76', '3:136', '3:15', '3:198', '48:5', '4:122', '4:13', '4:57', '57:12', '58:22', '5:119', '5:85', '61:12', '64:9', '85:11', '98:8', '9:100', '9:72', '9:89']
2025-07-07 10:00:29,012 - INFO -   Response: 4396 chars, Found 0 verses
2025-07-07 10:00:29,012 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:00:29,012 - INFO -   Matched verses: []
2025-07-07 10:00:30,013 - INFO - Processing question 244/881 (ID: 268) for qwen
2025-07-07 10:00:30,013 - INFO -   Ground truth verses: ['98:8', '9:100', '9:89', '4:13', '64:9', '4:122', '20:76', '5:85', '9:72', '4:57', '85:11', '48:5', '57:12', '3:136', '58:22', '61:12', '14:23', '3:198', '3:15', '5:119']
2025-07-07 10:00:30,013 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:00:45,807 - INFO - Extracted JSON from code block: 319 characters
2025-07-07 10:00:45,807 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 10:00:45,807 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:00:45,807 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 10:00:45,807 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 10:00:45,807 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:00:45,807 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:00:45,807 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 10:00:45,807 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 10:00:45,807 - INFO - Normalized predicted verses: ['2:255', '4:89']
2025-07-07 10:00:45,807 - INFO - Normalized ground truth verses: ['14:23', '20:76', '3:136', '3:15', '3:198', '48:5', '4:122', '4:13', '4:57', '57:12', '58:22', '5:119', '5:85', '61:12', '64:9', '85:11', '98:8', '9:100', '9:72', '9:89']
2025-07-07 10:00:45,807 - INFO -   Response: 319 chars, Found 2 verses
2025-07-07 10:00:45,807 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:00:45,807 - INFO -   Matched verses: []
2025-07-07 10:00:46,809 - INFO - Processing question 245/881 (ID: 269) for qwen
2025-07-07 10:00:46,809 - INFO -   Ground truth verses: ['98:8', '9:100', '9:89', '4:13', '64:9', '4:122', '20:76', '5:85', '9:72', '4:57', '85:11', '48:5', '57:12', '3:136', '58:22', '61:12', '14:23', '3:198', '3:15', '5:119']
2025-07-07 10:00:46,809 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:01:02,556 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 10:01:02,557 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:01:02,557 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:01:02,557 - INFO - Normalized predicted verses: []
2025-07-07 10:01:02,557 - INFO - Normalized ground truth verses: ['14:23', '20:76', '3:136', '3:15', '3:198', '48:5', '4:122', '4:13', '4:57', '57:12', '58:22', '5:119', '5:85', '61:12', '64:9', '85:11', '98:8', '9:100', '9:72', '9:89']
2025-07-07 10:01:02,557 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 10:01:02,557 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:01:02,557 - INFO -   Matched verses: []
2025-07-07 10:01:02,579 - INFO - Saved data to results/qwen_checkpoint_244.json
2025-07-07 10:01:02,579 - INFO - Checkpoint saved for qwen at question 245
2025-07-07 10:01:02,579 - INFO -   Progress: 245/881
2025-07-07 10:01:02,579 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 10:01:03,580 - INFO - Processing question 246/881 (ID: 270) for qwen
2025-07-07 10:01:03,580 - INFO -   Ground truth verses: ['98:8', '9:100', '9:89', '4:13', '64:9', '4:122', '20:76', '5:85', '9:72', '4:57', '85:11', '48:5', '57:12', '3:136', '58:22', '61:12', '14:23', '3:198', '3:15', '5:119']
2025-07-07 10:01:03,581 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:01:19,553 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:01:19,553 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:01:19,553 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:01:19,553 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:01:19,553 - INFO - Normalized predicted verses: []
2025-07-07 10:01:19,553 - INFO - Normalized ground truth verses: ['14:23', '20:76', '3:136', '3:15', '3:198', '48:5', '4:122', '4:13', '4:57', '57:12', '58:22', '5:119', '5:85', '61:12', '64:9', '85:11', '98:8', '9:100', '9:72', '9:89']
2025-07-07 10:01:19,553 - INFO -   Response: 4797 chars, Found 0 verses
2025-07-07 10:01:19,553 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:01:19,553 - INFO -   Matched verses: []
2025-07-07 10:01:20,554 - INFO - Processing question 247/881 (ID: 271) for qwen
2025-07-07 10:01:20,554 - INFO -   Ground truth verses: ['2:224']
2025-07-07 10:01:20,554 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:01:21,944 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 10:01:21,948 - INFO - Loaded 6236 verses for validation
2025-07-07 10:01:21,948 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 10:01:21,948 - INFO - Models to test: gemma
2025-07-07 10:01:21,948 - INFO - Questions limit: All
2025-07-07 10:01:21,948 - INFO - Using device: cuda
2025-07-07 10:01:22,157 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:01:22,157 - INFO - 
============================================================
2025-07-07 10:01:22,157 - INFO - Starting evaluation for GEMMA
2025-07-07 10:01:22,157 - INFO - ============================================================
2025-07-07 10:01:22,157 - INFO - Starting benchmark for GEMMA
2025-07-07 10:01:22,157 - INFO - Using device: cuda
2025-07-07 10:01:22,157 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:01:22,159 - INFO - Loaded checkpoint for gemma: 35 questions completed
2025-07-07 10:01:22,159 - INFO - Processing 846 questions for gemma (starting from 36)
2025-07-07 10:01:22,159 - INFO - Processing question 36/881 (ID: 40) for gemma
2025-07-07 10:01:22,159 - INFO -   Ground truth verses: ['73:11']
2025-07-07 10:01:22,159 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 10:01:22,159 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 10:01:22,402 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9af2-56242a835cb6097110ad93cf;a03bdb5c-2480-4573-840c-6753a0166782)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 10:01:22,402 - ERROR - Failed to load model gemma
2025-07-07 10:01:22,402 - WARNING -   Failed to get response for question 40
2025-07-07 10:01:23,404 - INFO - Processing question 37/881 (ID: 41) for gemma
2025-07-07 10:01:23,404 - INFO -   Ground truth verses: ['73:11']
2025-07-07 10:01:23,404 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 10:01:23,404 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 10:01:23,593 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9af3-38bef08902a6587b379f254b;669fcec2-fe81-48f7-a126-cd7ecb6bea96)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 10:01:23,593 - ERROR - Failed to load model gemma
2025-07-07 10:01:23,593 - WARNING -   Failed to get response for question 41
2025-07-07 10:01:24,594 - INFO - Processing question 38/881 (ID: 42) for gemma
2025-07-07 10:01:24,594 - INFO -   Ground truth verses: ['73:11']
2025-07-07 10:01:24,595 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 10:01:24,595 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 10:01:24,785 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9af4-4270f9e45807da6b71014e7f;f14d8184-2dee-4cfb-8843-e116011399e2)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 10:01:24,785 - ERROR - Failed to load model gemma
2025-07-07 10:01:24,785 - WARNING -   Failed to get response for question 42
2025-07-07 10:01:25,787 - INFO - Processing question 39/881 (ID: 43) for gemma
2025-07-07 10:01:25,787 - INFO -   Ground truth verses: ['73:11']
2025-07-07 10:01:25,787 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 10:01:25,787 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 10:01:36,364 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:01:36,364 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:01:36,364 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:01:36,364 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:01:36,364 - INFO - Normalized predicted verses: []
2025-07-07 10:01:36,364 - INFO - Normalized ground truth verses: ['2:224']
2025-07-07 10:01:36,364 - INFO -   Response: 4041 chars, Found 0 verses
2025-07-07 10:01:36,364 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:01:36,364 - INFO -   Matched verses: []
2025-07-07 10:01:37,365 - INFO - Processing question 248/881 (ID: 272) for qwen
2025-07-07 10:01:37,365 - INFO -   Ground truth verses: ['2:224']
2025-07-07 10:01:37,365 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:01:53,209 - INFO - Extracted JSON from code block: 313 characters
2025-07-07 10:01:53,210 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 10:01:53,210 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 10:01:53,210 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 10:01:53,210 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 10:01:53,210 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 10:01:53,210 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 10:01:53,210 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 10:01:53,210 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 10:01:53,210 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 10:01:53,210 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 10:01:53,210 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 10:01:53,210 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 10:01:53,210 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 10:01:53,210 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 10:01:53,210 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 10:01:53,210 - INFO - Normalized ground truth verses: ['2:224']
2025-07-07 10:01:53,210 - INFO -   Response: 313 chars, Found 2 verses
2025-07-07 10:01:53,210 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:01:53,210 - INFO -   Matched verses: []
2025-07-07 10:01:54,212 - INFO - Processing question 249/881 (ID: 273) for qwen
2025-07-07 10:01:54,212 - INFO -   Ground truth verses: ['2:224']
2025-07-07 10:01:54,212 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:01:54,929 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 10:01:54,933 - INFO - Loaded 6236 verses for validation
2025-07-07 10:01:54,933 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 10:01:54,933 - INFO - Models to test: gemma
2025-07-07 10:01:54,934 - INFO - Questions limit: All
2025-07-07 10:01:54,934 - INFO - Using device: cuda
2025-07-07 10:01:55,143 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:01:55,143 - INFO - 
============================================================
2025-07-07 10:01:55,143 - INFO - Starting evaluation for GEMMA
2025-07-07 10:01:55,143 - INFO - ============================================================
2025-07-07 10:01:55,143 - INFO - Starting benchmark for GEMMA
2025-07-07 10:01:55,143 - INFO - Using device: cuda
2025-07-07 10:01:55,143 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:01:55,144 - INFO - Loaded checkpoint for gemma: 35 questions completed
2025-07-07 10:01:55,144 - INFO - Processing 846 questions for gemma (starting from 36)
2025-07-07 10:01:55,144 - INFO - Processing question 36/881 (ID: 40) for gemma
2025-07-07 10:01:55,144 - INFO -   Ground truth verses: ['73:11']
2025-07-07 10:01:55,144 - INFO - Loading model: google/gemma-3-4b-it
2025-07-07 10:01:55,144 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 10:01:55,359 - ERROR - Failed to load model google/gemma-3-4b-it: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.
401 Client Error. (Request ID: Root=1-686b9b13-36699e4e0eb1735120f45c43;30227eb0-5a6c-4462-96cd-368275452f01)

Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.
Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.
2025-07-07 10:01:55,359 - ERROR - Failed to load model gemma
2025-07-07 10:01:55,359 - WARNING -   Failed to get response for question 40
2025-07-07 10:02:09,924 - INFO - Extracted JSON from code block: 101 characters
2025-07-07 10:02:09,924 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:02:09,924 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:02:09,924 - INFO - Normalized predicted verses: []
2025-07-07 10:02:09,924 - INFO - Normalized ground truth verses: ['2:224']
2025-07-07 10:02:09,924 - INFO -   Response: 101 chars, Found 0 verses
2025-07-07 10:02:09,924 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:02:09,924 - INFO -   Matched verses: []
2025-07-07 10:02:10,926 - INFO - Processing question 250/881 (ID: 274) for qwen
2025-07-07 10:02:10,926 - INFO -   Ground truth verses: ['2:224']
2025-07-07 10:02:10,926 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:02:26,813 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 10:02:26,813 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:02:26,813 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:02:26,813 - INFO - Normalized predicted verses: []
2025-07-07 10:02:26,813 - INFO - Normalized ground truth verses: ['2:224']
2025-07-07 10:02:26,813 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 10:02:26,813 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:02:26,813 - INFO -   Matched verses: []
2025-07-07 10:02:26,835 - INFO - Saved data to results/qwen_checkpoint_249.json
2025-07-07 10:02:26,835 - INFO - Checkpoint saved for qwen at question 250
2025-07-07 10:02:26,835 - INFO -   Progress: 250/881
2025-07-07 10:02:26,835 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 10:02:27,836 - INFO - Processing question 251/881 (ID: 275) for qwen
2025-07-07 10:02:27,836 - INFO -   Ground truth verses: ['2:224']
2025-07-07 10:02:27,836 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:02:43,517 - INFO - Extracted JSON from code block: 754 characters
2025-07-07 10:02:43,517 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-An'am:6]' -> 'Al-An'am:6'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-Ma'idah:8]' -> 'Al-Ma'idah:8'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-An'am:152]' -> 'Al-An'am:152'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 10:02:43,517 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-An'am:6]' -> 'Al-An'am:6'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-An'am:6]' -> 'Al-An'am:6'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-Ma'idah:8]' -> 'Al-Ma'idah:8'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-Ma'idah:8]' -> 'Al-Ma'idah:8'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-Hujurat:11]' -> 'Al-Hujurat:11'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-An'am:152]' -> 'Al-An'am:152'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-An'am:152]' -> 'Al-An'am:152'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 10:02:43,517 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 10:02:43,517 - INFO - Normalized predicted verses: ['25:56', '49:11', '4:152', '4:6', '5:8']
2025-07-07 10:02:43,517 - INFO - Normalized ground truth verses: ['2:224']
2025-07-07 10:02:43,517 - INFO -   Response: 754 chars, Found 5 verses
2025-07-07 10:02:43,517 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:02:43,517 - INFO -   Matched verses: []
2025-07-07 10:02:44,519 - INFO - Processing question 252/881 (ID: 276) for qwen
2025-07-07 10:02:44,519 - INFO -   Ground truth verses: ['45:21', '29:4']
2025-07-07 10:02:44,519 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:03:00,260 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 10:03:00,260 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 10:03:00,260 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 10:03:00,260 - INFO - Normalized predicted verses: ['15:10', '2:255', '4:86', '50:10', '5:10']
2025-07-07 10:03:00,260 - INFO - Normalized ground truth verses: ['29:4', '45:21']
2025-07-07 10:03:00,260 - INFO -   Response: 4675 chars, Found 5 verses
2025-07-07 10:03:00,260 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:03:00,260 - INFO -   Matched verses: []
2025-07-07 10:03:01,262 - INFO - Processing question 253/881 (ID: 277) for qwen
2025-07-07 10:03:01,262 - INFO -   Ground truth verses: ['45:21', '29:4']
2025-07-07 10:03:01,262 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:03:17,119 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 10:03:17,119 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:03:17,119 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:03:17,119 - INFO - Normalized predicted verses: []
2025-07-07 10:03:17,119 - INFO - Normalized ground truth verses: ['29:4', '45:21']
2025-07-07 10:03:17,119 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 10:03:17,119 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:03:17,119 - INFO -   Matched verses: []
2025-07-07 10:03:18,121 - INFO - Processing question 254/881 (ID: 278) for qwen
2025-07-07 10:03:18,121 - INFO -   Ground truth verses: ['45:21', '29:4']
2025-07-07 10:03:18,121 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:03:33,823 - INFO - Extracted JSON from code block: 754 characters
2025-07-07 10:03:33,823 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 10:03:33,823 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 10:03:33,823 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 10:03:33,823 - INFO - Normalized predicted verses: ['15:15', '25:56', '4:85', '50:12', '5:16']
2025-07-07 10:03:33,823 - INFO - Normalized ground truth verses: ['29:4', '45:21']
2025-07-07 10:03:33,823 - INFO -   Response: 754 chars, Found 5 verses
2025-07-07 10:03:33,823 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:03:33,824 - INFO -   Matched verses: []
2025-07-07 10:03:34,825 - INFO - Processing question 255/881 (ID: 279) for qwen
2025-07-07 10:03:34,825 - INFO -   Ground truth verses: ['45:21', '29:4']
2025-07-07 10:03:34,825 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:03:50,487 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 10:03:50,487 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:03:50,487 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:03:50,487 - INFO - Normalized predicted verses: []
2025-07-07 10:03:50,487 - INFO - Normalized ground truth verses: ['29:4', '45:21']
2025-07-07 10:03:50,487 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 10:03:50,487 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:03:50,487 - INFO -   Matched verses: []
2025-07-07 10:03:50,510 - INFO - Saved data to results/qwen_checkpoint_254.json
2025-07-07 10:03:50,510 - INFO - Checkpoint saved for qwen at question 255
2025-07-07 10:03:50,510 - INFO -   Progress: 255/881
2025-07-07 10:03:50,510 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 10:03:51,511 - INFO - Processing question 256/881 (ID: 280) for qwen
2025-07-07 10:03:51,512 - INFO -   Ground truth verses: ['45:21', '29:4']
2025-07-07 10:03:51,512 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:04:07,364 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 874), falling back to regex parsing
2025-07-07 10:04:07,364 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 874), falling back to regex parsing
2025-07-07 10:04:07,364 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 874), falling back to regex parsing
2025-07-07 10:04:07,364 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:04:07,364 - INFO - Normalized predicted verses: []
2025-07-07 10:04:07,365 - INFO - Normalized ground truth verses: ['29:4', '45:21']
2025-07-07 10:04:07,365 - INFO -   Response: 4275 chars, Found 0 verses
2025-07-07 10:04:07,365 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:04:07,365 - INFO -   Matched verses: []
2025-07-07 10:04:08,366 - INFO - Processing question 257/881 (ID: 281) for qwen
2025-07-07 10:04:08,366 - INFO -   Ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:04:08,366 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:04:24,075 - INFO - Extracted JSON from code block: 753 characters
2025-07-07 10:04:24,075 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:04:24,075 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:04:24,075 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 10:04:24,075 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:04:24,075 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 10:04:24,075 - WARNING - Invalid colon format: '[Al-Zumar:10]' -> 'Al-Zumar:10'
2025-07-07 10:04:24,075 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:04:24,075 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:04:24,075 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:04:24,076 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 10:04:24,076 - WARNING - Invalid colon format: '[Al-Ma'idah:16]' -> 'Al-Ma'idah:16'
2025-07-07 10:04:24,076 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:04:24,076 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:04:24,076 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 10:04:24,076 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 10:04:24,076 - WARNING - Invalid colon format: '[Al-Zumar:10]' -> 'Al-Zumar:10'
2025-07-07 10:04:24,076 - WARNING - Invalid colon format: '[Al-Zumar:10]' -> 'Al-Zumar:10'
2025-07-07 10:04:24,076 - INFO - Normalized predicted verses: ['15:10', '39:10', '50:10', '5:16', '6:84']
2025-07-07 10:04:24,076 - INFO - Normalized ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:04:24,076 - INFO -   Response: 753 chars, Found 5 verses
2025-07-07 10:04:24,076 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:04:24,076 - INFO -   Matched verses: []
2025-07-07 10:04:25,077 - INFO - Processing question 258/881 (ID: 282) for qwen
2025-07-07 10:04:25,077 - INFO -   Ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:04:25,077 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:04:40,779 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 10:04:40,779 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:04:40,779 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:04:40,779 - INFO - Normalized predicted verses: []
2025-07-07 10:04:40,779 - INFO - Normalized ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:04:40,780 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 10:04:40,780 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:04:40,780 - INFO -   Matched verses: []
2025-07-07 10:04:41,781 - INFO - Processing question 259/881 (ID: 283) for qwen
2025-07-07 10:04:41,781 - INFO -   Ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:04:41,781 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:04:57,528 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:04:57,528 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:04:57,528 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:04:57,528 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:04:57,528 - INFO - Normalized predicted verses: []
2025-07-07 10:04:57,528 - INFO - Normalized ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:04:57,529 - INFO -   Response: 4733 chars, Found 0 verses
2025-07-07 10:04:57,529 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:04:57,529 - INFO -   Matched verses: []
2025-07-07 10:04:58,530 - INFO - Processing question 260/881 (ID: 284) for qwen
2025-07-07 10:04:58,530 - INFO -   Ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:04:58,530 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:05:14,297 - INFO - Extracted JSON from code block: 754 characters
2025-07-07 10:05:14,297 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-An'am:68]' -> 'Al-An'am:68'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Ma'idah:56]' -> 'Al-Ma'idah:56'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:05:14,297 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-An'am:68]' -> 'Al-An'am:68'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-An'am:68]' -> 'Al-An'am:68'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Ma'idah:56]' -> 'Al-Ma'idah:56'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Ma'idah:56]' -> 'Al-Ma'idah:56'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:05:14,297 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:05:14,298 - INFO - Normalized predicted verses: ['15:10', '2:255', '2:68', '5:5', '5:56']
2025-07-07 10:05:14,298 - INFO - Normalized ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:05:14,298 - INFO -   Response: 754 chars, Found 5 verses
2025-07-07 10:05:14,298 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:05:14,298 - INFO -   Matched verses: []
2025-07-07 10:05:14,319 - INFO - Saved data to results/qwen_checkpoint_259.json
2025-07-07 10:05:14,319 - INFO - Checkpoint saved for qwen at question 260
2025-07-07 10:05:14,319 - INFO -   Progress: 260/881
2025-07-07 10:05:14,319 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 10:05:15,320 - INFO - Processing question 261/881 (ID: 285) for qwen
2025-07-07 10:05:15,321 - INFO -   Ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:05:15,321 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:05:31,156 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:05:31,156 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:05:31,156 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:05:31,156 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:05:31,156 - INFO - Normalized predicted verses: []
2025-07-07 10:05:31,156 - INFO - Normalized ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:05:31,156 - INFO -   Response: 4362 chars, Found 0 verses
2025-07-07 10:05:31,156 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:05:31,156 - INFO -   Matched verses: []
2025-07-07 10:05:32,158 - INFO - Processing question 262/881 (ID: 291) for qwen
2025-07-07 10:05:32,158 - INFO -   Ground truth verses: ['51:27', '37:91']
2025-07-07 10:05:32,158 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:05:47,993 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:05:47,993 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:05:47,993 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:05:47,993 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:05:47,993 - INFO - Normalized predicted verses: []
2025-07-07 10:05:47,993 - INFO - Normalized ground truth verses: ['37:91', '51:27']
2025-07-07 10:05:47,993 - INFO -   Response: 4936 chars, Found 0 verses
2025-07-07 10:05:47,993 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:05:47,993 - INFO -   Matched verses: []
2025-07-07 10:05:48,994 - INFO - Processing question 263/881 (ID: 292) for qwen
2025-07-07 10:05:48,994 - INFO -   Ground truth verses: ['51:27', '37:91']
2025-07-07 10:05:48,994 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:06:04,742 - INFO - Extracted JSON from code block: 749 characters
2025-07-07 10:06:04,743 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 10:06:04,743 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 10:06:04,743 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 10:06:04,743 - INFO - Normalized predicted verses: ['6:84', '6:85', '6:86', '6:87', '6:88']
2025-07-07 10:06:04,743 - INFO - Normalized ground truth verses: ['37:91', '51:27']
2025-07-07 10:06:04,743 - INFO -   Response: 749 chars, Found 5 verses
2025-07-07 10:06:04,743 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:06:04,743 - INFO -   Matched verses: []
2025-07-07 10:06:05,745 - INFO - Processing question 264/881 (ID: 293) for qwen
2025-07-07 10:06:05,745 - INFO -   Ground truth verses: ['51:27', '37:91']
2025-07-07 10:06:05,745 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:06:21,233 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:06:21,233 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:06:21,233 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:06:21,233 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:06:21,233 - INFO - Normalized predicted verses: []
2025-07-07 10:06:21,233 - INFO - Normalized ground truth verses: ['37:91', '51:27']
2025-07-07 10:06:21,233 - INFO -   Response: 4015 chars, Found 0 verses
2025-07-07 10:06:21,233 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:06:21,234 - INFO -   Matched verses: []
2025-07-07 10:06:22,235 - INFO - Processing question 265/881 (ID: 294) for qwen
2025-07-07 10:06:22,235 - INFO -   Ground truth verses: ['51:27', '37:91']
2025-07-07 10:06:22,235 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:06:37,931 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 10:06:37,932 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:06:37,932 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:06:37,932 - INFO - Normalized predicted verses: []
2025-07-07 10:06:37,932 - INFO - Normalized ground truth verses: ['37:91', '51:27']
2025-07-07 10:06:37,932 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 10:06:37,932 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:06:37,932 - INFO -   Matched verses: []
2025-07-07 10:06:37,955 - INFO - Saved data to results/qwen_checkpoint_264.json
2025-07-07 10:06:37,955 - INFO - Checkpoint saved for qwen at question 265
2025-07-07 10:06:37,955 - INFO -   Progress: 265/881
2025-07-07 10:06:37,955 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 10:06:38,956 - INFO - Processing question 266/881 (ID: 295) for qwen
2025-07-07 10:06:38,957 - INFO -   Ground truth verses: ['51:27', '37:91']
2025-07-07 10:06:38,957 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:06:54,750 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 10:06:54,751 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:06:54,751 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:06:54,751 - INFO - Normalized predicted verses: []
2025-07-07 10:06:54,751 - INFO - Normalized ground truth verses: ['37:91', '51:27']
2025-07-07 10:06:54,751 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 10:06:54,751 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:06:54,751 - INFO -   Matched verses: []
2025-07-07 10:06:55,752 - INFO - Processing question 267/881 (ID: 302) for qwen
2025-07-07 10:06:55,752 - INFO -   Ground truth verses: ['81:22']
2025-07-07 10:06:55,752 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:07:11,410 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:07:11,410 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:07:11,410 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:07:11,410 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:07:11,410 - INFO - Normalized predicted verses: []
2025-07-07 10:07:11,410 - INFO - Normalized ground truth verses: ['81:22']
2025-07-07 10:07:11,410 - INFO -   Response: 4166 chars, Found 0 verses
2025-07-07 10:07:11,410 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:07:11,410 - INFO -   Matched verses: []
2025-07-07 10:07:12,411 - INFO - Processing question 268/881 (ID: 304) for qwen
2025-07-07 10:07:12,412 - INFO -   Ground truth verses: ['81:22']
2025-07-07 10:07:12,412 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:07:28,147 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 10:07:28,147 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 10:07:28,147 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:07:28,147 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:07:28,147 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 10:07:28,147 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:07:28,147 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:07:28,147 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:07:28,147 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:07:28,147 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 10:07:28,147 - INFO - Normalized ground truth verses: ['81:22']
2025-07-07 10:07:28,147 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 10:07:28,147 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:07:28,147 - INFO -   Matched verses: []
2025-07-07 10:07:29,149 - INFO - Processing question 269/881 (ID: 305) for qwen
2025-07-07 10:07:29,149 - INFO -   Ground truth verses: ['81:22']
2025-07-07 10:07:29,149 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:07:44,710 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:07:44,710 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:07:44,710 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:07:44,710 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:07:44,710 - INFO - Normalized predicted verses: []
2025-07-07 10:07:44,710 - INFO - Normalized ground truth verses: ['81:22']
2025-07-07 10:07:44,710 - INFO -   Response: 4555 chars, Found 0 verses
2025-07-07 10:07:44,710 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:07:44,710 - INFO -   Matched verses: []
2025-07-07 10:07:45,711 - INFO - Processing question 270/881 (ID: 306) for qwen
2025-07-07 10:07:45,711 - INFO -   Ground truth verses: ['19:36', '3:51', '43:64', '36:4', '15:41', '23:73', '43:61', '37:118', '4:68', '1:6', '36:61']
2025-07-07 10:07:45,711 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:08:01,473 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:08:01,473 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:08:01,474 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:08:01,474 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:08:01,474 - INFO - Normalized predicted verses: []
2025-07-07 10:08:01,474 - INFO - Normalized ground truth verses: ['15:41', '19:36', '1:6', '23:73', '36:4', '36:61', '37:118', '3:51', '43:61', '43:64', '4:68']
2025-07-07 10:08:01,474 - INFO -   Response: 3067 chars, Found 0 verses
2025-07-07 10:08:01,474 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:08:01,474 - INFO -   Matched verses: []
2025-07-07 10:08:01,499 - INFO - Saved data to results/qwen_checkpoint_269.json
2025-07-07 10:08:01,499 - INFO - Checkpoint saved for qwen at question 270
2025-07-07 10:08:01,499 - INFO -   Progress: 270/881
2025-07-07 10:08:01,499 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 10:08:02,500 - INFO - Processing question 271/881 (ID: 307) for qwen
2025-07-07 10:08:02,500 - INFO -   Ground truth verses: ['19:36', '3:51', '43:64', '36:4', '15:41', '23:73', '43:61', '37:118', '4:68', '1:6', '36:61']
2025-07-07 10:08:02,500 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:08:18,263 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:08:18,264 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:08:18,264 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:08:18,264 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:08:18,264 - INFO - Normalized predicted verses: []
2025-07-07 10:08:18,264 - INFO - Normalized ground truth verses: ['15:41', '19:36', '1:6', '23:73', '36:4', '36:61', '37:118', '3:51', '43:61', '43:64', '4:68']
2025-07-07 10:08:18,264 - INFO -   Response: 4265 chars, Found 0 verses
2025-07-07 10:08:18,264 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:08:18,264 - INFO -   Matched verses: []
2025-07-07 10:08:19,265 - INFO - Processing question 272/881 (ID: 308) for qwen
2025-07-07 10:08:19,265 - INFO -   Ground truth verses: ['19:36', '3:51', '43:64', '36:4', '15:41', '23:73', '43:61', '37:118', '4:68', '1:6', '36:61']
2025-07-07 10:08:19,265 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:08:35,091 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:08:35,091 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:08:35,091 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:08:35,091 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:08:35,091 - INFO - Normalized predicted verses: []
2025-07-07 10:08:35,091 - INFO - Normalized ground truth verses: ['15:41', '19:36', '1:6', '23:73', '36:4', '36:61', '37:118', '3:51', '43:61', '43:64', '4:68']
2025-07-07 10:08:35,092 - INFO -   Response: 3897 chars, Found 0 verses
2025-07-07 10:08:35,092 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:08:35,092 - INFO -   Matched verses: []
2025-07-07 10:08:36,093 - INFO - Processing question 273/881 (ID: 309) for qwen
2025-07-07 10:08:36,093 - INFO -   Ground truth verses: ['19:36', '3:51', '43:64', '36:4', '15:41', '23:73', '43:61', '37:118', '4:68', '1:6', '36:61']
2025-07-07 10:08:36,093 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:08:51,951 - INFO - Extracted JSON from code block: 756 characters
2025-07-07 10:08:51,952 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Ma'idah:56]' -> 'Al-Ma'idah:56'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Furqan:70]' -> 'Al-Furqan:70'
2025-07-07 10:08:51,952 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Ma'idah:56]' -> 'Al-Ma'idah:56'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Ma'idah:56]' -> 'Al-Ma'idah:56'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Furqan:70]' -> 'Al-Furqan:70'
2025-07-07 10:08:51,952 - WARNING - Invalid colon format: '[Al-Furqan:70]' -> 'Al-Furqan:70'
2025-07-07 10:08:51,952 - INFO - Normalized predicted verses: ['15:7', '25:70', '2:255', '4:85', '5:56']
2025-07-07 10:08:51,952 - INFO - Normalized ground truth verses: ['15:41', '19:36', '1:6', '23:73', '36:4', '36:61', '37:118', '3:51', '43:61', '43:64', '4:68']
2025-07-07 10:08:51,952 - INFO -   Response: 756 chars, Found 5 verses
2025-07-07 10:08:51,952 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:08:51,952 - INFO -   Matched verses: []
2025-07-07 10:08:52,953 - INFO - Processing question 274/881 (ID: 310) for qwen
2025-07-07 10:08:52,954 - INFO -   Ground truth verses: ['19:36', '3:51', '43:64', '36:4', '15:41', '23:73', '43:61', '37:118', '4:68', '1:6', '36:61']
2025-07-07 10:08:52,954 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:09:08,736 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:09:08,736 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:09:08,736 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:09:08,736 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:09:08,737 - INFO - Normalized predicted verses: []
2025-07-07 10:09:08,737 - INFO - Normalized ground truth verses: ['15:41', '19:36', '1:6', '23:73', '36:4', '36:61', '37:118', '3:51', '43:61', '43:64', '4:68']
2025-07-07 10:09:08,737 - INFO -   Response: 4046 chars, Found 0 verses
2025-07-07 10:09:08,737 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:09:08,737 - INFO -   Matched verses: []
2025-07-07 10:09:09,738 - INFO - Processing question 275/881 (ID: 311) for qwen
2025-07-07 10:09:09,738 - INFO -   Ground truth verses: ['46:16']
2025-07-07 10:09:09,738 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:09:25,516 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:09:25,516 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:09:25,516 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:09:25,516 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:09:25,517 - INFO - Normalized predicted verses: []
2025-07-07 10:09:25,517 - INFO - Normalized ground truth verses: ['46:16']
2025-07-07 10:09:25,517 - INFO -   Response: 4208 chars, Found 0 verses
2025-07-07 10:09:25,517 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:09:25,517 - INFO -   Matched verses: []
2025-07-07 10:09:25,541 - INFO - Saved data to results/qwen_checkpoint_274.json
2025-07-07 10:09:25,541 - INFO - Checkpoint saved for qwen at question 275
2025-07-07 10:09:25,541 - INFO -   Progress: 275/881
2025-07-07 10:09:25,541 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 10:09:26,542 - INFO - Processing question 276/881 (ID: 312) for qwen
2025-07-07 10:09:26,542 - INFO -   Ground truth verses: ['46:16']
2025-07-07 10:09:26,542 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:09:42,316 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 881), falling back to regex parsing
2025-07-07 10:09:42,316 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 881), falling back to regex parsing
2025-07-07 10:09:42,316 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 881), falling back to regex parsing
2025-07-07 10:09:42,317 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:09:42,317 - INFO - Normalized predicted verses: []
2025-07-07 10:09:42,317 - INFO - Normalized ground truth verses: ['46:16']
2025-07-07 10:09:42,317 - INFO -   Response: 3982 chars, Found 0 verses
2025-07-07 10:09:42,317 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:09:42,317 - INFO -   Matched verses: []
2025-07-07 10:09:43,318 - INFO - Processing question 277/881 (ID: 313) for qwen
2025-07-07 10:09:43,318 - INFO -   Ground truth verses: ['46:16']
2025-07-07 10:09:43,318 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:09:59,097 - INFO - Extracted JSON from code block: 761 characters
2025-07-07 10:09:59,097 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:09:59,097 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:09:59,097 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 10:09:59,097 - WARNING - Invalid colon format: '[Al-Ma'idah:39]' -> 'Al-Ma'idah:39'
2025-07-07 10:09:59,097 - WARNING - Invalid colon format: '[Al-Ikhlas:11]' -> 'Al-Ikhlas:11'
2025-07-07 10:09:59,097 - WARNING - Invalid colon format: '[Al-Falaq:116]' -> 'Al-Falaq:116'
2025-07-07 10:09:59,097 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:09:59,097 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:09:59,097 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:09:59,097 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 10:09:59,098 - WARNING - Invalid colon format: '[Al-An'am:89]' -> 'Al-An'am:89'
2025-07-07 10:09:59,098 - WARNING - Invalid colon format: '[Al-Ma'idah:39]' -> 'Al-Ma'idah:39'
2025-07-07 10:09:59,098 - WARNING - Invalid colon format: '[Al-Ma'idah:39]' -> 'Al-Ma'idah:39'
2025-07-07 10:09:59,098 - WARNING - Invalid colon format: '[Al-Ikhlas:11]' -> 'Al-Ikhlas:11'
2025-07-07 10:09:59,098 - WARNING - Invalid colon format: '[Al-Ikhlas:11]' -> 'Al-Ikhlas:11'
2025-07-07 10:09:59,098 - WARNING - Invalid colon format: '[Al-Falaq:116]' -> 'Al-Falaq:116'
2025-07-07 10:09:59,098 - WARNING - Invalid colon format: '[Al-Falaq:116]' -> 'Al-Falaq:116'
2025-07-07 10:09:59,098 - INFO - Normalized predicted verses: ['11:11', '11:116', '2:255', '4:89', '5:39']
2025-07-07 10:09:59,098 - INFO - Normalized ground truth verses: ['46:16']
2025-07-07 10:09:59,098 - INFO -   Response: 761 chars, Found 5 verses
2025-07-07 10:09:59,098 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:09:59,098 - INFO -   Matched verses: []
2025-07-07 10:10:00,099 - INFO - Processing question 278/881 (ID: 314) for qwen
2025-07-07 10:10:00,099 - INFO -   Ground truth verses: ['46:16']
2025-07-07 10:10:00,099 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:10:15,851 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:10:15,851 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:10:15,851 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:10:15,851 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:10:15,851 - INFO - Normalized predicted verses: []
2025-07-07 10:10:15,851 - INFO - Normalized ground truth verses: ['46:16']
2025-07-07 10:10:15,851 - INFO -   Response: 4826 chars, Found 0 verses
2025-07-07 10:10:15,851 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:10:15,851 - INFO -   Matched verses: []
2025-07-07 10:10:16,853 - INFO - Processing question 279/881 (ID: 315) for qwen
2025-07-07 10:10:16,853 - INFO -   Ground truth verses: ['46:16']
2025-07-07 10:10:16,853 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:10:32,635 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 881), falling back to regex parsing
2025-07-07 10:10:32,635 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 881), falling back to regex parsing
2025-07-07 10:10:32,635 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 881), falling back to regex parsing
2025-07-07 10:10:32,635 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:10:32,635 - INFO - Normalized predicted verses: []
2025-07-07 10:10:32,635 - INFO - Normalized ground truth verses: ['46:16']
2025-07-07 10:10:32,635 - INFO -   Response: 4364 chars, Found 0 verses
2025-07-07 10:10:32,635 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:10:32,635 - INFO -   Matched verses: []
2025-07-07 10:10:33,636 - INFO - Processing question 280/881 (ID: 316) for qwen
2025-07-07 10:10:33,637 - INFO -   Ground truth verses: ['34:54']
2025-07-07 10:10:33,637 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:10:49,301 - INFO - Extracted JSON from code block: 752 characters
2025-07-07 10:10:49,301 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-An'am:164]' -> 'Al-An'am:164'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Hijr:9]' -> 'Al-Hijr:9'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:10:49,301 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-An'am:164]' -> 'Al-An'am:164'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-An'am:164]' -> 'Al-An'am:164'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Hijr:9]' -> 'Al-Hijr:9'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Hijr:9]' -> 'Al-Hijr:9'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:10:49,301 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:10:49,302 - INFO - Normalized predicted verses: ['15:9', '2:164', '2:255', '50:5', '5:5']
2025-07-07 10:10:49,302 - INFO - Normalized ground truth verses: ['34:54']
2025-07-07 10:10:49,302 - INFO -   Response: 752 chars, Found 5 verses
2025-07-07 10:10:49,302 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:10:49,302 - INFO -   Matched verses: []
2025-07-07 10:10:49,324 - INFO - Saved data to results/qwen_checkpoint_279.json
2025-07-07 10:10:49,324 - INFO - Checkpoint saved for qwen at question 280
2025-07-07 10:10:49,324 - INFO -   Progress: 280/881
2025-07-07 10:10:49,324 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 10:10:50,326 - INFO - Processing question 281/881 (ID: 318) for qwen
2025-07-07 10:10:50,326 - INFO -   Ground truth verses: ['34:54']
2025-07-07 10:10:50,326 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:11:06,098 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 874), falling back to regex parsing
2025-07-07 10:11:06,098 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 874), falling back to regex parsing
2025-07-07 10:11:06,098 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 874), falling back to regex parsing
2025-07-07 10:11:06,098 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:11:06,098 - INFO - Normalized predicted verses: []
2025-07-07 10:11:06,098 - INFO - Normalized ground truth verses: ['34:54']
2025-07-07 10:11:06,098 - INFO -   Response: 4027 chars, Found 0 verses
2025-07-07 10:11:06,098 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:11:06,098 - INFO -   Matched verses: []
2025-07-07 10:11:07,099 - INFO - Processing question 282/881 (ID: 319) for qwen
2025-07-07 10:11:07,099 - INFO -   Ground truth verses: ['34:54']
2025-07-07 10:11:07,100 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:11:12,967 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 10:11:12,973 - INFO - Loaded 6236 verses for validation
2025-07-07 10:11:12,973 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 10:11:12,973 - INFO - Models to test: fanar
2025-07-07 10:11:12,973 - INFO - Questions limit: All
2025-07-07 10:11:12,973 - INFO - Using device: cuda
2025-07-07 10:11:13,174 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:11:13,174 - INFO - 
============================================================
2025-07-07 10:11:13,174 - INFO - Starting evaluation for FANAR
2025-07-07 10:11:13,174 - INFO - ============================================================
2025-07-07 10:11:13,174 - INFO - Starting benchmark for FANAR
2025-07-07 10:11:13,174 - INFO - Using device: cuda
2025-07-07 10:11:13,174 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:11:13,175 - INFO - Processing 881 questions for fanar (starting from 1)
2025-07-07 10:11:13,175 - INFO - Processing question 1/881 (ID: 0) for fanar
2025-07-07 10:11:13,175 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 10:11:13,176 - INFO - Loading model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:11:13,176 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 10:11:23,588 - INFO - Extracted JSON from code block: 756 characters
2025-07-07 10:11:23,588 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:11:23,588 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:11:23,588 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 10:11:23,588 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 10:11:23,588 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 10:11:23,588 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 10:11:23,588 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:11:23,588 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:11:23,588 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:11:23,588 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 10:11:23,589 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 10:11:23,589 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 10:11:23,589 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 10:11:23,589 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 10:11:23,589 - WARNING - Invalid colon format: '[Al-Qamar:12]' -> 'Al-Qamar:12'
2025-07-07 10:11:23,589 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 10:11:23,589 - WARNING - Invalid colon format: '[Al-Furqan:56]' -> 'Al-Furqan:56'
2025-07-07 10:11:23,589 - INFO - Normalized predicted verses: ['15:15', '25:56', '50:12', '5:104', '6:84']
2025-07-07 10:11:23,589 - INFO - Normalized ground truth verses: ['34:54']
2025-07-07 10:11:23,589 - INFO -   Response: 756 chars, Found 5 verses
2025-07-07 10:11:23,589 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:11:23,589 - INFO -   Matched verses: []
2025-07-07 10:11:24,590 - INFO - Processing question 283/881 (ID: 320) for qwen
2025-07-07 10:11:24,590 - INFO -   Ground truth verses: ['34:54']
2025-07-07 10:11:24,590 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:11:41,183 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:11:41,183 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:11:41,184 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:11:41,184 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:11:41,184 - INFO - Normalized predicted verses: []
2025-07-07 10:11:41,184 - INFO - Normalized ground truth verses: ['34:54']
2025-07-07 10:11:41,184 - INFO -   Response: 3890 chars, Found 0 verses
2025-07-07 10:11:41,184 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:11:41,184 - INFO -   Matched verses: []
2025-07-07 10:11:42,185 - INFO - Processing question 284/881 (ID: 321) for qwen
2025-07-07 10:11:42,185 - INFO -   Ground truth verses: ['20:3', '80:9', '20:44', '79:26', '87:10']
2025-07-07 10:11:42,185 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:12:00,478 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:12:00,478 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:12:00,478 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:12:00,478 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:12:00,478 - INFO - Normalized predicted verses: []
2025-07-07 10:12:00,478 - INFO - Normalized ground truth verses: ['20:3', '20:44', '79:26', '80:9', '87:10']
2025-07-07 10:12:00,478 - INFO -   Response: 4267 chars, Found 0 verses
2025-07-07 10:12:00,478 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:00,478 - INFO -   Matched verses: []
2025-07-07 10:12:01,479 - INFO - Processing question 285/881 (ID: 322) for qwen
2025-07-07 10:12:01,480 - INFO -   Ground truth verses: ['20:3', '80:9', '20:44', '79:26', '87:10']
2025-07-07 10:12:01,480 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:12:02,075 - INFO - Successfully loaded QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:04,187 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:04,188 - WARNING -   Failed to get response for question 0
2025-07-07 10:12:05,189 - INFO - Processing question 2/881 (ID: 1) for fanar
2025-07-07 10:12:05,189 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 10:12:05,189 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:05,609 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:05,610 - WARNING -   Failed to get response for question 1
2025-07-07 10:12:06,611 - INFO - Processing question 3/881 (ID: 2) for fanar
2025-07-07 10:12:06,611 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 10:12:06,611 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:06,778 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:06,778 - WARNING -   Failed to get response for question 2
2025-07-07 10:12:06,873 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 10:12:06,877 - INFO - Loaded 6236 verses for validation
2025-07-07 10:12:06,877 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 10:12:06,877 - INFO - Models to test: deepseek
2025-07-07 10:12:06,877 - INFO - Questions limit: All
2025-07-07 10:12:06,877 - INFO - Using device: cuda
2025-07-07 10:12:07,099 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:12:07,099 - INFO - 
============================================================
2025-07-07 10:12:07,099 - INFO - Starting evaluation for DEEPSEEK
2025-07-07 10:12:07,099 - INFO - ============================================================
2025-07-07 10:12:07,099 - INFO - Starting benchmark for DEEPSEEK
2025-07-07 10:12:07,099 - INFO - Using device: cuda
2025-07-07 10:12:07,100 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:12:07,100 - INFO - Processing 881 questions for deepseek (starting from 1)
2025-07-07 10:12:07,100 - INFO - Processing question 1/881 (ID: 0) for deepseek
2025-07-07 10:12:07,100 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 10:12:07,100 - INFO - Loading model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:12:07,100 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 10:12:07,780 - INFO - Processing question 4/881 (ID: 3) for fanar
2025-07-07 10:12:07,780 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 10:12:07,780 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:07,950 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:07,950 - WARNING -   Failed to get response for question 3
2025-07-07 10:12:08,951 - INFO - Processing question 5/881 (ID: 4) for fanar
2025-07-07 10:12:08,952 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 10:12:08,952 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:09,121 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:09,121 - WARNING -   Failed to get response for question 4
2025-07-07 10:12:09,125 - INFO - Saved data to results/fanar_checkpoint_4.json
2025-07-07 10:12:09,125 - INFO - Checkpoint saved for fanar at question 5
2025-07-07 10:12:09,125 - INFO -   Progress: 5/881
2025-07-07 10:12:09,125 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:10,126 - INFO - Processing question 6/881 (ID: 5) for fanar
2025-07-07 10:12:10,126 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:12:10,126 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:10,290 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:10,291 - WARNING -   Failed to get response for question 5
2025-07-07 10:12:11,292 - INFO - Processing question 7/881 (ID: 6) for fanar
2025-07-07 10:12:11,292 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:12:11,292 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:11,459 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:11,459 - WARNING -   Failed to get response for question 6
2025-07-07 10:12:12,460 - INFO - Processing question 8/881 (ID: 7) for fanar
2025-07-07 10:12:12,461 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:12:12,461 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:12,797 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:12,797 - WARNING -   Failed to get response for question 7
2025-07-07 10:12:13,799 - INFO - Processing question 9/881 (ID: 8) for fanar
2025-07-07 10:12:13,799 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:12:13,799 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:13,969 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:13,970 - WARNING -   Failed to get response for question 8
2025-07-07 10:12:14,971 - INFO - Processing question 10/881 (ID: 9) for fanar
2025-07-07 10:12:14,971 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:12:14,971 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:15,143 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:15,144 - WARNING -   Failed to get response for question 9
2025-07-07 10:12:15,148 - INFO - Saved data to results/fanar_checkpoint_9.json
2025-07-07 10:12:15,148 - INFO - Checkpoint saved for fanar at question 10
2025-07-07 10:12:15,148 - INFO -   Progress: 10/881
2025-07-07 10:12:15,148 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:16,149 - INFO - Processing question 11/881 (ID: 10) for fanar
2025-07-07 10:12:16,149 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 10:12:16,149 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:16,323 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:16,324 - WARNING -   Failed to get response for question 10
2025-07-07 10:12:17,325 - INFO - Processing question 12/881 (ID: 11) for fanar
2025-07-07 10:12:17,325 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 10:12:17,326 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:17,494 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:17,494 - WARNING -   Failed to get response for question 11
2025-07-07 10:12:17,786 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:12:17,786 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:12:17,786 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:12:17,786 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:12:17,786 - INFO - Normalized predicted verses: []
2025-07-07 10:12:17,786 - INFO - Normalized ground truth verses: ['20:3', '20:44', '79:26', '80:9', '87:10']
2025-07-07 10:12:17,786 - INFO -   Response: 4077 chars, Found 0 verses
2025-07-07 10:12:17,786 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:17,786 - INFO -   Matched verses: []
2025-07-07 10:12:17,809 - INFO - Saved data to results/qwen_checkpoint_284.json
2025-07-07 10:12:17,809 - INFO - Checkpoint saved for qwen at question 285
2025-07-07 10:12:17,809 - INFO -   Progress: 285/881
2025-07-07 10:12:17,809 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.004
2025-07-07 10:12:18,496 - INFO - Processing question 13/881 (ID: 12) for fanar
2025-07-07 10:12:18,496 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 10:12:18,496 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:18,648 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:18,648 - WARNING -   Failed to get response for question 12
2025-07-07 10:12:18,810 - INFO - Processing question 286/881 (ID: 323) for qwen
2025-07-07 10:12:18,810 - INFO -   Ground truth verses: ['20:3', '80:9', '20:44', '79:26', '87:10']
2025-07-07 10:12:18,810 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:12:19,650 - INFO - Processing question 14/881 (ID: 13) for fanar
2025-07-07 10:12:19,650 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 10:12:19,650 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:20,005 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:20,006 - WARNING -   Failed to get response for question 13
2025-07-07 10:12:21,007 - INFO - Processing question 15/881 (ID: 14) for fanar
2025-07-07 10:12:21,007 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 10:12:21,007 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:21,175 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:21,176 - WARNING -   Failed to get response for question 14
2025-07-07 10:12:21,179 - INFO - Saved data to results/fanar_checkpoint_14.json
2025-07-07 10:12:21,180 - INFO - Checkpoint saved for fanar at question 15
2025-07-07 10:12:21,180 - INFO -   Progress: 15/881
2025-07-07 10:12:21,180 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:22,181 - INFO - Processing question 16/881 (ID: 15) for fanar
2025-07-07 10:12:22,181 - INFO -   Ground truth verses: ['52:44']
2025-07-07 10:12:22,181 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:22,350 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:22,350 - WARNING -   Failed to get response for question 15
2025-07-07 10:12:23,351 - INFO - Processing question 17/881 (ID: 16) for fanar
2025-07-07 10:12:23,352 - INFO -   Ground truth verses: ['52:44']
2025-07-07 10:12:23,352 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:23,520 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:23,520 - WARNING -   Failed to get response for question 16
2025-07-07 10:12:24,521 - INFO - Processing question 18/881 (ID: 17) for fanar
2025-07-07 10:12:24,522 - INFO -   Ground truth verses: ['52:44']
2025-07-07 10:12:24,522 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:24,692 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:24,693 - WARNING -   Failed to get response for question 17
2025-07-07 10:12:25,694 - INFO - Processing question 19/881 (ID: 18) for fanar
2025-07-07 10:12:25,694 - INFO -   Ground truth verses: ['52:44']
2025-07-07 10:12:25,694 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:25,867 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:25,867 - WARNING -   Failed to get response for question 18
2025-07-07 10:12:26,868 - INFO - Processing question 20/881 (ID: 19) for fanar
2025-07-07 10:12:26,869 - INFO -   Ground truth verses: ['52:44']
2025-07-07 10:12:26,869 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:27,036 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:27,036 - WARNING -   Failed to get response for question 19
2025-07-07 10:12:27,101 - INFO - Saved data to results/fanar_checkpoint_19.json
2025-07-07 10:12:27,101 - INFO - Checkpoint saved for fanar at question 20
2025-07-07 10:12:27,101 - INFO -   Progress: 20/881
2025-07-07 10:12:27,101 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:28,102 - INFO - Processing question 21/881 (ID: 20) for fanar
2025-07-07 10:12:28,102 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 10:12:28,102 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:28,426 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:28,427 - WARNING -   Failed to get response for question 20
2025-07-07 10:12:29,428 - INFO - Processing question 22/881 (ID: 21) for fanar
2025-07-07 10:12:29,428 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 10:12:29,428 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:29,596 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:29,596 - WARNING -   Failed to get response for question 21
2025-07-07 10:12:30,598 - INFO - Processing question 23/881 (ID: 22) for fanar
2025-07-07 10:12:30,598 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 10:12:30,598 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:30,765 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:30,766 - WARNING -   Failed to get response for question 22
2025-07-07 10:12:31,767 - INFO - Processing question 24/881 (ID: 23) for fanar
2025-07-07 10:12:31,767 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 10:12:31,767 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:31,933 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:31,934 - WARNING -   Failed to get response for question 23
2025-07-07 10:12:32,935 - INFO - Processing question 25/881 (ID: 24) for fanar
2025-07-07 10:12:32,935 - INFO -   Ground truth verses: ['101:10', '74:27', '69:3', '90:12', '101:3', '83:8', '86:2', '104:5', '82:17', '77:14', '97:2', '82:18', '83:19']
2025-07-07 10:12:32,935 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:33,103 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:33,103 - WARNING -   Failed to get response for question 24
2025-07-07 10:12:33,108 - INFO - Saved data to results/fanar_checkpoint_24.json
2025-07-07 10:12:33,108 - INFO - Checkpoint saved for fanar at question 25
2025-07-07 10:12:33,108 - INFO -   Progress: 25/881
2025-07-07 10:12:33,108 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:34,109 - INFO - Processing question 26/881 (ID: 30) for fanar
2025-07-07 10:12:34,110 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 10:12:34,110 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:34,281 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:34,281 - WARNING -   Failed to get response for question 30
2025-07-07 10:12:34,909 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 10:12:34,909 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:12:34,909 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:12:34,909 - INFO - Normalized predicted verses: []
2025-07-07 10:12:34,909 - INFO - Normalized ground truth verses: ['20:3', '20:44', '79:26', '80:9', '87:10']
2025-07-07 10:12:34,909 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 10:12:34,909 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:34,909 - INFO -   Matched verses: []
2025-07-07 10:12:35,283 - INFO - Processing question 27/881 (ID: 31) for fanar
2025-07-07 10:12:35,283 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 10:12:35,283 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:35,426 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:35,427 - WARNING -   Failed to get response for question 31
2025-07-07 10:12:35,910 - INFO - Processing question 287/881 (ID: 324) for qwen
2025-07-07 10:12:35,911 - INFO -   Ground truth verses: ['20:3', '80:9', '20:44', '79:26', '87:10']
2025-07-07 10:12:35,911 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:12:36,428 - INFO - Processing question 28/881 (ID: 32) for fanar
2025-07-07 10:12:36,429 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 10:12:36,429 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:36,744 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:36,745 - WARNING -   Failed to get response for question 32
2025-07-07 10:12:37,746 - INFO - Processing question 29/881 (ID: 33) for fanar
2025-07-07 10:12:37,746 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 10:12:37,746 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:37,910 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:37,911 - WARNING -   Failed to get response for question 33
2025-07-07 10:12:38,912 - INFO - Processing question 30/881 (ID: 34) for fanar
2025-07-07 10:12:38,912 - INFO -   Ground truth verses: ['44:52', '26:147', '26:57', '51:15', '15:45', '44:25', '26:134']
2025-07-07 10:12:38,912 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:39,079 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:39,079 - WARNING -   Failed to get response for question 34
2025-07-07 10:12:39,084 - INFO - Saved data to results/fanar_checkpoint_29.json
2025-07-07 10:12:39,084 - INFO - Checkpoint saved for fanar at question 30
2025-07-07 10:12:39,084 - INFO -   Progress: 30/881
2025-07-07 10:12:39,084 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:40,085 - INFO - Processing question 31/881 (ID: 35) for fanar
2025-07-07 10:12:40,085 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 10:12:40,085 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:40,251 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:40,252 - WARNING -   Failed to get response for question 35
2025-07-07 10:12:41,253 - INFO - Processing question 32/881 (ID: 36) for fanar
2025-07-07 10:12:41,253 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 10:12:41,253 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:41,420 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:41,421 - WARNING -   Failed to get response for question 36
2025-07-07 10:12:42,422 - INFO - Processing question 33/881 (ID: 37) for fanar
2025-07-07 10:12:42,422 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 10:12:42,422 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:42,589 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:42,589 - WARNING -   Failed to get response for question 37
2025-07-07 10:12:43,591 - INFO - Processing question 34/881 (ID: 38) for fanar
2025-07-07 10:12:43,591 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 10:12:43,591 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:43,907 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:43,908 - WARNING -   Failed to get response for question 38
2025-07-07 10:12:44,909 - INFO - Processing question 35/881 (ID: 39) for fanar
2025-07-07 10:12:44,909 - INFO -   Ground truth verses: ['53:23', '12:40', '7:71']
2025-07-07 10:12:44,909 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:45,078 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:45,078 - WARNING -   Failed to get response for question 39
2025-07-07 10:12:45,084 - INFO - Saved data to results/fanar_checkpoint_34.json
2025-07-07 10:12:45,084 - INFO - Checkpoint saved for fanar at question 35
2025-07-07 10:12:45,084 - INFO -   Progress: 35/881
2025-07-07 10:12:45,084 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:46,085 - INFO - Processing question 36/881 (ID: 40) for fanar
2025-07-07 10:12:46,085 - INFO -   Ground truth verses: ['73:11']
2025-07-07 10:12:46,085 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:46,252 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:46,252 - WARNING -   Failed to get response for question 40
2025-07-07 10:12:47,254 - INFO - Processing question 37/881 (ID: 41) for fanar
2025-07-07 10:12:47,254 - INFO -   Ground truth verses: ['73:11']
2025-07-07 10:12:47,254 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:47,419 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:47,419 - WARNING -   Failed to get response for question 41
2025-07-07 10:12:48,421 - INFO - Processing question 38/881 (ID: 42) for fanar
2025-07-07 10:12:48,421 - INFO -   Ground truth verses: ['73:11']
2025-07-07 10:12:48,421 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:48,586 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:48,587 - WARNING -   Failed to get response for question 42
2025-07-07 10:12:49,588 - INFO - Processing question 39/881 (ID: 43) for fanar
2025-07-07 10:12:49,589 - INFO -   Ground truth verses: ['73:11']
2025-07-07 10:12:49,589 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:49,755 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:49,755 - WARNING -   Failed to get response for question 43
2025-07-07 10:12:50,757 - INFO - Processing question 40/881 (ID: 44) for fanar
2025-07-07 10:12:50,757 - INFO -   Ground truth verses: ['73:11']
2025-07-07 10:12:50,757 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:50,921 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:50,922 - WARNING -   Failed to get response for question 44
2025-07-07 10:12:50,927 - INFO - Saved data to results/fanar_checkpoint_39.json
2025-07-07 10:12:50,927 - INFO - Checkpoint saved for fanar at question 40
2025-07-07 10:12:50,927 - INFO -   Progress: 40/881
2025-07-07 10:12:50,927 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:51,928 - INFO - Processing question 41/881 (ID: 45) for fanar
2025-07-07 10:12:51,929 - INFO -   Ground truth verses: ['52:4']
2025-07-07 10:12:51,929 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:52,082 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:12:52,082 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:12:52,082 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:12:52,082 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:12:52,082 - INFO - Normalized predicted verses: []
2025-07-07 10:12:52,082 - INFO - Normalized ground truth verses: ['20:3', '20:44', '79:26', '80:9', '87:10']
2025-07-07 10:12:52,082 - INFO -   Response: 3922 chars, Found 0 verses
2025-07-07 10:12:52,082 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:52,082 - INFO -   Matched verses: []
2025-07-07 10:12:52,250 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:52,251 - WARNING -   Failed to get response for question 45
2025-07-07 10:12:53,083 - INFO - Processing question 288/881 (ID: 325) for qwen
2025-07-07 10:12:53,084 - INFO -   Ground truth verses: ['20:3', '80:9', '20:44', '79:26', '87:10']
2025-07-07 10:12:53,084 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:12:53,252 - INFO - Processing question 42/881 (ID: 46) for fanar
2025-07-07 10:12:53,252 - INFO -   Ground truth verses: ['52:4']
2025-07-07 10:12:53,252 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:53,420 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:53,421 - WARNING -   Failed to get response for question 46
2025-07-07 10:12:54,422 - INFO - Processing question 43/881 (ID: 47) for fanar
2025-07-07 10:12:54,422 - INFO -   Ground truth verses: ['52:4']
2025-07-07 10:12:54,423 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:54,588 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:54,589 - WARNING -   Failed to get response for question 47
2025-07-07 10:12:55,590 - INFO - Processing question 44/881 (ID: 48) for fanar
2025-07-07 10:12:55,590 - INFO -   Ground truth verses: ['52:4']
2025-07-07 10:12:55,590 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:55,755 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:55,755 - WARNING -   Failed to get response for question 48
2025-07-07 10:12:56,756 - INFO - Processing question 45/881 (ID: 49) for fanar
2025-07-07 10:12:56,757 - INFO -   Ground truth verses: ['52:4']
2025-07-07 10:12:56,757 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:56,925 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:56,926 - WARNING -   Failed to get response for question 49
2025-07-07 10:12:56,930 - INFO - Saved data to results/fanar_checkpoint_44.json
2025-07-07 10:12:56,930 - INFO - Checkpoint saved for fanar at question 45
2025-07-07 10:12:56,931 - INFO -   Progress: 45/881
2025-07-07 10:12:56,931 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:12:57,932 - INFO - Processing question 46/881 (ID: 50) for fanar
2025-07-07 10:12:57,932 - INFO -   Ground truth verses: ['36:29', '36:32', '36:49', '38:15', '36:53']
2025-07-07 10:12:57,932 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:58,101 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:58,102 - WARNING -   Failed to get response for question 50
2025-07-07 10:12:59,103 - INFO - Processing question 47/881 (ID: 51) for fanar
2025-07-07 10:12:59,103 - INFO -   Ground truth verses: ['36:29', '36:32', '36:49', '38:15', '36:53']
2025-07-07 10:12:59,103 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:12:59,274 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:12:59,275 - WARNING -   Failed to get response for question 51
2025-07-07 10:13:00,276 - INFO - Processing question 48/881 (ID: 52) for fanar
2025-07-07 10:13:00,276 - INFO -   Ground truth verses: ['36:29', '36:32', '36:49', '38:15', '36:53']
2025-07-07 10:13:00,276 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:13:00,622 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:13:00,623 - WARNING -   Failed to get response for question 52
2025-07-07 10:13:01,625 - INFO - Processing question 49/881 (ID: 53) for fanar
2025-07-07 10:13:01,625 - INFO -   Ground truth verses: ['36:29', '36:32', '36:49', '38:15', '36:53']
2025-07-07 10:13:01,625 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:13:01,794 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:13:01,795 - WARNING -   Failed to get response for question 53
2025-07-07 10:13:02,796 - INFO - Processing question 50/881 (ID: 54) for fanar
2025-07-07 10:13:02,796 - INFO -   Ground truth verses: ['36:29', '36:32', '36:49', '38:15', '36:53']
2025-07-07 10:13:02,796 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:13:02,964 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:13:02,964 - WARNING -   Failed to get response for question 54
2025-07-07 10:13:02,969 - INFO - Saved data to results/fanar_checkpoint_49.json
2025-07-07 10:13:02,970 - INFO - Checkpoint saved for fanar at question 50
2025-07-07 10:13:02,970 - INFO -   Progress: 50/881
2025-07-07 10:13:02,970 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:13:03,971 - INFO - Processing question 51/881 (ID: 55) for fanar
2025-07-07 10:13:03,971 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 10:13:03,971 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:13:04,141 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:13:04,142 - WARNING -   Failed to get response for question 55
2025-07-07 10:13:05,143 - INFO - Processing question 52/881 (ID: 56) for fanar
2025-07-07 10:13:05,143 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 10:13:05,143 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:13:05,314 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:13:05,315 - WARNING -   Failed to get response for question 56
2025-07-07 10:13:06,316 - INFO - Processing question 53/881 (ID: 57) for fanar
2025-07-07 10:13:06,316 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 10:13:06,316 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:13:06,484 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:13:06,485 - WARNING -   Failed to get response for question 57
2025-07-07 10:13:07,486 - INFO - Processing question 54/881 (ID: 58) for fanar
2025-07-07 10:13:07,486 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 10:13:07,486 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:13:07,828 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:13:07,828 - WARNING -   Failed to get response for question 58
2025-07-07 10:13:09,127 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:13:09,127 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:13:09,127 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:13:09,127 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:13:09,127 - INFO - Normalized predicted verses: []
2025-07-07 10:13:09,127 - INFO - Normalized ground truth verses: ['20:3', '20:44', '79:26', '80:9', '87:10']
2025-07-07 10:13:09,127 - INFO -   Response: 4144 chars, Found 0 verses
2025-07-07 10:13:09,128 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:13:09,128 - INFO -   Matched verses: []
2025-07-07 10:13:10,129 - INFO - Processing question 289/881 (ID: 326) for qwen
2025-07-07 10:13:10,129 - INFO -   Ground truth verses: ['34:5', '34:38', '5:86', '5:10', '22:51']
2025-07-07 10:13:10,129 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:13:25,975 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 10:13:25,975 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 10:13:25,975 - WARNING - Failed to parse JSON response: Expecting value: line 1 column 13 (char 12), falling back to regex parsing
2025-07-07 10:13:25,976 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:13:25,976 - INFO - Normalized predicted verses: []
2025-07-07 10:13:25,976 - INFO - Normalized ground truth verses: ['22:51', '34:38', '34:5', '5:10', '5:86']
2025-07-07 10:13:25,976 - INFO -   Response: 4360 chars, Found 0 verses
2025-07-07 10:13:25,976 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:13:25,976 - INFO -   Matched verses: []
2025-07-07 10:13:26,977 - INFO - Processing question 290/881 (ID: 327) for qwen
2025-07-07 10:13:26,977 - INFO -   Ground truth verses: ['34:5', '34:38', '5:86', '5:10', '22:51']
2025-07-07 10:13:26,977 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:13:42,524 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 10:13:42,524 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 10:13:42,524 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:13:42,524 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:13:42,524 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 10:13:42,524 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:13:42,524 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:13:42,524 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:13:42,524 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:13:42,524 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 10:13:42,524 - INFO - Normalized ground truth verses: ['22:51', '34:38', '34:5', '5:10', '5:86']
2025-07-07 10:13:42,524 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 10:13:42,524 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:13:42,524 - INFO -   Matched verses: []
2025-07-07 10:13:42,720 - INFO - Saved data to results/qwen_checkpoint_289.json
2025-07-07 10:13:42,720 - INFO - Checkpoint saved for qwen at question 290
2025-07-07 10:13:42,720 - INFO -   Progress: 290/881
2025-07-07 10:13:42,720 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.003
2025-07-07 10:13:43,721 - INFO - Processing question 291/881 (ID: 328) for qwen
2025-07-07 10:13:43,721 - INFO -   Ground truth verses: ['34:5', '34:38', '5:86', '5:10', '22:51']
2025-07-07 10:13:43,721 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:13:48,403 - INFO - Successfully loaded suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:14:00,730 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:14:00,730 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:14:00,730 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:14:00,730 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:14:00,730 - INFO - Normalized predicted verses: []
2025-07-07 10:14:00,730 - INFO - Normalized ground truth verses: ['22:51', '34:38', '34:5', '5:10', '5:86']
2025-07-07 10:14:00,730 - INFO -   Response: 4169 chars, Found 0 verses
2025-07-07 10:14:00,730 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:14:00,730 - INFO -   Matched verses: []
2025-07-07 10:14:01,731 - INFO - Processing question 292/881 (ID: 329) for qwen
2025-07-07 10:14:01,732 - INFO -   Ground truth verses: ['34:5', '34:38', '5:86', '5:10', '22:51']
2025-07-07 10:14:01,732 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:14:04,585 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:14:04,585 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:14:04,585 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:14:04,585 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:14:04,585 - INFO - Normalized predicted verses: []
2025-07-07 10:14:04,585 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 10:14:04,585 - INFO -   Response: 3579 chars, Found 0 verses
2025-07-07 10:14:04,585 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:14:04,585 - INFO -   Matched verses: []
2025-07-07 10:14:05,586 - INFO - Processing question 2/881 (ID: 1) for deepseek
2025-07-07 10:14:05,586 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 10:14:05,586 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:14:19,369 - INFO - Extracted JSON from code block: 749 characters
2025-07-07 10:14:19,369 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 10:14:19,369 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 10:14:19,369 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 10:14:19,369 - INFO - Normalized predicted verses: ['6:84', '6:85', '6:86', '6:87', '6:88']
2025-07-07 10:14:19,369 - INFO - Normalized ground truth verses: ['22:51', '34:38', '34:5', '5:10', '5:86']
2025-07-07 10:14:19,370 - INFO -   Response: 749 chars, Found 5 verses
2025-07-07 10:14:19,370 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:14:19,370 - INFO -   Matched verses: []
2025-07-07 10:14:20,371 - INFO - Processing question 293/881 (ID: 330) for qwen
2025-07-07 10:14:20,371 - INFO -   Ground truth verses: ['34:5', '34:38', '5:86', '5:10', '22:51']
2025-07-07 10:14:20,371 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:14:23,105 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 865), falling back to regex parsing
2025-07-07 10:14:23,105 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 865), falling back to regex parsing
2025-07-07 10:14:23,105 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 865), falling back to regex parsing
2025-07-07 10:14:23,105 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:14:23,105 - INFO - Normalized predicted verses: []
2025-07-07 10:14:23,105 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 10:14:23,105 - INFO -   Response: 4219 chars, Found 0 verses
2025-07-07 10:14:23,105 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:14:23,105 - INFO -   Matched verses: []
2025-07-07 10:14:24,106 - INFO - Processing question 3/881 (ID: 2) for deepseek
2025-07-07 10:14:24,107 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 10:14:24,107 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:14:37,937 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:14:37,937 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:14:37,937 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:14:37,937 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:14:37,937 - INFO - Normalized predicted verses: []
2025-07-07 10:14:37,937 - INFO - Normalized ground truth verses: ['22:51', '34:38', '34:5', '5:10', '5:86']
2025-07-07 10:14:37,937 - INFO -   Response: 4325 chars, Found 0 verses
2025-07-07 10:14:37,937 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:14:37,937 - INFO -   Matched verses: []
2025-07-07 10:14:38,939 - INFO - Processing question 294/881 (ID: 331) for qwen
2025-07-07 10:14:38,939 - INFO -   Ground truth verses: ['41:42']
2025-07-07 10:14:38,939 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:14:41,549 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:14:41,549 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:14:41,549 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:14:41,549 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:14:41,549 - INFO - Normalized predicted verses: []
2025-07-07 10:14:41,549 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 10:14:41,549 - INFO -   Response: 4479 chars, Found 0 verses
2025-07-07 10:14:41,549 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:14:41,549 - INFO -   Matched verses: []
2025-07-07 10:14:42,551 - INFO - Processing question 4/881 (ID: 3) for deepseek
2025-07-07 10:14:42,551 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 10:14:42,551 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:14:56,523 - INFO - Extracted JSON from code block: 751 characters
2025-07-07 10:14:56,523 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Ma'idah:63]' -> 'Al-Ma'idah:63'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Qamar:1]' -> 'Al-Qamar:1'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Zumar:12]' -> 'Al-Zumar:12'
2025-07-07 10:14:56,524 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-An'am:82]' -> 'Al-An'am:82'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Ma'idah:63]' -> 'Al-Ma'idah:63'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Ma'idah:63]' -> 'Al-Ma'idah:63'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Qamar:1]' -> 'Al-Qamar:1'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Qamar:1]' -> 'Al-Qamar:1'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Zumar:12]' -> 'Al-Zumar:12'
2025-07-07 10:14:56,524 - WARNING - Invalid colon format: '[Al-Zumar:12]' -> 'Al-Zumar:12'
2025-07-07 10:14:56,524 - INFO - Normalized predicted verses: ['15:10', '39:12', '4:82', '50:1', '5:63']
2025-07-07 10:14:56,524 - INFO - Normalized ground truth verses: ['41:42']
2025-07-07 10:14:56,524 - INFO -   Response: 751 chars, Found 5 verses
2025-07-07 10:14:56,524 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:14:56,524 - INFO -   Matched verses: []
2025-07-07 10:14:57,525 - INFO - Processing question 295/881 (ID: 332) for qwen
2025-07-07 10:14:57,525 - INFO -   Ground truth verses: ['41:42']
2025-07-07 10:14:57,525 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:15:00,050 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 633), falling back to regex parsing
2025-07-07 10:15:00,050 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 633), falling back to regex parsing
2025-07-07 10:15:00,050 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 633), falling back to regex parsing
2025-07-07 10:15:00,050 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:15:00,050 - INFO - Normalized predicted verses: []
2025-07-07 10:15:00,050 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 10:15:00,050 - INFO -   Response: 2934 chars, Found 0 verses
2025-07-07 10:15:00,050 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:00,050 - INFO -   Matched verses: []
2025-07-07 10:15:01,052 - INFO - Processing question 5/881 (ID: 4) for deepseek
2025-07-07 10:15:01,052 - INFO -   Ground truth verses: ['3:23', '4:51', '4:44']
2025-07-07 10:15:01,052 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:15:02,339 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 10:15:02,343 - INFO - Loaded 6236 verses for validation
2025-07-07 10:15:02,343 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 10:15:02,344 - INFO - Models to test: fanar
2025-07-07 10:15:02,344 - INFO - Questions limit: All
2025-07-07 10:15:02,344 - INFO - Using device: cuda
2025-07-07 10:15:02,570 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:15:02,570 - INFO - 
============================================================
2025-07-07 10:15:02,570 - INFO - Starting evaluation for FANAR
2025-07-07 10:15:02,570 - INFO - ============================================================
2025-07-07 10:15:02,570 - INFO - Starting benchmark for FANAR
2025-07-07 10:15:02,570 - INFO - Using device: cuda
2025-07-07 10:15:02,571 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:15:02,572 - INFO - Loaded checkpoint for fanar: 50 questions completed
2025-07-07 10:15:02,572 - INFO - Processing 831 questions for fanar (starting from 51)
2025-07-07 10:15:02,572 - INFO - Processing question 51/881 (ID: 55) for fanar
2025-07-07 10:15:02,573 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 10:15:02,573 - INFO - Loading model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:02,573 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 10:15:12,337 - INFO - Successfully loaded QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:14,214 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:14,215 - WARNING -   Failed to get response for question 55
2025-07-07 10:15:14,215 - INFO - Processing question 52/881 (ID: 56) for fanar
2025-07-07 10:15:14,215 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 10:15:14,215 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:14,552 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:14,553 - WARNING -   Failed to get response for question 56
2025-07-07 10:15:14,553 - INFO - Processing question 53/881 (ID: 57) for fanar
2025-07-07 10:15:14,553 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 10:15:14,553 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:14,816 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:14,816 - WARNING -   Failed to get response for question 57
2025-07-07 10:15:14,816 - INFO - Processing question 54/881 (ID: 58) for fanar
2025-07-07 10:15:14,816 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 10:15:14,816 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:15,012 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:15,013 - WARNING -   Failed to get response for question 58
2025-07-07 10:15:15,013 - INFO - Processing question 55/881 (ID: 59) for fanar
2025-07-07 10:15:15,013 - INFO -   Ground truth verses: ['107:5', '51:11']
2025-07-07 10:15:15,013 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:15,220 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:15,221 - WARNING -   Failed to get response for question 59
2025-07-07 10:15:15,226 - INFO - Saved data to results/fanar_checkpoint_54.json
2025-07-07 10:15:15,226 - INFO - Checkpoint saved for fanar at question 55
2025-07-07 10:15:15,226 - INFO -   Progress: 55/881
2025-07-07 10:15:15,226 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:15,226 - INFO - Processing question 56/881 (ID: 60) for fanar
2025-07-07 10:15:15,227 - INFO -   Ground truth verses: ['28:65', '5:109']
2025-07-07 10:15:15,227 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:15,441 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:15,442 - WARNING -   Failed to get response for question 60
2025-07-07 10:15:15,442 - INFO - Processing question 57/881 (ID: 61) for fanar
2025-07-07 10:15:15,442 - INFO -   Ground truth verses: ['28:65', '5:109']
2025-07-07 10:15:15,442 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:15,645 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:15,645 - WARNING -   Failed to get response for question 61
2025-07-07 10:15:15,645 - INFO - Processing question 58/881 (ID: 62) for fanar
2025-07-07 10:15:15,645 - INFO -   Ground truth verses: ['28:65', '5:109']
2025-07-07 10:15:15,645 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:15,913 - INFO - Extracted JSON from code block: 748 characters
2025-07-07 10:15:15,913 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-Hijr:56]' -> 'Al-Hijr:56'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Qaf:1]' -> 'Qaf:1'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 10:15:15,913 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-Hijr:56]' -> 'Al-Hijr:56'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-Hijr:56]' -> 'Al-Hijr:56'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Qaf:1]' -> 'Qaf:1'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Qaf:1]' -> 'Qaf:1'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 10:15:15,913 - WARNING - Invalid colon format: '[Al-Ma'idah:5]' -> 'Al-Ma'idah:5'
2025-07-07 10:15:15,913 - INFO - Normalized predicted verses: ['15:56', '2:255', '50:1', '5:5', '6:84']
2025-07-07 10:15:15,913 - INFO - Normalized ground truth verses: ['41:42']
2025-07-07 10:15:15,913 - INFO -   Response: 748 chars, Found 5 verses
2025-07-07 10:15:15,913 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:15,913 - INFO -   Matched verses: []
2025-07-07 10:15:15,937 - INFO - Saved data to results/qwen_checkpoint_294.json
2025-07-07 10:15:15,937 - INFO - Checkpoint saved for qwen at question 295
2025-07-07 10:15:15,937 - INFO -   Progress: 295/881
2025-07-07 10:15:15,937 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.003
2025-07-07 10:15:15,990 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:15,991 - WARNING -   Failed to get response for question 62
2025-07-07 10:15:15,991 - INFO - Processing question 59/881 (ID: 63) for fanar
2025-07-07 10:15:15,991 - INFO -   Ground truth verses: ['28:65', '5:109']
2025-07-07 10:15:15,991 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:16,164 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:16,165 - WARNING -   Failed to get response for question 63
2025-07-07 10:15:16,165 - INFO - Processing question 60/881 (ID: 64) for fanar
2025-07-07 10:15:16,165 - INFO -   Ground truth verses: ['28:65', '5:109']
2025-07-07 10:15:16,165 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:16,335 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:16,335 - WARNING -   Failed to get response for question 64
2025-07-07 10:15:16,340 - INFO - Saved data to results/fanar_checkpoint_59.json
2025-07-07 10:15:16,340 - INFO - Checkpoint saved for fanar at question 60
2025-07-07 10:15:16,340 - INFO -   Progress: 60/881
2025-07-07 10:15:16,340 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:16,340 - INFO - Processing question 61/881 (ID: 65) for fanar
2025-07-07 10:15:16,340 - INFO -   Ground truth verses: ['9:6']
2025-07-07 10:15:16,340 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:16,515 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:16,515 - WARNING -   Failed to get response for question 65
2025-07-07 10:15:16,515 - INFO - Processing question 62/881 (ID: 66) for fanar
2025-07-07 10:15:16,515 - INFO -   Ground truth verses: ['9:6']
2025-07-07 10:15:16,515 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:16,695 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:16,695 - WARNING -   Failed to get response for question 66
2025-07-07 10:15:16,695 - INFO - Processing question 63/881 (ID: 68) for fanar
2025-07-07 10:15:16,695 - INFO -   Ground truth verses: ['9:6']
2025-07-07 10:15:16,696 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:16,870 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:16,871 - WARNING -   Failed to get response for question 68
2025-07-07 10:15:16,871 - INFO - Processing question 64/881 (ID: 69) for fanar
2025-07-07 10:15:16,871 - INFO -   Ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 10:15:16,871 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:16,938 - INFO - Processing question 296/881 (ID: 333) for qwen
2025-07-07 10:15:16,938 - INFO -   Ground truth verses: ['41:42']
2025-07-07 10:15:16,938 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:15:17,183 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:17,184 - WARNING -   Failed to get response for question 69
2025-07-07 10:15:17,184 - INFO - Processing question 65/881 (ID: 70) for fanar
2025-07-07 10:15:17,184 - INFO -   Ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 10:15:17,184 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:17,398 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:17,399 - WARNING -   Failed to get response for question 70
2025-07-07 10:15:17,404 - INFO - Saved data to results/fanar_checkpoint_64.json
2025-07-07 10:15:17,404 - INFO - Checkpoint saved for fanar at question 65
2025-07-07 10:15:17,404 - INFO -   Progress: 65/881
2025-07-07 10:15:17,405 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:17,405 - INFO - Processing question 66/881 (ID: 71) for fanar
2025-07-07 10:15:17,405 - INFO -   Ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 10:15:17,405 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:17,602 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:17,603 - WARNING -   Failed to get response for question 71
2025-07-07 10:15:17,603 - INFO - Processing question 67/881 (ID: 72) for fanar
2025-07-07 10:15:17,603 - INFO -   Ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 10:15:17,603 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:17,809 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:17,810 - WARNING -   Failed to get response for question 72
2025-07-07 10:15:17,810 - INFO - Processing question 68/881 (ID: 73) for fanar
2025-07-07 10:15:17,810 - INFO -   Ground truth verses: ['11:52', '6:6', '71:11']
2025-07-07 10:15:17,810 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:18,018 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:18,018 - WARNING -   Failed to get response for question 73
2025-07-07 10:15:18,018 - INFO - Processing question 69/881 (ID: 74) for fanar
2025-07-07 10:15:18,018 - INFO -   Ground truth verses: ['15:63', '44:50']
2025-07-07 10:15:18,018 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:18,218 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:18,218 - WARNING -   Failed to get response for question 74
2025-07-07 10:15:18,218 - INFO - Processing question 70/881 (ID: 75) for fanar
2025-07-07 10:15:18,218 - INFO -   Ground truth verses: ['15:63', '44:50']
2025-07-07 10:15:18,218 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:18,418 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:18,419 - WARNING -   Failed to get response for question 75
2025-07-07 10:15:18,424 - INFO - Saved data to results/fanar_checkpoint_69.json
2025-07-07 10:15:18,424 - INFO - Checkpoint saved for fanar at question 70
2025-07-07 10:15:18,424 - INFO -   Progress: 70/881
2025-07-07 10:15:18,424 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:18,424 - INFO - Processing question 71/881 (ID: 76) for fanar
2025-07-07 10:15:18,424 - INFO -   Ground truth verses: ['15:63', '44:50']
2025-07-07 10:15:18,424 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:18,790 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:18,791 - WARNING -   Failed to get response for question 76
2025-07-07 10:15:18,791 - INFO - Processing question 72/881 (ID: 79) for fanar
2025-07-07 10:15:18,791 - INFO -   Ground truth verses: ['43:87', '43:9', '31:25', '14:33', '29:61']
2025-07-07 10:15:18,791 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:18,998 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:18,999 - WARNING -   Failed to get response for question 79
2025-07-07 10:15:18,999 - INFO - Processing question 73/881 (ID: 80) for fanar
2025-07-07 10:15:18,999 - INFO -   Ground truth verses: ['43:87', '43:9', '31:25', '14:33', '29:61']
2025-07-07 10:15:18,999 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:19,208 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:19,209 - WARNING -   Failed to get response for question 80
2025-07-07 10:15:19,209 - INFO - Processing question 74/881 (ID: 81) for fanar
2025-07-07 10:15:19,209 - INFO -   Ground truth verses: ['43:87', '43:9', '31:25', '14:33', '29:61']
2025-07-07 10:15:19,209 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:19,404 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:19,404 - WARNING -   Failed to get response for question 81
2025-07-07 10:15:19,404 - INFO - Processing question 75/881 (ID: 82) for fanar
2025-07-07 10:15:19,404 - INFO -   Ground truth verses: ['43:87', '43:9', '31:25', '14:33', '29:61']
2025-07-07 10:15:19,404 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:19,605 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:19,606 - WARNING -   Failed to get response for question 82
2025-07-07 10:15:19,611 - INFO - Saved data to results/fanar_checkpoint_74.json
2025-07-07 10:15:19,611 - INFO - Checkpoint saved for fanar at question 75
2025-07-07 10:15:19,611 - INFO -   Progress: 75/881
2025-07-07 10:15:19,611 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:19,611 - INFO - Processing question 76/881 (ID: 83) for fanar
2025-07-07 10:15:19,611 - INFO -   Ground truth verses: ['43:87', '43:9', '31:25', '14:33', '29:61']
2025-07-07 10:15:19,611 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:19,808 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:19,809 - WARNING -   Failed to get response for question 83
2025-07-07 10:15:19,809 - INFO - Processing question 77/881 (ID: 84) for fanar
2025-07-07 10:15:19,809 - INFO -   Ground truth verses: ['59:1', '57:1', '62:1', '45:37', '64:1', '4:132', '3:109', '4:131', '4:126', '42:4', '59:24', '3:129', '14:2', '22:64', '34:1', '53:31', '20:6', '42:53', '31:26', '30:27', '2:284', '16:49', '45:13', '49:16', '3:29', '10:68', '61:1']
2025-07-07 10:15:19,809 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:20,042 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:15:20,042 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:15:20,042 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:15:20,042 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:15:20,042 - INFO - Normalized predicted verses: []
2025-07-07 10:15:20,042 - INFO - Normalized ground truth verses: ['3:23', '4:44', '4:51']
2025-07-07 10:15:20,042 - INFO -   Response: 4329 chars, Found 0 verses
2025-07-07 10:15:20,042 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:20,042 - INFO -   Matched verses: []
2025-07-07 10:15:20,045 - INFO - Saved data to results/deepseek_checkpoint_4.json
2025-07-07 10:15:20,045 - INFO - Checkpoint saved for deepseek at question 5
2025-07-07 10:15:20,046 - INFO -   Progress: 5/881
2025-07-07 10:15:20,046 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:20,130 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:20,130 - WARNING -   Failed to get response for question 84
2025-07-07 10:15:20,130 - INFO - Processing question 78/881 (ID: 85) for fanar
2025-07-07 10:15:20,130 - INFO -   Ground truth verses: ['59:1', '57:1', '62:1', '45:37', '64:1', '4:132', '3:109', '4:131', '4:126', '42:4', '59:24', '3:129', '14:2', '22:64', '34:1', '53:31', '20:6', '42:53', '31:26', '30:27', '2:284', '16:49', '45:13', '49:16', '3:29', '10:68', '61:1']
2025-07-07 10:15:20,130 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:20,292 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:20,292 - WARNING -   Failed to get response for question 85
2025-07-07 10:15:20,292 - INFO - Processing question 79/881 (ID: 86) for fanar
2025-07-07 10:15:20,292 - INFO -   Ground truth verses: ['59:1', '57:1', '62:1', '45:37', '64:1', '4:132', '3:109', '4:131', '4:126', '42:4', '59:24', '3:129', '14:2', '22:64', '34:1', '53:31', '20:6', '42:53', '31:26', '30:27', '2:284', '16:49', '45:13', '49:16', '3:29', '10:68', '61:1']
2025-07-07 10:15:20,292 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:20,453 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:20,453 - WARNING -   Failed to get response for question 86
2025-07-07 10:15:20,453 - INFO - Processing question 80/881 (ID: 87) for fanar
2025-07-07 10:15:20,453 - INFO -   Ground truth verses: ['59:1', '57:1', '62:1', '45:37', '64:1', '4:132', '3:109', '4:131', '4:126', '42:4', '59:24', '3:129', '14:2', '22:64', '34:1', '53:31', '20:6', '42:53', '31:26', '30:27', '2:284', '16:49', '45:13', '49:16', '3:29', '10:68', '61:1']
2025-07-07 10:15:20,453 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:20,617 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:20,618 - WARNING -   Failed to get response for question 87
2025-07-07 10:15:20,623 - INFO - Saved data to results/fanar_checkpoint_79.json
2025-07-07 10:15:20,623 - INFO - Checkpoint saved for fanar at question 80
2025-07-07 10:15:20,623 - INFO -   Progress: 80/881
2025-07-07 10:15:20,623 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:20,623 - INFO - Processing question 81/881 (ID: 88) for fanar
2025-07-07 10:15:20,623 - INFO -   Ground truth verses: ['59:1', '57:1', '62:1', '45:37', '64:1', '4:132', '3:109', '4:131', '4:126', '42:4', '59:24', '3:129', '14:2', '22:64', '34:1', '53:31', '20:6', '42:53', '31:26', '30:27', '2:284', '16:49', '45:13', '49:16', '3:29', '10:68', '61:1']
2025-07-07 10:15:20,623 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:20,784 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:20,784 - WARNING -   Failed to get response for question 88
2025-07-07 10:15:20,784 - INFO - Processing question 82/881 (ID: 89) for fanar
2025-07-07 10:15:20,784 - INFO -   Ground truth verses: ['10:55', '28:13', '52:47', '43:78', '44:39']
2025-07-07 10:15:20,785 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:20,949 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:20,950 - WARNING -   Failed to get response for question 89
2025-07-07 10:15:20,950 - INFO - Processing question 83/881 (ID: 90) for fanar
2025-07-07 10:15:20,950 - INFO -   Ground truth verses: ['10:55', '28:13', '52:47', '43:78', '44:39']
2025-07-07 10:15:20,950 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:21,047 - INFO - Processing question 6/881 (ID: 5) for deepseek
2025-07-07 10:15:21,047 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:15:21,047 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:15:21,113 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:21,114 - WARNING -   Failed to get response for question 90
2025-07-07 10:15:21,114 - INFO - Processing question 84/881 (ID: 91) for fanar
2025-07-07 10:15:21,114 - INFO -   Ground truth verses: ['10:55', '28:13', '52:47', '43:78', '44:39']
2025-07-07 10:15:21,114 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:21,470 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:21,470 - WARNING -   Failed to get response for question 91
2025-07-07 10:15:21,470 - INFO - Processing question 85/881 (ID: 92) for fanar
2025-07-07 10:15:21,470 - INFO -   Ground truth verses: ['10:55', '28:13', '52:47', '43:78', '44:39']
2025-07-07 10:15:21,470 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:21,671 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:21,672 - WARNING -   Failed to get response for question 92
2025-07-07 10:15:21,677 - INFO - Saved data to results/fanar_checkpoint_84.json
2025-07-07 10:15:21,677 - INFO - Checkpoint saved for fanar at question 85
2025-07-07 10:15:21,677 - INFO -   Progress: 85/881
2025-07-07 10:15:21,677 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:21,677 - INFO - Processing question 86/881 (ID: 93) for fanar
2025-07-07 10:15:21,677 - INFO -   Ground truth verses: ['10:55', '28:13', '52:47', '43:78', '44:39']
2025-07-07 10:15:21,677 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:21,876 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:21,877 - WARNING -   Failed to get response for question 93
2025-07-07 10:15:21,877 - INFO - Processing question 87/881 (ID: 94) for fanar
2025-07-07 10:15:21,877 - INFO -   Ground truth verses: ['34:11', '23:51']
2025-07-07 10:15:21,877 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:22,072 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:22,072 - WARNING -   Failed to get response for question 94
2025-07-07 10:15:22,072 - INFO - Processing question 88/881 (ID: 95) for fanar
2025-07-07 10:15:22,073 - INFO -   Ground truth verses: ['34:11', '23:51']
2025-07-07 10:15:22,073 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:22,268 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:22,268 - WARNING -   Failed to get response for question 95
2025-07-07 10:15:22,268 - INFO - Processing question 89/881 (ID: 96) for fanar
2025-07-07 10:15:22,269 - INFO -   Ground truth verses: ['34:11', '23:51']
2025-07-07 10:15:22,269 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:22,464 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:22,464 - WARNING -   Failed to get response for question 96
2025-07-07 10:15:22,465 - INFO - Processing question 90/881 (ID: 97) for fanar
2025-07-07 10:15:22,465 - INFO -   Ground truth verses: ['34:11', '23:51']
2025-07-07 10:15:22,465 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:22,664 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:22,665 - WARNING -   Failed to get response for question 97
2025-07-07 10:15:22,670 - INFO - Saved data to results/fanar_checkpoint_89.json
2025-07-07 10:15:22,670 - INFO - Checkpoint saved for fanar at question 90
2025-07-07 10:15:22,670 - INFO -   Progress: 90/881
2025-07-07 10:15:22,670 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:22,671 - INFO - Processing question 91/881 (ID: 98) for fanar
2025-07-07 10:15:22,671 - INFO -   Ground truth verses: ['34:11', '23:51']
2025-07-07 10:15:22,671 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:22,994 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:22,994 - WARNING -   Failed to get response for question 98
2025-07-07 10:15:22,994 - INFO - Processing question 92/881 (ID: 99) for fanar
2025-07-07 10:15:22,994 - INFO -   Ground truth verses: ['2:189']
2025-07-07 10:15:22,994 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:23,198 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:23,198 - WARNING -   Failed to get response for question 99
2025-07-07 10:15:23,198 - INFO - Processing question 93/881 (ID: 100) for fanar
2025-07-07 10:15:23,198 - INFO -   Ground truth verses: ['2:189']
2025-07-07 10:15:23,198 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:23,392 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:23,392 - WARNING -   Failed to get response for question 100
2025-07-07 10:15:23,392 - INFO - Processing question 94/881 (ID: 101) for fanar
2025-07-07 10:15:23,392 - INFO -   Ground truth verses: ['2:189']
2025-07-07 10:15:23,392 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:23,589 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:23,589 - WARNING -   Failed to get response for question 101
2025-07-07 10:15:23,589 - INFO - Processing question 95/881 (ID: 102) for fanar
2025-07-07 10:15:23,589 - INFO -   Ground truth verses: ['2:189']
2025-07-07 10:15:23,589 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:23,785 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:23,786 - WARNING -   Failed to get response for question 102
2025-07-07 10:15:23,793 - INFO - Saved data to results/fanar_checkpoint_94.json
2025-07-07 10:15:23,793 - INFO - Checkpoint saved for fanar at question 95
2025-07-07 10:15:23,793 - INFO -   Progress: 95/881
2025-07-07 10:15:23,793 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:23,793 - INFO - Processing question 96/881 (ID: 103) for fanar
2025-07-07 10:15:23,793 - INFO -   Ground truth verses: ['2:189']
2025-07-07 10:15:23,793 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:23,994 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:23,995 - WARNING -   Failed to get response for question 103
2025-07-07 10:15:23,995 - INFO - Processing question 97/881 (ID: 104) for fanar
2025-07-07 10:15:23,995 - INFO -   Ground truth verses: ['75:25']
2025-07-07 10:15:23,995 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:24,313 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:24,314 - WARNING -   Failed to get response for question 104
2025-07-07 10:15:24,314 - INFO - Processing question 98/881 (ID: 105) for fanar
2025-07-07 10:15:24,314 - INFO -   Ground truth verses: ['75:25']
2025-07-07 10:15:24,314 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:24,512 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:24,512 - WARNING -   Failed to get response for question 105
2025-07-07 10:15:24,513 - INFO - Processing question 99/881 (ID: 106) for fanar
2025-07-07 10:15:24,513 - INFO -   Ground truth verses: ['75:25']
2025-07-07 10:15:24,513 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:24,711 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:24,712 - WARNING -   Failed to get response for question 106
2025-07-07 10:15:24,712 - INFO - Processing question 100/881 (ID: 107) for fanar
2025-07-07 10:15:24,712 - INFO -   Ground truth verses: ['75:25']
2025-07-07 10:15:24,712 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:24,907 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:24,907 - WARNING -   Failed to get response for question 107
2025-07-07 10:15:24,914 - INFO - Saved data to results/fanar_checkpoint_99.json
2025-07-07 10:15:24,914 - INFO - Checkpoint saved for fanar at question 100
2025-07-07 10:15:24,914 - INFO -   Progress: 100/881
2025-07-07 10:15:24,914 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:24,914 - INFO - Processing question 101/881 (ID: 108) for fanar
2025-07-07 10:15:24,914 - INFO -   Ground truth verses: ['75:25']
2025-07-07 10:15:24,914 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:25,119 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:25,119 - WARNING -   Failed to get response for question 108
2025-07-07 10:15:25,119 - INFO - Processing question 102/881 (ID: 109) for fanar
2025-07-07 10:15:25,119 - INFO -   Ground truth verses: ['7:132']
2025-07-07 10:15:25,119 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:25,318 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:25,319 - WARNING -   Failed to get response for question 109
2025-07-07 10:15:25,319 - INFO - Processing question 103/881 (ID: 110) for fanar
2025-07-07 10:15:25,319 - INFO -   Ground truth verses: ['7:132']
2025-07-07 10:15:25,319 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:25,523 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:25,523 - WARNING -   Failed to get response for question 110
2025-07-07 10:15:25,523 - INFO - Processing question 104/881 (ID: 111) for fanar
2025-07-07 10:15:25,523 - INFO -   Ground truth verses: ['7:132']
2025-07-07 10:15:25,523 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:25,845 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:25,846 - WARNING -   Failed to get response for question 111
2025-07-07 10:15:25,846 - INFO - Processing question 105/881 (ID: 112) for fanar
2025-07-07 10:15:25,846 - INFO -   Ground truth verses: ['7:132']
2025-07-07 10:15:25,846 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:26,055 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:26,056 - WARNING -   Failed to get response for question 112
2025-07-07 10:15:26,062 - INFO - Saved data to results/fanar_checkpoint_104.json
2025-07-07 10:15:26,062 - INFO - Checkpoint saved for fanar at question 105
2025-07-07 10:15:26,062 - INFO -   Progress: 105/881
2025-07-07 10:15:26,062 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:26,062 - INFO - Processing question 106/881 (ID: 113) for fanar
2025-07-07 10:15:26,062 - INFO -   Ground truth verses: ['7:132']
2025-07-07 10:15:26,062 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:26,276 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:26,277 - WARNING -   Failed to get response for question 113
2025-07-07 10:15:26,277 - INFO - Processing question 107/881 (ID: 115) for fanar
2025-07-07 10:15:26,277 - INFO -   Ground truth verses: ['68:5', '37:179', '37:175', '52:15', '51:21', '56:85', '69:38', '69:39']
2025-07-07 10:15:26,277 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:26,482 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:26,483 - WARNING -   Failed to get response for question 115
2025-07-07 10:15:26,483 - INFO - Processing question 108/881 (ID: 116) for fanar
2025-07-07 10:15:26,483 - INFO -   Ground truth verses: ['68:5', '37:179', '37:175', '52:15', '51:21', '56:85', '69:38', '69:39']
2025-07-07 10:15:26,483 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:26,683 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:26,684 - WARNING -   Failed to get response for question 116
2025-07-07 10:15:26,684 - INFO - Processing question 109/881 (ID: 117) for fanar
2025-07-07 10:15:26,684 - INFO -   Ground truth verses: ['68:5', '37:179', '37:175', '52:15', '51:21', '56:85', '69:38', '69:39']
2025-07-07 10:15:26,684 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:26,883 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:26,883 - WARNING -   Failed to get response for question 117
2025-07-07 10:15:26,883 - INFO - Processing question 110/881 (ID: 118) for fanar
2025-07-07 10:15:26,883 - INFO -   Ground truth verses: ['68:5', '37:179', '37:175', '52:15', '51:21', '56:85', '69:38', '69:39']
2025-07-07 10:15:26,883 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:27,081 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:27,081 - WARNING -   Failed to get response for question 118
2025-07-07 10:15:27,089 - INFO - Saved data to results/fanar_checkpoint_109.json
2025-07-07 10:15:27,090 - INFO - Checkpoint saved for fanar at question 110
2025-07-07 10:15:27,090 - INFO -   Progress: 110/881
2025-07-07 10:15:27,090 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:27,090 - INFO - Processing question 111/881 (ID: 119) for fanar
2025-07-07 10:15:27,090 - INFO -   Ground truth verses: ['3:121']
2025-07-07 10:15:27,090 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:27,457 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:27,458 - WARNING -   Failed to get response for question 119
2025-07-07 10:15:27,458 - INFO - Processing question 112/881 (ID: 120) for fanar
2025-07-07 10:15:27,458 - INFO -   Ground truth verses: ['3:121']
2025-07-07 10:15:27,458 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:27,660 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:27,660 - WARNING -   Failed to get response for question 120
2025-07-07 10:15:27,660 - INFO - Processing question 113/881 (ID: 121) for fanar
2025-07-07 10:15:27,660 - INFO -   Ground truth verses: ['3:121']
2025-07-07 10:15:27,660 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:27,854 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:27,854 - WARNING -   Failed to get response for question 121
2025-07-07 10:15:27,854 - INFO - Processing question 114/881 (ID: 122) for fanar
2025-07-07 10:15:27,854 - INFO -   Ground truth verses: ['3:121']
2025-07-07 10:15:27,854 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:28,052 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:28,053 - WARNING -   Failed to get response for question 122
2025-07-07 10:15:28,053 - INFO - Processing question 115/881 (ID: 123) for fanar
2025-07-07 10:15:28,053 - INFO -   Ground truth verses: ['3:121']
2025-07-07 10:15:28,053 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:28,250 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:28,251 - WARNING -   Failed to get response for question 123
2025-07-07 10:15:28,258 - INFO - Saved data to results/fanar_checkpoint_114.json
2025-07-07 10:15:28,258 - INFO - Checkpoint saved for fanar at question 115
2025-07-07 10:15:28,258 - INFO -   Progress: 115/881
2025-07-07 10:15:28,258 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:28,258 - INFO - Processing question 116/881 (ID: 124) for fanar
2025-07-07 10:15:28,258 - INFO -   Ground truth verses: ['39:39', '6:135', '11:93', '11:121']
2025-07-07 10:15:28,258 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:28,454 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:28,454 - WARNING -   Failed to get response for question 124
2025-07-07 10:15:28,454 - INFO - Processing question 117/881 (ID: 125) for fanar
2025-07-07 10:15:28,454 - INFO -   Ground truth verses: ['39:39', '6:135', '11:93', '11:121']
2025-07-07 10:15:28,454 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:28,783 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:28,783 - WARNING -   Failed to get response for question 125
2025-07-07 10:15:28,783 - INFO - Processing question 118/881 (ID: 126) for fanar
2025-07-07 10:15:28,783 - INFO -   Ground truth verses: ['39:39', '6:135', '11:93', '11:121']
2025-07-07 10:15:28,783 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:28,988 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:28,989 - WARNING -   Failed to get response for question 126
2025-07-07 10:15:28,989 - INFO - Processing question 119/881 (ID: 128) for fanar
2025-07-07 10:15:28,989 - INFO -   Ground truth verses: ['39:39', '6:135', '11:93', '11:121']
2025-07-07 10:15:28,989 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:29,189 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:29,189 - WARNING -   Failed to get response for question 128
2025-07-07 10:15:29,189 - INFO - Processing question 120/881 (ID: 129) for fanar
2025-07-07 10:15:29,189 - INFO -   Ground truth verses: ['26:213', '28:70', '23:117', '25:68', '15:96', '51:51', '28:88']
2025-07-07 10:15:29,189 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:29,388 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:29,388 - WARNING -   Failed to get response for question 129
2025-07-07 10:15:29,395 - INFO - Saved data to results/fanar_checkpoint_119.json
2025-07-07 10:15:29,395 - INFO - Checkpoint saved for fanar at question 120
2025-07-07 10:15:29,395 - INFO -   Progress: 120/881
2025-07-07 10:15:29,395 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:29,395 - INFO - Processing question 121/881 (ID: 130) for fanar
2025-07-07 10:15:29,395 - INFO -   Ground truth verses: ['26:213', '28:70', '23:117', '25:68', '15:96', '51:51', '28:88']
2025-07-07 10:15:29,395 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:29,600 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:29,601 - WARNING -   Failed to get response for question 130
2025-07-07 10:15:29,601 - INFO - Processing question 122/881 (ID: 131) for fanar
2025-07-07 10:15:29,601 - INFO -   Ground truth verses: ['26:213', '28:70', '23:117', '25:68', '15:96', '51:51', '28:88']
2025-07-07 10:15:29,601 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:29,802 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:29,802 - WARNING -   Failed to get response for question 131
2025-07-07 10:15:29,802 - INFO - Processing question 123/881 (ID: 132) for fanar
2025-07-07 10:15:29,802 - INFO -   Ground truth verses: ['26:213', '28:70', '23:117', '25:68', '15:96', '51:51', '28:88']
2025-07-07 10:15:29,802 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:30,001 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:30,002 - WARNING -   Failed to get response for question 132
2025-07-07 10:15:30,002 - INFO - Processing question 124/881 (ID: 139) for fanar
2025-07-07 10:15:30,002 - INFO -   Ground truth verses: ['57:5', '3:109', '8:44']
2025-07-07 10:15:30,002 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:30,323 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:30,324 - WARNING -   Failed to get response for question 139
2025-07-07 10:15:30,324 - INFO - Processing question 125/881 (ID: 140) for fanar
2025-07-07 10:15:30,324 - INFO -   Ground truth verses: ['57:5', '3:109', '8:44']
2025-07-07 10:15:30,324 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:30,525 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:30,526 - WARNING -   Failed to get response for question 140
2025-07-07 10:15:30,533 - INFO - Saved data to results/fanar_checkpoint_124.json
2025-07-07 10:15:30,533 - INFO - Checkpoint saved for fanar at question 125
2025-07-07 10:15:30,533 - INFO -   Progress: 125/881
2025-07-07 10:15:30,533 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:30,533 - INFO - Processing question 126/881 (ID: 141) for fanar
2025-07-07 10:15:30,533 - INFO -   Ground truth verses: ['57:5', '3:109', '8:44']
2025-07-07 10:15:30,533 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:30,742 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:30,742 - WARNING -   Failed to get response for question 141
2025-07-07 10:15:30,742 - INFO - Processing question 127/881 (ID: 142) for fanar
2025-07-07 10:15:30,742 - INFO -   Ground truth verses: ['57:5', '3:109', '8:44']
2025-07-07 10:15:30,742 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:30,944 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:30,945 - WARNING -   Failed to get response for question 142
2025-07-07 10:15:30,945 - INFO - Processing question 128/881 (ID: 143) for fanar
2025-07-07 10:15:30,945 - INFO -   Ground truth verses: ['57:5', '3:109', '8:44']
2025-07-07 10:15:30,945 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:31,147 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:31,148 - WARNING -   Failed to get response for question 143
2025-07-07 10:15:31,148 - INFO - Processing question 129/881 (ID: 144) for fanar
2025-07-07 10:15:31,148 - INFO -   Ground truth verses: ['56:93', '26:101', '78:25', '56:42', '70:10']
2025-07-07 10:15:31,148 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:31,345 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:31,346 - WARNING -   Failed to get response for question 144
2025-07-07 10:15:31,346 - INFO - Processing question 130/881 (ID: 145) for fanar
2025-07-07 10:15:31,346 - INFO -   Ground truth verses: ['56:93', '26:101', '78:25', '56:42', '70:10']
2025-07-07 10:15:31,346 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:31,545 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:31,546 - WARNING -   Failed to get response for question 145
2025-07-07 10:15:31,552 - INFO - Saved data to results/fanar_checkpoint_129.json
2025-07-07 10:15:31,552 - INFO - Checkpoint saved for fanar at question 130
2025-07-07 10:15:31,552 - INFO -   Progress: 130/881
2025-07-07 10:15:31,552 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:31,552 - INFO - Processing question 131/881 (ID: 146) for fanar
2025-07-07 10:15:31,552 - INFO -   Ground truth verses: ['56:93', '26:101', '78:25', '56:42', '70:10']
2025-07-07 10:15:31,553 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:31,871 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:31,872 - WARNING -   Failed to get response for question 146
2025-07-07 10:15:31,872 - INFO - Processing question 132/881 (ID: 147) for fanar
2025-07-07 10:15:31,872 - INFO -   Ground truth verses: ['56:93', '26:101', '78:25', '56:42', '70:10']
2025-07-07 10:15:31,872 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:32,070 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:32,071 - WARNING -   Failed to get response for question 147
2025-07-07 10:15:32,071 - INFO - Processing question 133/881 (ID: 148) for fanar
2025-07-07 10:15:32,071 - INFO -   Ground truth verses: ['56:93', '26:101', '78:25', '56:42', '70:10']
2025-07-07 10:15:32,071 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:32,270 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:32,270 - WARNING -   Failed to get response for question 148
2025-07-07 10:15:32,270 - INFO - Processing question 134/881 (ID: 149) for fanar
2025-07-07 10:15:32,270 - INFO -   Ground truth verses: ['6:160', '27:89', '27:90', '28:84']
2025-07-07 10:15:32,270 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:32,472 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:32,472 - WARNING -   Failed to get response for question 149
2025-07-07 10:15:32,472 - INFO - Processing question 135/881 (ID: 150) for fanar
2025-07-07 10:15:32,472 - INFO -   Ground truth verses: ['6:160', '27:89', '27:90', '28:84']
2025-07-07 10:15:32,472 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:32,676 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:32,676 - WARNING -   Failed to get response for question 150
2025-07-07 10:15:32,683 - INFO - Saved data to results/fanar_checkpoint_134.json
2025-07-07 10:15:32,683 - INFO - Checkpoint saved for fanar at question 135
2025-07-07 10:15:32,683 - INFO -   Progress: 135/881
2025-07-07 10:15:32,683 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:32,683 - INFO - Processing question 136/881 (ID: 151) for fanar
2025-07-07 10:15:32,683 - INFO -   Ground truth verses: ['6:160', '27:89', '27:90', '28:84']
2025-07-07 10:15:32,684 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:32,889 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:32,890 - WARNING -   Failed to get response for question 151
2025-07-07 10:15:32,890 - INFO - Processing question 137/881 (ID: 152) for fanar
2025-07-07 10:15:32,890 - INFO -   Ground truth verses: ['6:160', '27:89', '27:90', '28:84']
2025-07-07 10:15:32,890 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:33,214 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:33,214 - WARNING -   Failed to get response for question 152
2025-07-07 10:15:33,214 - INFO - Processing question 138/881 (ID: 153) for fanar
2025-07-07 10:15:33,214 - INFO -   Ground truth verses: ['6:160', '27:89', '27:90', '28:84']
2025-07-07 10:15:33,214 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:33,419 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:33,420 - WARNING -   Failed to get response for question 153
2025-07-07 10:15:33,420 - INFO - Processing question 139/881 (ID: 154) for fanar
2025-07-07 10:15:33,420 - INFO -   Ground truth verses: ['16:65', '13:12', '29:63', '45:5', '2:164', '30:24']
2025-07-07 10:15:33,420 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:33,619 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:33,619 - WARNING -   Failed to get response for question 154
2025-07-07 10:15:33,619 - INFO - Processing question 140/881 (ID: 155) for fanar
2025-07-07 10:15:33,619 - INFO -   Ground truth verses: ['16:65', '13:12', '29:63', '45:5', '2:164', '30:24']
2025-07-07 10:15:33,619 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:33,692 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:15:33,692 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:15:33,693 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:15:33,693 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:15:33,693 - INFO - Normalized predicted verses: []
2025-07-07 10:15:33,693 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 10:15:33,693 - INFO -   Response: 2370 chars, Found 0 verses
2025-07-07 10:15:33,693 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:33,693 - INFO -   Matched verses: []
2025-07-07 10:15:33,810 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:33,810 - WARNING -   Failed to get response for question 155
2025-07-07 10:15:33,817 - INFO - Saved data to results/fanar_checkpoint_139.json
2025-07-07 10:15:33,817 - INFO - Checkpoint saved for fanar at question 140
2025-07-07 10:15:33,817 - INFO -   Progress: 140/881
2025-07-07 10:15:33,817 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:33,817 - INFO - Processing question 141/881 (ID: 156) for fanar
2025-07-07 10:15:33,817 - INFO -   Ground truth verses: ['16:65', '13:12', '29:63', '45:5', '2:164', '30:24']
2025-07-07 10:15:33,817 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:33,974 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:33,975 - WARNING -   Failed to get response for question 156
2025-07-07 10:15:33,975 - INFO - Processing question 142/881 (ID: 157) for fanar
2025-07-07 10:15:33,975 - INFO -   Ground truth verses: ['16:65', '13:12', '29:63', '45:5', '2:164', '30:24']
2025-07-07 10:15:33,975 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:34,136 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:34,136 - WARNING -   Failed to get response for question 157
2025-07-07 10:15:34,136 - INFO - Processing question 143/881 (ID: 158) for fanar
2025-07-07 10:15:34,136 - INFO -   Ground truth verses: ['16:65', '13:12', '29:63', '45:5', '2:164', '30:24']
2025-07-07 10:15:34,136 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:34,296 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:34,297 - WARNING -   Failed to get response for question 158
2025-07-07 10:15:34,297 - INFO - Processing question 144/881 (ID: 159) for fanar
2025-07-07 10:15:34,297 - INFO -   Ground truth verses: ['9:72', '9:89', '4:13', '85:11', '20:76', '64:9', '5:119', '48:5', '5:85', '4:57', '57:12', '4:122', '22:14', '3:136', '61:12']
2025-07-07 10:15:34,297 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:34,592 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:34,593 - WARNING -   Failed to get response for question 159
2025-07-07 10:15:34,593 - INFO - Processing question 145/881 (ID: 160) for fanar
2025-07-07 10:15:34,593 - INFO -   Ground truth verses: ['9:72', '9:89', '4:13', '85:11', '20:76', '64:9', '5:119', '48:5', '5:85', '4:57', '57:12', '4:122', '22:14', '3:136', '61:12']
2025-07-07 10:15:34,593 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:34,694 - INFO - Processing question 7/881 (ID: 6) for deepseek
2025-07-07 10:15:34,694 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:15:34,694 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:15:34,754 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:34,754 - WARNING -   Failed to get response for question 160
2025-07-07 10:15:34,762 - INFO - Saved data to results/fanar_checkpoint_144.json
2025-07-07 10:15:34,762 - INFO - Checkpoint saved for fanar at question 145
2025-07-07 10:15:34,762 - INFO -   Progress: 145/881
2025-07-07 10:15:34,762 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:34,762 - INFO - Processing question 146/881 (ID: 161) for fanar
2025-07-07 10:15:34,762 - INFO -   Ground truth verses: ['9:72', '9:89', '4:13', '85:11', '20:76', '64:9', '5:119', '48:5', '5:85', '4:57', '57:12', '4:122', '22:14', '3:136', '61:12']
2025-07-07 10:15:34,762 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:34,959 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:34,959 - WARNING -   Failed to get response for question 161
2025-07-07 10:15:34,959 - INFO - Processing question 147/881 (ID: 162) for fanar
2025-07-07 10:15:34,959 - INFO -   Ground truth verses: ['9:72', '9:89', '4:13', '85:11', '20:76', '64:9', '5:119', '48:5', '5:85', '4:57', '57:12', '4:122', '22:14', '3:136', '61:12']
2025-07-07 10:15:34,959 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:35,159 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:35,160 - WARNING -   Failed to get response for question 162
2025-07-07 10:15:35,160 - INFO - Processing question 148/881 (ID: 163) for fanar
2025-07-07 10:15:35,160 - INFO -   Ground truth verses: ['9:72', '9:89', '4:13', '85:11', '20:76', '64:9', '5:119', '48:5', '5:85', '4:57', '57:12', '4:122', '22:14', '3:136', '61:12']
2025-07-07 10:15:35,160 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:35,354 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:35,355 - WARNING -   Failed to get response for question 163
2025-07-07 10:15:35,355 - INFO - Processing question 149/881 (ID: 164) for fanar
2025-07-07 10:15:35,355 - INFO -   Ground truth verses: ['21:67', '16:73', '21:98', '29:17']
2025-07-07 10:15:35,355 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:35,552 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:35,552 - WARNING -   Failed to get response for question 164
2025-07-07 10:15:35,552 - INFO - Processing question 150/881 (ID: 165) for fanar
2025-07-07 10:15:35,552 - INFO -   Ground truth verses: ['21:67', '16:73', '21:98', '29:17']
2025-07-07 10:15:35,552 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:35,749 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:35,749 - WARNING -   Failed to get response for question 165
2025-07-07 10:15:35,757 - INFO - Saved data to results/fanar_checkpoint_149.json
2025-07-07 10:15:35,757 - INFO - Checkpoint saved for fanar at question 150
2025-07-07 10:15:35,757 - INFO -   Progress: 150/881
2025-07-07 10:15:35,757 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:35,757 - INFO - Processing question 151/881 (ID: 166) for fanar
2025-07-07 10:15:35,757 - INFO -   Ground truth verses: ['21:67', '16:73', '21:98', '29:17']
2025-07-07 10:15:35,757 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:36,094 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:36,094 - WARNING -   Failed to get response for question 166
2025-07-07 10:15:36,095 - INFO - Processing question 152/881 (ID: 167) for fanar
2025-07-07 10:15:36,095 - INFO -   Ground truth verses: ['21:67', '16:73', '21:98', '29:17']
2025-07-07 10:15:36,095 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:36,286 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:36,286 - WARNING -   Failed to get response for question 167
2025-07-07 10:15:36,286 - INFO - Processing question 153/881 (ID: 168) for fanar
2025-07-07 10:15:36,287 - INFO -   Ground truth verses: ['21:67', '16:73', '21:98', '29:17']
2025-07-07 10:15:36,287 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:36,487 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:36,487 - WARNING -   Failed to get response for question 168
2025-07-07 10:15:36,487 - INFO - Processing question 154/881 (ID: 169) for fanar
2025-07-07 10:15:36,487 - INFO -   Ground truth verses: ['3:183']
2025-07-07 10:15:36,487 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:36,688 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:36,689 - WARNING -   Failed to get response for question 169
2025-07-07 10:15:36,689 - INFO - Processing question 155/881 (ID: 170) for fanar
2025-07-07 10:15:36,689 - INFO -   Ground truth verses: ['3:183']
2025-07-07 10:15:36,689 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:36,895 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:36,895 - WARNING -   Failed to get response for question 170
2025-07-07 10:15:36,903 - INFO - Saved data to results/fanar_checkpoint_154.json
2025-07-07 10:15:36,903 - INFO - Checkpoint saved for fanar at question 155
2025-07-07 10:15:36,903 - INFO -   Progress: 155/881
2025-07-07 10:15:36,903 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:36,903 - INFO - Processing question 156/881 (ID: 171) for fanar
2025-07-07 10:15:36,903 - INFO -   Ground truth verses: ['3:183']
2025-07-07 10:15:36,903 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:37,100 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:37,101 - WARNING -   Failed to get response for question 171
2025-07-07 10:15:37,101 - INFO - Processing question 157/881 (ID: 172) for fanar
2025-07-07 10:15:37,101 - INFO -   Ground truth verses: ['3:183']
2025-07-07 10:15:37,101 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:37,299 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:37,299 - WARNING -   Failed to get response for question 172
2025-07-07 10:15:37,299 - INFO - Processing question 158/881 (ID: 173) for fanar
2025-07-07 10:15:37,299 - INFO -   Ground truth verses: ['3:183']
2025-07-07 10:15:37,299 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:37,654 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:37,654 - WARNING -   Failed to get response for question 173
2025-07-07 10:15:37,654 - INFO - Processing question 159/881 (ID: 174) for fanar
2025-07-07 10:15:37,654 - INFO -   Ground truth verses: ['2:204']
2025-07-07 10:15:37,654 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:37,854 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:37,854 - WARNING -   Failed to get response for question 174
2025-07-07 10:15:37,854 - INFO - Processing question 160/881 (ID: 175) for fanar
2025-07-07 10:15:37,854 - INFO -   Ground truth verses: ['2:204']
2025-07-07 10:15:37,855 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:38,030 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:15:38,030 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:15:38,030 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:15:38,030 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:15:38,030 - INFO - Normalized predicted verses: []
2025-07-07 10:15:38,030 - INFO - Normalized ground truth verses: ['41:42']
2025-07-07 10:15:38,030 - INFO -   Response: 5158 chars, Found 0 verses
2025-07-07 10:15:38,030 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:38,030 - INFO -   Matched verses: []
2025-07-07 10:15:38,051 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:38,052 - WARNING -   Failed to get response for question 175
2025-07-07 10:15:38,059 - INFO - Saved data to results/fanar_checkpoint_159.json
2025-07-07 10:15:38,059 - INFO - Checkpoint saved for fanar at question 160
2025-07-07 10:15:38,059 - INFO -   Progress: 160/881
2025-07-07 10:15:38,059 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:38,059 - INFO - Processing question 161/881 (ID: 176) for fanar
2025-07-07 10:15:38,059 - INFO -   Ground truth verses: ['2:204']
2025-07-07 10:15:38,059 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:38,223 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:38,223 - WARNING -   Failed to get response for question 176
2025-07-07 10:15:38,223 - INFO - Processing question 162/881 (ID: 177) for fanar
2025-07-07 10:15:38,223 - INFO -   Ground truth verses: ['2:204']
2025-07-07 10:15:38,223 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:38,389 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:38,390 - WARNING -   Failed to get response for question 177
2025-07-07 10:15:38,390 - INFO - Processing question 163/881 (ID: 178) for fanar
2025-07-07 10:15:38,390 - INFO -   Ground truth verses: ['2:204']
2025-07-07 10:15:38,390 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:38,564 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:38,564 - WARNING -   Failed to get response for question 178
2025-07-07 10:15:38,564 - INFO - Processing question 164/881 (ID: 179) for fanar
2025-07-07 10:15:38,564 - INFO -   Ground truth verses: ['2:96']
2025-07-07 10:15:38,564 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:38,873 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:38,874 - WARNING -   Failed to get response for question 179
2025-07-07 10:15:38,874 - INFO - Processing question 165/881 (ID: 180) for fanar
2025-07-07 10:15:38,874 - INFO -   Ground truth verses: ['2:96']
2025-07-07 10:15:38,874 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:39,031 - INFO - Processing question 297/881 (ID: 334) for qwen
2025-07-07 10:15:39,031 - INFO -   Ground truth verses: ['41:42']
2025-07-07 10:15:39,032 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:15:39,045 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:39,046 - WARNING -   Failed to get response for question 180
2025-07-07 10:15:39,053 - INFO - Saved data to results/fanar_checkpoint_164.json
2025-07-07 10:15:39,053 - INFO - Checkpoint saved for fanar at question 165
2025-07-07 10:15:39,053 - INFO -   Progress: 165/881
2025-07-07 10:15:39,053 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:39,053 - INFO - Processing question 166/881 (ID: 181) for fanar
2025-07-07 10:15:39,054 - INFO -   Ground truth verses: ['2:96']
2025-07-07 10:15:39,054 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:39,253 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:39,253 - WARNING -   Failed to get response for question 181
2025-07-07 10:15:39,254 - INFO - Processing question 167/881 (ID: 182) for fanar
2025-07-07 10:15:39,254 - INFO -   Ground truth verses: ['2:96']
2025-07-07 10:15:39,254 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:39,449 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:39,449 - WARNING -   Failed to get response for question 182
2025-07-07 10:15:39,449 - INFO - Processing question 168/881 (ID: 183) for fanar
2025-07-07 10:15:39,449 - INFO -   Ground truth verses: ['2:96']
2025-07-07 10:15:39,449 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:39,645 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:39,646 - WARNING -   Failed to get response for question 183
2025-07-07 10:15:39,646 - INFO - Processing question 169/881 (ID: 184) for fanar
2025-07-07 10:15:39,646 - INFO -   Ground truth verses: ['84:16', '81:15', '69:38', '75:2']
2025-07-07 10:15:39,646 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:39,841 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:39,842 - WARNING -   Failed to get response for question 184
2025-07-07 10:15:39,842 - INFO - Processing question 170/881 (ID: 185) for fanar
2025-07-07 10:15:39,842 - INFO -   Ground truth verses: ['84:16', '81:15', '69:38', '75:2']
2025-07-07 10:15:39,842 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:40,038 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:40,038 - WARNING -   Failed to get response for question 185
2025-07-07 10:15:40,046 - INFO - Saved data to results/fanar_checkpoint_169.json
2025-07-07 10:15:40,046 - INFO - Checkpoint saved for fanar at question 170
2025-07-07 10:15:40,046 - INFO -   Progress: 170/881
2025-07-07 10:15:40,046 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:40,046 - INFO - Processing question 171/881 (ID: 186) for fanar
2025-07-07 10:15:40,046 - INFO -   Ground truth verses: ['84:16', '81:15', '69:38', '75:2']
2025-07-07 10:15:40,047 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:40,387 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:40,388 - WARNING -   Failed to get response for question 186
2025-07-07 10:15:40,388 - INFO - Processing question 172/881 (ID: 187) for fanar
2025-07-07 10:15:40,388 - INFO -   Ground truth verses: ['84:16', '81:15', '69:38', '75:2']
2025-07-07 10:15:40,388 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:40,586 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:40,587 - WARNING -   Failed to get response for question 187
2025-07-07 10:15:40,587 - INFO - Processing question 173/881 (ID: 189) for fanar
2025-07-07 10:15:40,587 - INFO -   Ground truth verses: ['16:91', '13:20']
2025-07-07 10:15:40,587 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:40,781 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:40,781 - WARNING -   Failed to get response for question 189
2025-07-07 10:15:40,781 - INFO - Processing question 174/881 (ID: 190) for fanar
2025-07-07 10:15:40,781 - INFO -   Ground truth verses: ['16:91', '13:20']
2025-07-07 10:15:40,782 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:40,977 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:40,978 - WARNING -   Failed to get response for question 190
2025-07-07 10:15:40,978 - INFO - Processing question 175/881 (ID: 192) for fanar
2025-07-07 10:15:40,978 - INFO -   Ground truth verses: ['16:91', '13:20']
2025-07-07 10:15:40,978 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:41,173 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:41,174 - WARNING -   Failed to get response for question 192
2025-07-07 10:15:41,182 - INFO - Saved data to results/fanar_checkpoint_174.json
2025-07-07 10:15:41,182 - INFO - Checkpoint saved for fanar at question 175
2025-07-07 10:15:41,182 - INFO -   Progress: 175/881
2025-07-07 10:15:41,182 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:41,182 - INFO - Processing question 176/881 (ID: 194) for fanar
2025-07-07 10:15:41,182 - INFO -   Ground truth verses: ['20:120']
2025-07-07 10:15:41,182 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:41,388 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:41,389 - WARNING -   Failed to get response for question 194
2025-07-07 10:15:41,389 - INFO - Processing question 177/881 (ID: 195) for fanar
2025-07-07 10:15:41,389 - INFO -   Ground truth verses: ['20:120']
2025-07-07 10:15:41,389 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:41,589 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:41,589 - WARNING -   Failed to get response for question 195
2025-07-07 10:15:41,589 - INFO - Processing question 178/881 (ID: 196) for fanar
2025-07-07 10:15:41,590 - INFO -   Ground truth verses: ['20:120']
2025-07-07 10:15:41,590 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:41,921 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:41,921 - WARNING -   Failed to get response for question 196
2025-07-07 10:15:41,922 - INFO - Processing question 179/881 (ID: 197) for fanar
2025-07-07 10:15:41,922 - INFO -   Ground truth verses: ['20:120']
2025-07-07 10:15:41,922 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:42,121 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:42,122 - WARNING -   Failed to get response for question 197
2025-07-07 10:15:42,122 - INFO - Processing question 180/881 (ID: 198) for fanar
2025-07-07 10:15:42,122 - INFO -   Ground truth verses: ['20:120']
2025-07-07 10:15:42,122 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:42,319 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:42,320 - WARNING -   Failed to get response for question 198
2025-07-07 10:15:42,328 - INFO - Saved data to results/fanar_checkpoint_179.json
2025-07-07 10:15:42,328 - INFO - Checkpoint saved for fanar at question 180
2025-07-07 10:15:42,328 - INFO -   Progress: 180/881
2025-07-07 10:15:42,328 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:42,328 - INFO - Processing question 181/881 (ID: 199) for fanar
2025-07-07 10:15:42,328 - INFO -   Ground truth verses: ['5:4']
2025-07-07 10:15:42,329 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:42,537 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:42,537 - WARNING -   Failed to get response for question 199
2025-07-07 10:15:42,537 - INFO - Processing question 182/881 (ID: 200) for fanar
2025-07-07 10:15:42,537 - INFO -   Ground truth verses: ['5:4']
2025-07-07 10:15:42,537 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:42,737 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:42,738 - WARNING -   Failed to get response for question 200
2025-07-07 10:15:42,738 - INFO - Processing question 183/881 (ID: 201) for fanar
2025-07-07 10:15:42,738 - INFO -   Ground truth verses: ['5:4']
2025-07-07 10:15:42,738 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:42,936 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:42,937 - WARNING -   Failed to get response for question 201
2025-07-07 10:15:42,937 - INFO - Processing question 184/881 (ID: 202) for fanar
2025-07-07 10:15:42,937 - INFO -   Ground truth verses: ['5:4']
2025-07-07 10:15:42,937 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:43,138 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:43,139 - WARNING -   Failed to get response for question 202
2025-07-07 10:15:43,139 - INFO - Processing question 185/881 (ID: 203) for fanar
2025-07-07 10:15:43,139 - INFO -   Ground truth verses: ['5:4']
2025-07-07 10:15:43,139 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:43,465 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:43,466 - WARNING -   Failed to get response for question 203
2025-07-07 10:15:43,475 - INFO - Saved data to results/fanar_checkpoint_184.json
2025-07-07 10:15:43,475 - INFO - Checkpoint saved for fanar at question 185
2025-07-07 10:15:43,475 - INFO -   Progress: 185/881
2025-07-07 10:15:43,475 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:43,475 - INFO - Processing question 186/881 (ID: 204) for fanar
2025-07-07 10:15:43,475 - INFO -   Ground truth verses: ['56:38', '69:45', '74:39', '20:17']
2025-07-07 10:15:43,475 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:43,677 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:43,677 - WARNING -   Failed to get response for question 204
2025-07-07 10:15:43,677 - INFO - Processing question 187/881 (ID: 205) for fanar
2025-07-07 10:15:43,677 - INFO -   Ground truth verses: ['56:38', '69:45', '74:39', '20:17']
2025-07-07 10:15:43,677 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:43,873 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:43,874 - WARNING -   Failed to get response for question 205
2025-07-07 10:15:43,874 - INFO - Processing question 188/881 (ID: 206) for fanar
2025-07-07 10:15:43,874 - INFO -   Ground truth verses: ['56:38', '69:45', '74:39', '20:17']
2025-07-07 10:15:43,874 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:44,072 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:44,072 - WARNING -   Failed to get response for question 206
2025-07-07 10:15:44,072 - INFO - Processing question 189/881 (ID: 207) for fanar
2025-07-07 10:15:44,072 - INFO -   Ground truth verses: ['56:38', '69:45', '74:39', '20:17']
2025-07-07 10:15:44,072 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:44,271 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:44,271 - WARNING -   Failed to get response for question 207
2025-07-07 10:15:44,271 - INFO - Processing question 190/881 (ID: 208) for fanar
2025-07-07 10:15:44,271 - INFO -   Ground truth verses: ['56:38', '69:45', '74:39', '20:17']
2025-07-07 10:15:44,272 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:44,469 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:44,469 - WARNING -   Failed to get response for question 208
2025-07-07 10:15:44,478 - INFO - Saved data to results/fanar_checkpoint_189.json
2025-07-07 10:15:44,478 - INFO - Checkpoint saved for fanar at question 190
2025-07-07 10:15:44,478 - INFO -   Progress: 190/881
2025-07-07 10:15:44,478 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:44,478 - INFO - Processing question 191/881 (ID: 209) for fanar
2025-07-07 10:15:44,478 - INFO -   Ground truth verses: ['33:37']
2025-07-07 10:15:44,478 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:44,687 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:44,687 - WARNING -   Failed to get response for question 209
2025-07-07 10:15:44,687 - INFO - Processing question 192/881 (ID: 210) for fanar
2025-07-07 10:15:44,687 - INFO -   Ground truth verses: ['33:37']
2025-07-07 10:15:44,687 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:45,016 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:45,017 - WARNING -   Failed to get response for question 210
2025-07-07 10:15:45,017 - INFO - Processing question 193/881 (ID: 211) for fanar
2025-07-07 10:15:45,017 - INFO -   Ground truth verses: ['33:37']
2025-07-07 10:15:45,017 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:45,223 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:45,224 - WARNING -   Failed to get response for question 211
2025-07-07 10:15:45,224 - INFO - Processing question 194/881 (ID: 212) for fanar
2025-07-07 10:15:45,224 - INFO -   Ground truth verses: ['33:37']
2025-07-07 10:15:45,224 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:45,423 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:45,424 - WARNING -   Failed to get response for question 212
2025-07-07 10:15:45,424 - INFO - Processing question 195/881 (ID: 213) for fanar
2025-07-07 10:15:45,424 - INFO -   Ground truth verses: ['33:37']
2025-07-07 10:15:45,424 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:45,622 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:45,622 - WARNING -   Failed to get response for question 213
2025-07-07 10:15:45,631 - INFO - Saved data to results/fanar_checkpoint_194.json
2025-07-07 10:15:45,631 - INFO - Checkpoint saved for fanar at question 195
2025-07-07 10:15:45,631 - INFO -   Progress: 195/881
2025-07-07 10:15:45,631 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:45,631 - INFO - Processing question 196/881 (ID: 214) for fanar
2025-07-07 10:15:45,631 - INFO -   Ground truth verses: ['33:17', '4:173', '4:123']
2025-07-07 10:15:45,631 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:45,837 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:45,837 - WARNING -   Failed to get response for question 214
2025-07-07 10:15:45,837 - INFO - Processing question 197/881 (ID: 215) for fanar
2025-07-07 10:15:45,837 - INFO -   Ground truth verses: ['33:17', '4:173', '4:123']
2025-07-07 10:15:45,837 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:46,036 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:46,037 - WARNING -   Failed to get response for question 215
2025-07-07 10:15:46,037 - INFO - Processing question 198/881 (ID: 216) for fanar
2025-07-07 10:15:46,037 - INFO -   Ground truth verses: ['33:17', '4:173', '4:123']
2025-07-07 10:15:46,037 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:46,238 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:46,238 - WARNING -   Failed to get response for question 216
2025-07-07 10:15:46,238 - INFO - Processing question 199/881 (ID: 217) for fanar
2025-07-07 10:15:46,238 - INFO -   Ground truth verses: ['33:17', '4:173', '4:123']
2025-07-07 10:15:46,238 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:46,571 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:46,571 - WARNING -   Failed to get response for question 217
2025-07-07 10:15:46,571 - INFO - Processing question 200/881 (ID: 218) for fanar
2025-07-07 10:15:46,571 - INFO -   Ground truth verses: ['33:17', '4:173', '4:123']
2025-07-07 10:15:46,571 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:46,777 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:46,778 - WARNING -   Failed to get response for question 218
2025-07-07 10:15:46,786 - INFO - Saved data to results/fanar_checkpoint_199.json
2025-07-07 10:15:46,786 - INFO - Checkpoint saved for fanar at question 200
2025-07-07 10:15:46,786 - INFO -   Progress: 200/881
2025-07-07 10:15:46,786 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:46,786 - INFO - Processing question 201/881 (ID: 219) for fanar
2025-07-07 10:15:46,786 - INFO -   Ground truth verses: ['32:12']
2025-07-07 10:15:46,786 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:46,992 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:46,992 - WARNING -   Failed to get response for question 219
2025-07-07 10:15:46,992 - INFO - Processing question 202/881 (ID: 220) for fanar
2025-07-07 10:15:46,992 - INFO -   Ground truth verses: ['32:12']
2025-07-07 10:15:46,992 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:47,200 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:47,201 - WARNING -   Failed to get response for question 220
2025-07-07 10:15:47,201 - INFO - Processing question 203/881 (ID: 221) for fanar
2025-07-07 10:15:47,201 - INFO -   Ground truth verses: ['32:12']
2025-07-07 10:15:47,201 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:47,395 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:47,395 - WARNING -   Failed to get response for question 221
2025-07-07 10:15:47,395 - INFO - Processing question 204/881 (ID: 222) for fanar
2025-07-07 10:15:47,395 - INFO -   Ground truth verses: ['32:12']
2025-07-07 10:15:47,395 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:47,595 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:47,596 - WARNING -   Failed to get response for question 222
2025-07-07 10:15:47,596 - INFO - Processing question 205/881 (ID: 223) for fanar
2025-07-07 10:15:47,596 - INFO -   Ground truth verses: ['32:12']
2025-07-07 10:15:47,596 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:47,794 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:47,795 - WARNING -   Failed to get response for question 223
2025-07-07 10:15:47,804 - INFO - Saved data to results/fanar_checkpoint_204.json
2025-07-07 10:15:47,804 - INFO - Checkpoint saved for fanar at question 205
2025-07-07 10:15:47,804 - INFO -   Progress: 205/881
2025-07-07 10:15:47,804 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:47,804 - INFO - Processing question 206/881 (ID: 224) for fanar
2025-07-07 10:15:47,804 - INFO -   Ground truth verses: ['84:20', '57:8']
2025-07-07 10:15:47,804 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:48,154 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:48,155 - WARNING -   Failed to get response for question 224
2025-07-07 10:15:48,155 - INFO - Processing question 207/881 (ID: 225) for fanar
2025-07-07 10:15:48,155 - INFO -   Ground truth verses: ['84:20', '57:8']
2025-07-07 10:15:48,155 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:48,358 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:48,359 - WARNING -   Failed to get response for question 225
2025-07-07 10:15:48,359 - INFO - Processing question 208/881 (ID: 226) for fanar
2025-07-07 10:15:48,359 - INFO -   Ground truth verses: ['84:20', '57:8']
2025-07-07 10:15:48,359 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:48,557 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:48,557 - WARNING -   Failed to get response for question 226
2025-07-07 10:15:48,558 - INFO - Processing question 209/881 (ID: 227) for fanar
2025-07-07 10:15:48,558 - INFO -   Ground truth verses: ['84:20', '57:8']
2025-07-07 10:15:48,558 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:48,757 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:48,758 - WARNING -   Failed to get response for question 227
2025-07-07 10:15:48,758 - INFO - Processing question 210/881 (ID: 228) for fanar
2025-07-07 10:15:48,758 - INFO -   Ground truth verses: ['84:20', '57:8']
2025-07-07 10:15:48,758 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:48,958 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:48,958 - WARNING -   Failed to get response for question 228
2025-07-07 10:15:48,967 - INFO - Saved data to results/fanar_checkpoint_209.json
2025-07-07 10:15:48,967 - INFO - Checkpoint saved for fanar at question 210
2025-07-07 10:15:48,967 - INFO -   Progress: 210/881
2025-07-07 10:15:48,967 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:48,967 - INFO - Processing question 211/881 (ID: 229) for fanar
2025-07-07 10:15:48,967 - INFO -   Ground truth verses: ['9:123']
2025-07-07 10:15:48,967 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:49,172 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:49,173 - WARNING -   Failed to get response for question 229
2025-07-07 10:15:49,173 - INFO - Processing question 212/881 (ID: 230) for fanar
2025-07-07 10:15:49,173 - INFO -   Ground truth verses: ['9:123']
2025-07-07 10:15:49,173 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:49,374 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:49,374 - WARNING -   Failed to get response for question 230
2025-07-07 10:15:49,374 - INFO - Processing question 213/881 (ID: 231) for fanar
2025-07-07 10:15:49,375 - INFO -   Ground truth verses: ['9:123']
2025-07-07 10:15:49,375 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:49,715 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:49,716 - WARNING -   Failed to get response for question 231
2025-07-07 10:15:49,716 - INFO - Processing question 214/881 (ID: 232) for fanar
2025-07-07 10:15:49,716 - INFO -   Ground truth verses: ['9:123']
2025-07-07 10:15:49,716 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:49,919 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:49,920 - WARNING -   Failed to get response for question 232
2025-07-07 10:15:49,920 - INFO - Processing question 215/881 (ID: 233) for fanar
2025-07-07 10:15:49,920 - INFO -   Ground truth verses: ['9:123']
2025-07-07 10:15:49,920 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:50,120 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:50,120 - WARNING -   Failed to get response for question 233
2025-07-07 10:15:50,129 - INFO - Saved data to results/fanar_checkpoint_214.json
2025-07-07 10:15:50,130 - INFO - Checkpoint saved for fanar at question 215
2025-07-07 10:15:50,130 - INFO -   Progress: 215/881
2025-07-07 10:15:50,130 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:50,130 - INFO - Processing question 216/881 (ID: 234) for fanar
2025-07-07 10:15:50,130 - INFO -   Ground truth verses: ['42:46', '42:44']
2025-07-07 10:15:50,130 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:50,335 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:50,336 - WARNING -   Failed to get response for question 234
2025-07-07 10:15:50,336 - INFO - Processing question 217/881 (ID: 235) for fanar
2025-07-07 10:15:50,336 - INFO -   Ground truth verses: ['42:46', '42:44']
2025-07-07 10:15:50,336 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:50,537 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:50,538 - WARNING -   Failed to get response for question 235
2025-07-07 10:15:50,538 - INFO - Processing question 218/881 (ID: 236) for fanar
2025-07-07 10:15:50,538 - INFO -   Ground truth verses: ['42:46', '42:44']
2025-07-07 10:15:50,538 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:50,741 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:50,741 - WARNING -   Failed to get response for question 236
2025-07-07 10:15:50,741 - INFO - Processing question 219/881 (ID: 237) for fanar
2025-07-07 10:15:50,741 - INFO -   Ground truth verses: ['42:46', '42:44']
2025-07-07 10:15:50,741 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:50,941 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:50,942 - WARNING -   Failed to get response for question 237
2025-07-07 10:15:50,942 - INFO - Processing question 220/881 (ID: 238) for fanar
2025-07-07 10:15:50,942 - INFO -   Ground truth verses: ['42:46', '42:44']
2025-07-07 10:15:50,942 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:51,277 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:51,278 - WARNING -   Failed to get response for question 238
2025-07-07 10:15:51,287 - INFO - Saved data to results/fanar_checkpoint_219.json
2025-07-07 10:15:51,287 - INFO - Checkpoint saved for fanar at question 220
2025-07-07 10:15:51,287 - INFO -   Progress: 220/881
2025-07-07 10:15:51,287 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:51,287 - INFO - Processing question 221/881 (ID: 239) for fanar
2025-07-07 10:15:51,287 - INFO -   Ground truth verses: ['100:11']
2025-07-07 10:15:51,287 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:51,490 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:51,491 - WARNING -   Failed to get response for question 239
2025-07-07 10:15:51,491 - INFO - Processing question 222/881 (ID: 240) for fanar
2025-07-07 10:15:51,491 - INFO -   Ground truth verses: ['100:11']
2025-07-07 10:15:51,491 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:51,695 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:51,695 - WARNING -   Failed to get response for question 240
2025-07-07 10:15:51,695 - INFO - Processing question 223/881 (ID: 241) for fanar
2025-07-07 10:15:51,695 - INFO -   Ground truth verses: ['100:11']
2025-07-07 10:15:51,696 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:51,894 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:51,894 - WARNING -   Failed to get response for question 241
2025-07-07 10:15:51,894 - INFO - Processing question 224/881 (ID: 242) for fanar
2025-07-07 10:15:51,894 - INFO -   Ground truth verses: ['100:11']
2025-07-07 10:15:51,894 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:52,095 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:52,096 - WARNING -   Failed to get response for question 242
2025-07-07 10:15:52,096 - INFO - Processing question 225/881 (ID: 243) for fanar
2025-07-07 10:15:52,096 - INFO -   Ground truth verses: ['100:11']
2025-07-07 10:15:52,096 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:52,297 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:52,297 - WARNING -   Failed to get response for question 243
2025-07-07 10:15:52,307 - INFO - Saved data to results/fanar_checkpoint_224.json
2025-07-07 10:15:52,307 - INFO - Checkpoint saved for fanar at question 225
2025-07-07 10:15:52,307 - INFO -   Progress: 225/881
2025-07-07 10:15:52,307 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:52,307 - INFO - Processing question 226/881 (ID: 244) for fanar
2025-07-07 10:15:52,307 - INFO -   Ground truth verses: ['7:82', '29:24', '29:29', '27:56']
2025-07-07 10:15:52,307 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:52,501 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:52,502 - WARNING -   Failed to get response for question 244
2025-07-07 10:15:52,502 - INFO - Processing question 227/881 (ID: 245) for fanar
2025-07-07 10:15:52,502 - INFO -   Ground truth verses: ['7:82', '29:24', '29:29', '27:56']
2025-07-07 10:15:52,502 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:52,840 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:52,841 - WARNING -   Failed to get response for question 245
2025-07-07 10:15:52,841 - INFO - Processing question 228/881 (ID: 246) for fanar
2025-07-07 10:15:52,841 - INFO -   Ground truth verses: ['56:53', '37:66']
2025-07-07 10:15:52,841 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:53,049 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:53,050 - WARNING -   Failed to get response for question 246
2025-07-07 10:15:53,050 - INFO - Processing question 229/881 (ID: 247) for fanar
2025-07-07 10:15:53,050 - INFO -   Ground truth verses: ['56:53', '37:66']
2025-07-07 10:15:53,050 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:53,253 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:53,253 - WARNING -   Failed to get response for question 247
2025-07-07 10:15:53,253 - INFO - Processing question 230/881 (ID: 249) for fanar
2025-07-07 10:15:53,253 - INFO -   Ground truth verses: ['56:53', '37:66']
2025-07-07 10:15:53,253 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:53,455 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:53,455 - WARNING -   Failed to get response for question 249
2025-07-07 10:15:53,465 - INFO - Saved data to results/fanar_checkpoint_229.json
2025-07-07 10:15:53,465 - INFO - Checkpoint saved for fanar at question 230
2025-07-07 10:15:53,465 - INFO -   Progress: 230/881
2025-07-07 10:15:53,465 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:53,465 - INFO - Processing question 231/881 (ID: 250) for fanar
2025-07-07 10:15:53,465 - INFO -   Ground truth verses: ['56:53', '37:66']
2025-07-07 10:15:53,465 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:53,678 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:53,678 - WARNING -   Failed to get response for question 250
2025-07-07 10:15:53,678 - INFO - Processing question 232/881 (ID: 256) for fanar
2025-07-07 10:15:53,678 - INFO -   Ground truth verses: ['17:30', '34:39', '39:52', '30:37', '42:12', '13:26', '29:62', '30:6', '40:57', '34:28', '34:36']
2025-07-07 10:15:53,678 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:53,880 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:53,880 - WARNING -   Failed to get response for question 256
2025-07-07 10:15:53,880 - INFO - Processing question 233/881 (ID: 257) for fanar
2025-07-07 10:15:53,880 - INFO -   Ground truth verses: ['17:30', '34:39', '39:52', '30:37', '42:12', '13:26', '29:62', '30:6', '40:57', '34:28', '34:36']
2025-07-07 10:15:53,880 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:54,080 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:54,080 - WARNING -   Failed to get response for question 257
2025-07-07 10:15:54,080 - INFO - Processing question 234/881 (ID: 258) for fanar
2025-07-07 10:15:54,080 - INFO -   Ground truth verses: ['17:30', '34:39', '39:52', '30:37', '42:12', '13:26', '29:62', '30:6', '40:57', '34:28', '34:36']
2025-07-07 10:15:54,080 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:54,417 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:54,417 - WARNING -   Failed to get response for question 258
2025-07-07 10:15:54,417 - INFO - Processing question 235/881 (ID: 259) for fanar
2025-07-07 10:15:54,417 - INFO -   Ground truth verses: ['17:30', '34:39', '39:52', '30:37', '42:12', '13:26', '29:62', '30:6', '40:57', '34:28', '34:36']
2025-07-07 10:15:54,417 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:54,621 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:54,621 - WARNING -   Failed to get response for question 259
2025-07-07 10:15:54,632 - INFO - Saved data to results/fanar_checkpoint_234.json
2025-07-07 10:15:54,632 - INFO - Checkpoint saved for fanar at question 235
2025-07-07 10:15:54,632 - INFO -   Progress: 235/881
2025-07-07 10:15:54,632 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:54,632 - INFO - Processing question 236/881 (ID: 260) for fanar
2025-07-07 10:15:54,632 - INFO -   Ground truth verses: ['17:30', '34:39', '39:52', '30:37', '42:12', '13:26', '29:62', '30:6', '40:57', '34:28', '34:36']
2025-07-07 10:15:54,632 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:54,837 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:54,838 - WARNING -   Failed to get response for question 260
2025-07-07 10:15:54,838 - INFO - Processing question 237/881 (ID: 261) for fanar
2025-07-07 10:15:54,838 - INFO -   Ground truth verses: ['19:3', '26:10']
2025-07-07 10:15:54,838 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:55,040 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:55,041 - WARNING -   Failed to get response for question 261
2025-07-07 10:15:55,041 - INFO - Processing question 238/881 (ID: 262) for fanar
2025-07-07 10:15:55,041 - INFO -   Ground truth verses: ['19:3', '26:10']
2025-07-07 10:15:55,041 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:55,246 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:55,247 - WARNING -   Failed to get response for question 262
2025-07-07 10:15:55,247 - INFO - Processing question 239/881 (ID: 263) for fanar
2025-07-07 10:15:55,247 - INFO -   Ground truth verses: ['19:3', '26:10']
2025-07-07 10:15:55,247 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:55,448 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:55,449 - WARNING -   Failed to get response for question 263
2025-07-07 10:15:55,449 - INFO - Processing question 240/881 (ID: 264) for fanar
2025-07-07 10:15:55,449 - INFO -   Ground truth verses: ['19:3', '26:10']
2025-07-07 10:15:55,449 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:55,653 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:55,654 - WARNING -   Failed to get response for question 264
2025-07-07 10:15:55,664 - INFO - Saved data to results/fanar_checkpoint_239.json
2025-07-07 10:15:55,664 - INFO - Checkpoint saved for fanar at question 240
2025-07-07 10:15:55,664 - INFO -   Progress: 240/881
2025-07-07 10:15:55,664 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:55,664 - INFO - Processing question 241/881 (ID: 265) for fanar
2025-07-07 10:15:55,664 - INFO -   Ground truth verses: ['19:3', '26:10']
2025-07-07 10:15:55,664 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:56,008 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:56,009 - WARNING -   Failed to get response for question 265
2025-07-07 10:15:56,009 - INFO - Processing question 242/881 (ID: 266) for fanar
2025-07-07 10:15:56,009 - INFO -   Ground truth verses: ['98:8', '9:100', '9:89', '4:13', '64:9', '4:122', '20:76', '5:85', '9:72', '4:57', '85:11', '48:5', '57:12', '3:136', '58:22', '61:12', '14:23', '3:198', '3:15', '5:119']
2025-07-07 10:15:56,009 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:56,041 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 633), falling back to regex parsing
2025-07-07 10:15:56,041 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 633), falling back to regex parsing
2025-07-07 10:15:56,041 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 633), falling back to regex parsing
2025-07-07 10:15:56,041 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:15:56,041 - INFO - Normalized predicted verses: []
2025-07-07 10:15:56,041 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 10:15:56,041 - INFO -   Response: 3223 chars, Found 0 verses
2025-07-07 10:15:56,041 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:56,041 - INFO -   Matched verses: []
2025-07-07 10:15:56,185 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:56,186 - WARNING -   Failed to get response for question 266
2025-07-07 10:15:56,186 - INFO - Processing question 243/881 (ID: 267) for fanar
2025-07-07 10:15:56,186 - INFO -   Ground truth verses: ['98:8', '9:100', '9:89', '4:13', '64:9', '4:122', '20:76', '5:85', '9:72', '4:57', '85:11', '48:5', '57:12', '3:136', '58:22', '61:12', '14:23', '3:198', '3:15', '5:119']
2025-07-07 10:15:56,186 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:56,347 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:56,348 - WARNING -   Failed to get response for question 267
2025-07-07 10:15:56,348 - INFO - Processing question 244/881 (ID: 268) for fanar
2025-07-07 10:15:56,348 - INFO -   Ground truth verses: ['98:8', '9:100', '9:89', '4:13', '64:9', '4:122', '20:76', '5:85', '9:72', '4:57', '85:11', '48:5', '57:12', '3:136', '58:22', '61:12', '14:23', '3:198', '3:15', '5:119']
2025-07-07 10:15:56,348 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:56,509 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:56,510 - WARNING -   Failed to get response for question 268
2025-07-07 10:15:56,510 - INFO - Processing question 245/881 (ID: 269) for fanar
2025-07-07 10:15:56,510 - INFO -   Ground truth verses: ['98:8', '9:100', '9:89', '4:13', '64:9', '4:122', '20:76', '5:85', '9:72', '4:57', '85:11', '48:5', '57:12', '3:136', '58:22', '61:12', '14:23', '3:198', '3:15', '5:119']
2025-07-07 10:15:56,510 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:56,674 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:56,675 - WARNING -   Failed to get response for question 269
2025-07-07 10:15:56,685 - INFO - Saved data to results/fanar_checkpoint_244.json
2025-07-07 10:15:56,685 - INFO - Checkpoint saved for fanar at question 245
2025-07-07 10:15:56,685 - INFO -   Progress: 245/881
2025-07-07 10:15:56,685 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:56,685 - INFO - Processing question 246/881 (ID: 270) for fanar
2025-07-07 10:15:56,685 - INFO -   Ground truth verses: ['98:8', '9:100', '9:89', '4:13', '64:9', '4:122', '20:76', '5:85', '9:72', '4:57', '85:11', '48:5', '57:12', '3:136', '58:22', '61:12', '14:23', '3:198', '3:15', '5:119']
2025-07-07 10:15:56,685 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:56,846 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:56,846 - WARNING -   Failed to get response for question 270
2025-07-07 10:15:56,846 - INFO - Processing question 247/881 (ID: 271) for fanar
2025-07-07 10:15:56,846 - INFO -   Ground truth verses: ['2:224']
2025-07-07 10:15:56,846 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:57,006 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:57,007 - WARNING -   Failed to get response for question 271
2025-07-07 10:15:57,007 - INFO - Processing question 248/881 (ID: 272) for fanar
2025-07-07 10:15:57,007 - INFO -   Ground truth verses: ['2:224']
2025-07-07 10:15:57,007 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:57,042 - INFO - Processing question 8/881 (ID: 7) for deepseek
2025-07-07 10:15:57,042 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:15:57,042 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:15:57,177 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:57,177 - WARNING -   Failed to get response for question 272
2025-07-07 10:15:57,177 - INFO - Processing question 249/881 (ID: 273) for fanar
2025-07-07 10:15:57,177 - INFO -   Ground truth verses: ['2:224']
2025-07-07 10:15:57,177 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:57,505 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:57,505 - WARNING -   Failed to get response for question 273
2025-07-07 10:15:57,505 - INFO - Processing question 250/881 (ID: 274) for fanar
2025-07-07 10:15:57,505 - INFO -   Ground truth verses: ['2:224']
2025-07-07 10:15:57,505 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:57,713 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:57,713 - WARNING -   Failed to get response for question 274
2025-07-07 10:15:57,723 - INFO - Saved data to results/fanar_checkpoint_249.json
2025-07-07 10:15:57,723 - INFO - Checkpoint saved for fanar at question 250
2025-07-07 10:15:57,723 - INFO -   Progress: 250/881
2025-07-07 10:15:57,723 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:57,723 - INFO - Processing question 251/881 (ID: 275) for fanar
2025-07-07 10:15:57,724 - INFO -   Ground truth verses: ['2:224']
2025-07-07 10:15:57,724 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:57,926 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:57,926 - WARNING -   Failed to get response for question 275
2025-07-07 10:15:57,926 - INFO - Processing question 252/881 (ID: 276) for fanar
2025-07-07 10:15:57,926 - INFO -   Ground truth verses: ['45:21', '29:4']
2025-07-07 10:15:57,926 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:58,123 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:58,124 - WARNING -   Failed to get response for question 276
2025-07-07 10:15:58,124 - INFO - Processing question 253/881 (ID: 277) for fanar
2025-07-07 10:15:58,124 - INFO -   Ground truth verses: ['45:21', '29:4']
2025-07-07 10:15:58,124 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:58,321 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:58,321 - WARNING -   Failed to get response for question 277
2025-07-07 10:15:58,321 - INFO - Processing question 254/881 (ID: 278) for fanar
2025-07-07 10:15:58,321 - INFO -   Ground truth verses: ['45:21', '29:4']
2025-07-07 10:15:58,321 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:58,522 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:58,522 - WARNING -   Failed to get response for question 278
2025-07-07 10:15:58,522 - INFO - Processing question 255/881 (ID: 279) for fanar
2025-07-07 10:15:58,522 - INFO -   Ground truth verses: ['45:21', '29:4']
2025-07-07 10:15:58,522 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:58,860 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:58,861 - WARNING -   Failed to get response for question 279
2025-07-07 10:15:58,871 - INFO - Saved data to results/fanar_checkpoint_254.json
2025-07-07 10:15:58,871 - INFO - Checkpoint saved for fanar at question 255
2025-07-07 10:15:58,871 - INFO -   Progress: 255/881
2025-07-07 10:15:58,871 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:58,871 - INFO - Processing question 256/881 (ID: 280) for fanar
2025-07-07 10:15:58,871 - INFO -   Ground truth verses: ['45:21', '29:4']
2025-07-07 10:15:58,871 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:59,077 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:59,077 - WARNING -   Failed to get response for question 280
2025-07-07 10:15:59,077 - INFO - Processing question 257/881 (ID: 281) for fanar
2025-07-07 10:15:59,077 - INFO -   Ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:15:59,077 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:59,281 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:59,281 - WARNING -   Failed to get response for question 281
2025-07-07 10:15:59,281 - INFO - Processing question 258/881 (ID: 282) for fanar
2025-07-07 10:15:59,281 - INFO -   Ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:15:59,281 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:59,489 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:59,490 - WARNING -   Failed to get response for question 282
2025-07-07 10:15:59,490 - INFO - Processing question 259/881 (ID: 283) for fanar
2025-07-07 10:15:59,490 - INFO -   Ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:15:59,490 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:59,696 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:59,697 - WARNING -   Failed to get response for question 283
2025-07-07 10:15:59,697 - INFO - Processing question 260/881 (ID: 284) for fanar
2025-07-07 10:15:59,697 - INFO -   Ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:15:59,697 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:15:59,894 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:15:59,894 - WARNING -   Failed to get response for question 284
2025-07-07 10:15:59,905 - INFO - Saved data to results/fanar_checkpoint_259.json
2025-07-07 10:15:59,905 - INFO - Checkpoint saved for fanar at question 260
2025-07-07 10:15:59,905 - INFO -   Progress: 260/881
2025-07-07 10:15:59,905 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:15:59,905 - INFO - Processing question 261/881 (ID: 285) for fanar
2025-07-07 10:15:59,905 - INFO -   Ground truth verses: ['37:149', '37:153', '52:39']
2025-07-07 10:15:59,905 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:00,107 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:00,108 - WARNING -   Failed to get response for question 285
2025-07-07 10:16:00,108 - INFO - Processing question 262/881 (ID: 291) for fanar
2025-07-07 10:16:00,108 - INFO -   Ground truth verses: ['51:27', '37:91']
2025-07-07 10:16:00,108 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:00,309 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:00,309 - WARNING -   Failed to get response for question 291
2025-07-07 10:16:00,309 - INFO - Processing question 263/881 (ID: 292) for fanar
2025-07-07 10:16:00,309 - INFO -   Ground truth verses: ['51:27', '37:91']
2025-07-07 10:16:00,309 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:00,503 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 10:16:00,503 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:16:00,503 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:16:00,503 - INFO - Normalized predicted verses: []
2025-07-07 10:16:00,503 - INFO - Normalized ground truth verses: ['41:42']
2025-07-07 10:16:00,503 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 10:16:00,503 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:00,503 - INFO -   Matched verses: []
2025-07-07 10:16:00,675 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:00,675 - WARNING -   Failed to get response for question 292
2025-07-07 10:16:00,675 - INFO - Processing question 264/881 (ID: 293) for fanar
2025-07-07 10:16:00,675 - INFO -   Ground truth verses: ['51:27', '37:91']
2025-07-07 10:16:00,675 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:00,844 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:00,845 - WARNING -   Failed to get response for question 293
2025-07-07 10:16:00,845 - INFO - Processing question 265/881 (ID: 294) for fanar
2025-07-07 10:16:00,845 - INFO -   Ground truth verses: ['51:27', '37:91']
2025-07-07 10:16:00,845 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:01,012 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:01,012 - WARNING -   Failed to get response for question 294
2025-07-07 10:16:01,023 - INFO - Saved data to results/fanar_checkpoint_264.json
2025-07-07 10:16:01,023 - INFO - Checkpoint saved for fanar at question 265
2025-07-07 10:16:01,023 - INFO -   Progress: 265/881
2025-07-07 10:16:01,023 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:01,023 - INFO - Processing question 266/881 (ID: 295) for fanar
2025-07-07 10:16:01,023 - INFO -   Ground truth verses: ['51:27', '37:91']
2025-07-07 10:16:01,023 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:01,189 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:01,189 - WARNING -   Failed to get response for question 295
2025-07-07 10:16:01,189 - INFO - Processing question 267/881 (ID: 302) for fanar
2025-07-07 10:16:01,189 - INFO -   Ground truth verses: ['81:22']
2025-07-07 10:16:01,189 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:01,357 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:01,357 - WARNING -   Failed to get response for question 302
2025-07-07 10:16:01,357 - INFO - Processing question 268/881 (ID: 304) for fanar
2025-07-07 10:16:01,358 - INFO -   Ground truth verses: ['81:22']
2025-07-07 10:16:01,358 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:01,504 - INFO - Processing question 298/881 (ID: 335) for qwen
2025-07-07 10:16:01,505 - INFO -   Ground truth verses: ['41:42']
2025-07-07 10:16:01,505 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:16:01,522 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:01,522 - WARNING -   Failed to get response for question 304
2025-07-07 10:16:01,523 - INFO - Processing question 269/881 (ID: 305) for fanar
2025-07-07 10:16:01,523 - INFO -   Ground truth verses: ['81:22']
2025-07-07 10:16:01,523 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:01,724 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:01,724 - WARNING -   Failed to get response for question 305
2025-07-07 10:16:01,724 - INFO - Processing question 270/881 (ID: 306) for fanar
2025-07-07 10:16:01,724 - INFO -   Ground truth verses: ['19:36', '3:51', '43:64', '36:4', '15:41', '23:73', '43:61', '37:118', '4:68', '1:6', '36:61']
2025-07-07 10:16:01,725 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:02,056 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:02,057 - WARNING -   Failed to get response for question 306
2025-07-07 10:16:02,067 - INFO - Saved data to results/fanar_checkpoint_269.json
2025-07-07 10:16:02,067 - INFO - Checkpoint saved for fanar at question 270
2025-07-07 10:16:02,068 - INFO -   Progress: 270/881
2025-07-07 10:16:02,068 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:02,068 - INFO - Processing question 271/881 (ID: 307) for fanar
2025-07-07 10:16:02,068 - INFO -   Ground truth verses: ['19:36', '3:51', '43:64', '36:4', '15:41', '23:73', '43:61', '37:118', '4:68', '1:6', '36:61']
2025-07-07 10:16:02,068 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:02,275 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:02,275 - WARNING -   Failed to get response for question 307
2025-07-07 10:16:02,275 - INFO - Processing question 272/881 (ID: 308) for fanar
2025-07-07 10:16:02,275 - INFO -   Ground truth verses: ['19:36', '3:51', '43:64', '36:4', '15:41', '23:73', '43:61', '37:118', '4:68', '1:6', '36:61']
2025-07-07 10:16:02,275 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:02,473 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:02,474 - WARNING -   Failed to get response for question 308
2025-07-07 10:16:02,474 - INFO - Processing question 273/881 (ID: 309) for fanar
2025-07-07 10:16:02,474 - INFO -   Ground truth verses: ['19:36', '3:51', '43:64', '36:4', '15:41', '23:73', '43:61', '37:118', '4:68', '1:6', '36:61']
2025-07-07 10:16:02,474 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:02,678 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:02,679 - WARNING -   Failed to get response for question 309
2025-07-07 10:16:02,679 - INFO - Processing question 274/881 (ID: 310) for fanar
2025-07-07 10:16:02,679 - INFO -   Ground truth verses: ['19:36', '3:51', '43:64', '36:4', '15:41', '23:73', '43:61', '37:118', '4:68', '1:6', '36:61']
2025-07-07 10:16:02,679 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:02,890 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:02,890 - WARNING -   Failed to get response for question 310
2025-07-07 10:16:02,890 - INFO - Processing question 275/881 (ID: 311) for fanar
2025-07-07 10:16:02,890 - INFO -   Ground truth verses: ['46:16']
2025-07-07 10:16:02,890 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:03,087 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:03,088 - WARNING -   Failed to get response for question 311
2025-07-07 10:16:03,100 - INFO - Saved data to results/fanar_checkpoint_274.json
2025-07-07 10:16:03,100 - INFO - Checkpoint saved for fanar at question 275
2025-07-07 10:16:03,100 - INFO -   Progress: 275/881
2025-07-07 10:16:03,100 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:03,100 - INFO - Processing question 276/881 (ID: 312) for fanar
2025-07-07 10:16:03,100 - INFO -   Ground truth verses: ['46:16']
2025-07-07 10:16:03,100 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:03,303 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:03,304 - WARNING -   Failed to get response for question 312
2025-07-07 10:16:03,304 - INFO - Processing question 277/881 (ID: 313) for fanar
2025-07-07 10:16:03,304 - INFO -   Ground truth verses: ['46:16']
2025-07-07 10:16:03,304 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:03,647 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:03,648 - WARNING -   Failed to get response for question 313
2025-07-07 10:16:03,648 - INFO - Processing question 278/881 (ID: 314) for fanar
2025-07-07 10:16:03,648 - INFO -   Ground truth verses: ['46:16']
2025-07-07 10:16:03,648 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:03,854 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:03,854 - WARNING -   Failed to get response for question 314
2025-07-07 10:16:03,854 - INFO - Processing question 279/881 (ID: 315) for fanar
2025-07-07 10:16:03,854 - INFO -   Ground truth verses: ['46:16']
2025-07-07 10:16:03,854 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:04,050 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:04,050 - WARNING -   Failed to get response for question 315
2025-07-07 10:16:04,050 - INFO - Processing question 280/881 (ID: 316) for fanar
2025-07-07 10:16:04,050 - INFO -   Ground truth verses: ['34:54']
2025-07-07 10:16:04,050 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:04,248 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:04,248 - WARNING -   Failed to get response for question 316
2025-07-07 10:16:04,259 - INFO - Saved data to results/fanar_checkpoint_279.json
2025-07-07 10:16:04,259 - INFO - Checkpoint saved for fanar at question 280
2025-07-07 10:16:04,259 - INFO -   Progress: 280/881
2025-07-07 10:16:04,259 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:04,259 - INFO - Processing question 281/881 (ID: 318) for fanar
2025-07-07 10:16:04,259 - INFO -   Ground truth verses: ['34:54']
2025-07-07 10:16:04,259 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:04,461 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:04,462 - WARNING -   Failed to get response for question 318
2025-07-07 10:16:04,462 - INFO - Processing question 282/881 (ID: 319) for fanar
2025-07-07 10:16:04,462 - INFO -   Ground truth verses: ['34:54']
2025-07-07 10:16:04,462 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:04,663 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:04,664 - WARNING -   Failed to get response for question 319
2025-07-07 10:16:04,664 - INFO - Processing question 283/881 (ID: 320) for fanar
2025-07-07 10:16:04,664 - INFO -   Ground truth verses: ['34:54']
2025-07-07 10:16:04,664 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:04,861 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:04,861 - WARNING -   Failed to get response for question 320
2025-07-07 10:16:04,861 - INFO - Processing question 284/881 (ID: 321) for fanar
2025-07-07 10:16:04,862 - INFO -   Ground truth verses: ['20:3', '80:9', '20:44', '79:26', '87:10']
2025-07-07 10:16:04,862 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:05,198 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:05,198 - WARNING -   Failed to get response for question 321
2025-07-07 10:16:05,198 - INFO - Processing question 285/881 (ID: 322) for fanar
2025-07-07 10:16:05,198 - INFO -   Ground truth verses: ['20:3', '80:9', '20:44', '79:26', '87:10']
2025-07-07 10:16:05,198 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:05,401 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:05,401 - WARNING -   Failed to get response for question 322
2025-07-07 10:16:05,412 - INFO - Saved data to results/fanar_checkpoint_284.json
2025-07-07 10:16:05,412 - INFO - Checkpoint saved for fanar at question 285
2025-07-07 10:16:05,412 - INFO -   Progress: 285/881
2025-07-07 10:16:05,412 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:05,412 - INFO - Processing question 286/881 (ID: 323) for fanar
2025-07-07 10:16:05,412 - INFO -   Ground truth verses: ['20:3', '80:9', '20:44', '79:26', '87:10']
2025-07-07 10:16:05,412 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:05,613 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:05,614 - WARNING -   Failed to get response for question 323
2025-07-07 10:16:05,614 - INFO - Processing question 287/881 (ID: 324) for fanar
2025-07-07 10:16:05,614 - INFO -   Ground truth verses: ['20:3', '80:9', '20:44', '79:26', '87:10']
2025-07-07 10:16:05,614 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:05,813 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:05,814 - WARNING -   Failed to get response for question 324
2025-07-07 10:16:05,814 - INFO - Processing question 288/881 (ID: 325) for fanar
2025-07-07 10:16:05,814 - INFO -   Ground truth verses: ['20:3', '80:9', '20:44', '79:26', '87:10']
2025-07-07 10:16:05,814 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:06,011 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:06,012 - WARNING -   Failed to get response for question 325
2025-07-07 10:16:06,012 - INFO - Processing question 289/881 (ID: 326) for fanar
2025-07-07 10:16:06,012 - INFO -   Ground truth verses: ['34:5', '34:38', '5:86', '5:10', '22:51']
2025-07-07 10:16:06,012 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:06,210 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:06,210 - WARNING -   Failed to get response for question 326
2025-07-07 10:16:06,210 - INFO - Processing question 290/881 (ID: 327) for fanar
2025-07-07 10:16:06,210 - INFO -   Ground truth verses: ['34:5', '34:38', '5:86', '5:10', '22:51']
2025-07-07 10:16:06,210 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:06,409 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:06,409 - WARNING -   Failed to get response for question 327
2025-07-07 10:16:06,421 - INFO - Saved data to results/fanar_checkpoint_289.json
2025-07-07 10:16:06,421 - INFO - Checkpoint saved for fanar at question 290
2025-07-07 10:16:06,421 - INFO -   Progress: 290/881
2025-07-07 10:16:06,421 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:06,421 - INFO - Processing question 291/881 (ID: 328) for fanar
2025-07-07 10:16:06,421 - INFO -   Ground truth verses: ['34:5', '34:38', '5:86', '5:10', '22:51']
2025-07-07 10:16:06,421 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:06,763 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:06,764 - WARNING -   Failed to get response for question 328
2025-07-07 10:16:06,764 - INFO - Processing question 292/881 (ID: 329) for fanar
2025-07-07 10:16:06,764 - INFO -   Ground truth verses: ['34:5', '34:38', '5:86', '5:10', '22:51']
2025-07-07 10:16:06,764 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:06,964 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:06,965 - WARNING -   Failed to get response for question 329
2025-07-07 10:16:06,965 - INFO - Processing question 293/881 (ID: 330) for fanar
2025-07-07 10:16:06,965 - INFO -   Ground truth verses: ['34:5', '34:38', '5:86', '5:10', '22:51']
2025-07-07 10:16:06,965 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:07,167 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:07,167 - WARNING -   Failed to get response for question 330
2025-07-07 10:16:07,167 - INFO - Processing question 294/881 (ID: 331) for fanar
2025-07-07 10:16:07,167 - INFO -   Ground truth verses: ['41:42']
2025-07-07 10:16:07,167 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:07,363 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:07,363 - WARNING -   Failed to get response for question 331
2025-07-07 10:16:07,364 - INFO - Processing question 295/881 (ID: 332) for fanar
2025-07-07 10:16:07,364 - INFO -   Ground truth verses: ['41:42']
2025-07-07 10:16:07,364 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:07,562 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:07,563 - WARNING -   Failed to get response for question 332
2025-07-07 10:16:07,574 - INFO - Saved data to results/fanar_checkpoint_294.json
2025-07-07 10:16:07,574 - INFO - Checkpoint saved for fanar at question 295
2025-07-07 10:16:07,574 - INFO -   Progress: 295/881
2025-07-07 10:16:07,574 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:07,574 - INFO - Processing question 296/881 (ID: 333) for fanar
2025-07-07 10:16:07,574 - INFO -   Ground truth verses: ['41:42']
2025-07-07 10:16:07,575 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:07,777 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:07,778 - WARNING -   Failed to get response for question 333
2025-07-07 10:16:07,778 - INFO - Processing question 297/881 (ID: 334) for fanar
2025-07-07 10:16:07,778 - INFO -   Ground truth verses: ['41:42']
2025-07-07 10:16:07,778 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:07,977 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:07,978 - WARNING -   Failed to get response for question 334
2025-07-07 10:16:07,978 - INFO - Processing question 298/881 (ID: 335) for fanar
2025-07-07 10:16:07,978 - INFO -   Ground truth verses: ['41:42']
2025-07-07 10:16:07,978 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:08,317 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:08,317 - WARNING -   Failed to get response for question 335
2025-07-07 10:16:08,317 - INFO - Processing question 299/881 (ID: 337) for fanar
2025-07-07 10:16:08,317 - INFO -   Ground truth verses: ['37:54']
2025-07-07 10:16:08,317 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:08,518 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:08,519 - WARNING -   Failed to get response for question 337
2025-07-07 10:16:08,519 - INFO - Processing question 300/881 (ID: 338) for fanar
2025-07-07 10:16:08,519 - INFO -   Ground truth verses: ['37:54']
2025-07-07 10:16:08,519 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:08,723 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:08,723 - WARNING -   Failed to get response for question 338
2025-07-07 10:16:08,735 - INFO - Saved data to results/fanar_checkpoint_299.json
2025-07-07 10:16:08,735 - INFO - Checkpoint saved for fanar at question 300
2025-07-07 10:16:08,735 - INFO -   Progress: 300/881
2025-07-07 10:16:08,735 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:08,735 - INFO - Processing question 301/881 (ID: 341) for fanar
2025-07-07 10:16:08,735 - INFO -   Ground truth verses: ['6:5', '43:7', '15:11', '16:34', '26:6']
2025-07-07 10:16:08,735 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:08,935 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:08,936 - WARNING -   Failed to get response for question 341
2025-07-07 10:16:08,936 - INFO - Processing question 302/881 (ID: 342) for fanar
2025-07-07 10:16:08,936 - INFO -   Ground truth verses: ['6:5', '43:7', '15:11', '16:34', '26:6']
2025-07-07 10:16:08,936 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:09,134 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:09,135 - WARNING -   Failed to get response for question 342
2025-07-07 10:16:09,135 - INFO - Processing question 303/881 (ID: 343) for fanar
2025-07-07 10:16:09,135 - INFO -   Ground truth verses: ['6:5', '43:7', '15:11', '16:34', '26:6']
2025-07-07 10:16:09,135 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:09,333 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:09,334 - WARNING -   Failed to get response for question 343
2025-07-07 10:16:09,334 - INFO - Processing question 304/881 (ID: 344) for fanar
2025-07-07 10:16:09,334 - INFO -   Ground truth verses: ['6:5', '43:7', '15:11', '16:34', '26:6']
2025-07-07 10:16:09,334 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:09,536 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:09,536 - WARNING -   Failed to get response for question 344
2025-07-07 10:16:09,536 - INFO - Processing question 305/881 (ID: 345) for fanar
2025-07-07 10:16:09,536 - INFO -   Ground truth verses: ['6:5', '43:7', '15:11', '16:34', '26:6']
2025-07-07 10:16:09,537 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:09,884 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:09,885 - WARNING -   Failed to get response for question 345
2025-07-07 10:16:09,897 - INFO - Saved data to results/fanar_checkpoint_304.json
2025-07-07 10:16:09,897 - INFO - Checkpoint saved for fanar at question 305
2025-07-07 10:16:09,897 - INFO -   Progress: 305/881
2025-07-07 10:16:09,897 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:09,897 - INFO - Processing question 306/881 (ID: 346) for fanar
2025-07-07 10:16:09,897 - INFO -   Ground truth verses: ['69:49']
2025-07-07 10:16:09,897 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:10,096 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:10,096 - WARNING -   Failed to get response for question 346
2025-07-07 10:16:10,097 - INFO - Processing question 307/881 (ID: 347) for fanar
2025-07-07 10:16:10,097 - INFO -   Ground truth verses: ['69:49']
2025-07-07 10:16:10,097 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:10,295 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:10,296 - WARNING -   Failed to get response for question 347
2025-07-07 10:16:10,296 - INFO - Processing question 308/881 (ID: 349) for fanar
2025-07-07 10:16:10,296 - INFO -   Ground truth verses: ['69:49']
2025-07-07 10:16:10,296 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:10,496 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:10,496 - WARNING -   Failed to get response for question 349
2025-07-07 10:16:10,496 - INFO - Processing question 309/881 (ID: 350) for fanar
2025-07-07 10:16:10,496 - INFO -   Ground truth verses: ['69:49']
2025-07-07 10:16:10,496 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:10,699 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:10,699 - WARNING -   Failed to get response for question 350
2025-07-07 10:16:10,699 - INFO - Processing question 310/881 (ID: 351) for fanar
2025-07-07 10:16:10,699 - INFO -   Ground truth verses: ['42:13']
2025-07-07 10:16:10,699 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:10,901 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:10,902 - WARNING -   Failed to get response for question 351
2025-07-07 10:16:10,913 - INFO - Saved data to results/fanar_checkpoint_309.json
2025-07-07 10:16:10,914 - INFO - Checkpoint saved for fanar at question 310
2025-07-07 10:16:10,914 - INFO -   Progress: 310/881
2025-07-07 10:16:10,914 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:10,914 - INFO - Processing question 311/881 (ID: 352) for fanar
2025-07-07 10:16:10,914 - INFO -   Ground truth verses: ['42:13']
2025-07-07 10:16:10,914 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:11,118 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:11,118 - WARNING -   Failed to get response for question 352
2025-07-07 10:16:11,119 - INFO - Processing question 312/881 (ID: 353) for fanar
2025-07-07 10:16:11,119 - INFO -   Ground truth verses: ['42:13']
2025-07-07 10:16:11,119 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:11,318 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:11,319 - WARNING -   Failed to get response for question 353
2025-07-07 10:16:11,319 - INFO - Processing question 313/881 (ID: 354) for fanar
2025-07-07 10:16:11,319 - INFO -   Ground truth verses: ['42:13']
2025-07-07 10:16:11,319 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:11,663 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:11,663 - WARNING -   Failed to get response for question 354
2025-07-07 10:16:11,663 - INFO - Processing question 314/881 (ID: 355) for fanar
2025-07-07 10:16:11,663 - INFO -   Ground truth verses: ['42:13']
2025-07-07 10:16:11,663 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:11,864 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:11,864 - WARNING -   Failed to get response for question 355
2025-07-07 10:16:11,864 - INFO - Processing question 315/881 (ID: 357) for fanar
2025-07-07 10:16:11,864 - INFO -   Ground truth verses: ['7:185', '45:6', '77:50']
2025-07-07 10:16:11,865 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:12,061 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:12,061 - WARNING -   Failed to get response for question 357
2025-07-07 10:16:12,073 - INFO - Saved data to results/fanar_checkpoint_314.json
2025-07-07 10:16:12,073 - INFO - Checkpoint saved for fanar at question 315
2025-07-07 10:16:12,073 - INFO -   Progress: 315/881
2025-07-07 10:16:12,073 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:12,073 - INFO - Processing question 316/881 (ID: 358) for fanar
2025-07-07 10:16:12,073 - INFO -   Ground truth verses: ['7:185', '45:6', '77:50']
2025-07-07 10:16:12,073 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:12,280 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:12,281 - WARNING -   Failed to get response for question 358
2025-07-07 10:16:12,281 - INFO - Processing question 317/881 (ID: 359) for fanar
2025-07-07 10:16:12,281 - INFO -   Ground truth verses: ['7:185', '45:6', '77:50']
2025-07-07 10:16:12,281 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:12,483 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:12,484 - WARNING -   Failed to get response for question 359
2025-07-07 10:16:12,484 - INFO - Processing question 318/881 (ID: 360) for fanar
2025-07-07 10:16:12,484 - INFO -   Ground truth verses: ['7:185', '45:6', '77:50']
2025-07-07 10:16:12,484 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:12,687 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:12,687 - WARNING -   Failed to get response for question 360
2025-07-07 10:16:12,687 - INFO - Processing question 319/881 (ID: 361) for fanar
2025-07-07 10:16:12,687 - INFO -   Ground truth verses: ['6:74']
2025-07-07 10:16:12,687 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:12,886 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:12,887 - WARNING -   Failed to get response for question 361
2025-07-07 10:16:12,887 - INFO - Processing question 320/881 (ID: 362) for fanar
2025-07-07 10:16:12,887 - INFO -   Ground truth verses: ['6:74']
2025-07-07 10:16:12,887 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:13,236 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:13,237 - WARNING -   Failed to get response for question 362
2025-07-07 10:16:13,249 - INFO - Saved data to results/fanar_checkpoint_319.json
2025-07-07 10:16:13,249 - INFO - Checkpoint saved for fanar at question 320
2025-07-07 10:16:13,249 - INFO -   Progress: 320/881
2025-07-07 10:16:13,249 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:13,249 - INFO - Processing question 321/881 (ID: 363) for fanar
2025-07-07 10:16:13,249 - INFO -   Ground truth verses: ['6:74']
2025-07-07 10:16:13,249 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:13,450 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:13,450 - WARNING -   Failed to get response for question 363
2025-07-07 10:16:13,450 - INFO - Processing question 322/881 (ID: 364) for fanar
2025-07-07 10:16:13,450 - INFO -   Ground truth verses: ['6:74']
2025-07-07 10:16:13,450 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:13,657 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:13,657 - WARNING -   Failed to get response for question 364
2025-07-07 10:16:13,657 - INFO - Processing question 323/881 (ID: 365) for fanar
2025-07-07 10:16:13,657 - INFO -   Ground truth verses: ['6:74']
2025-07-07 10:16:13,657 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:13,854 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:13,855 - WARNING -   Failed to get response for question 365
2025-07-07 10:16:13,855 - INFO - Processing question 324/881 (ID: 366) for fanar
2025-07-07 10:16:13,855 - INFO -   Ground truth verses: ['2:190', '2:244', '4:76', '4:74']
2025-07-07 10:16:13,855 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:14,055 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:14,055 - WARNING -   Failed to get response for question 366
2025-07-07 10:16:14,056 - INFO - Processing question 325/881 (ID: 367) for fanar
2025-07-07 10:16:14,056 - INFO -   Ground truth verses: ['2:190', '2:244', '4:76', '4:74']
2025-07-07 10:16:14,056 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:14,261 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:14,261 - WARNING -   Failed to get response for question 367
2025-07-07 10:16:14,273 - INFO - Saved data to results/fanar_checkpoint_324.json
2025-07-07 10:16:14,273 - INFO - Checkpoint saved for fanar at question 325
2025-07-07 10:16:14,273 - INFO -   Progress: 325/881
2025-07-07 10:16:14,273 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:14,273 - INFO - Processing question 326/881 (ID: 368) for fanar
2025-07-07 10:16:14,273 - INFO -   Ground truth verses: ['2:190', '2:244', '4:76', '4:74']
2025-07-07 10:16:14,273 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:14,479 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:14,479 - WARNING -   Failed to get response for question 368
2025-07-07 10:16:14,480 - INFO - Processing question 327/881 (ID: 369) for fanar
2025-07-07 10:16:14,480 - INFO -   Ground truth verses: ['2:190', '2:244', '4:76', '4:74']
2025-07-07 10:16:14,480 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:14,832 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:14,833 - WARNING -   Failed to get response for question 369
2025-07-07 10:16:14,833 - INFO - Processing question 328/881 (ID: 370) for fanar
2025-07-07 10:16:14,833 - INFO -   Ground truth verses: ['2:190', '2:244', '4:76', '4:74']
2025-07-07 10:16:14,833 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:15,031 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:15,032 - WARNING -   Failed to get response for question 370
2025-07-07 10:16:15,032 - INFO - Processing question 329/881 (ID: 371) for fanar
2025-07-07 10:16:15,032 - INFO -   Ground truth verses: ['4:131', '4:132', '61:1', '59:1', '3:109', '64:1', '49:16', '62:1', '3:129', '42:4', '2:284', '22:64', '14:2', '3:29', '42:53', '53:31', '4:170', '20:6', '57:1', '31:26', '24:64', '48:7', '34:1', '16:49', '5:97', '45:13', '58:7', '4:126']
2025-07-07 10:16:15,032 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:15,236 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:15,237 - WARNING -   Failed to get response for question 371
2025-07-07 10:16:15,237 - INFO - Processing question 330/881 (ID: 372) for fanar
2025-07-07 10:16:15,237 - INFO -   Ground truth verses: ['4:131', '4:132', '61:1', '59:1', '3:109', '64:1', '49:16', '62:1', '3:129', '42:4', '2:284', '22:64', '14:2', '3:29', '42:53', '53:31', '4:170', '20:6', '57:1', '31:26', '24:64', '48:7', '34:1', '16:49', '5:97', '45:13', '58:7', '4:126']
2025-07-07 10:16:15,237 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:15,445 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:15,446 - WARNING -   Failed to get response for question 372
2025-07-07 10:16:15,460 - INFO - Saved data to results/fanar_checkpoint_329.json
2025-07-07 10:16:15,460 - INFO - Checkpoint saved for fanar at question 330
2025-07-07 10:16:15,460 - INFO -   Progress: 330/881
2025-07-07 10:16:15,460 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:15,460 - INFO - Processing question 331/881 (ID: 373) for fanar
2025-07-07 10:16:15,460 - INFO -   Ground truth verses: ['4:131', '4:132', '61:1', '59:1', '3:109', '64:1', '49:16', '62:1', '3:129', '42:4', '2:284', '22:64', '14:2', '3:29', '42:53', '53:31', '4:170', '20:6', '57:1', '31:26', '24:64', '48:7', '34:1', '16:49', '5:97', '45:13', '58:7', '4:126']
2025-07-07 10:16:15,460 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:15,667 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:15,667 - WARNING -   Failed to get response for question 373
2025-07-07 10:16:15,667 - INFO - Processing question 332/881 (ID: 374) for fanar
2025-07-07 10:16:15,667 - INFO -   Ground truth verses: ['4:131', '4:132', '61:1', '59:1', '3:109', '64:1', '49:16', '62:1', '3:129', '42:4', '2:284', '22:64', '14:2', '3:29', '42:53', '53:31', '4:170', '20:6', '57:1', '31:26', '24:64', '48:7', '34:1', '16:49', '5:97', '45:13', '58:7', '4:126']
2025-07-07 10:16:15,668 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:15,868 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:15,868 - WARNING -   Failed to get response for question 374
2025-07-07 10:16:15,868 - INFO - Processing question 333/881 (ID: 375) for fanar
2025-07-07 10:16:15,868 - INFO -   Ground truth verses: ['4:131', '4:132', '61:1', '59:1', '3:109', '64:1', '49:16', '62:1', '3:129', '42:4', '2:284', '22:64', '14:2', '3:29', '42:53', '53:31', '4:170', '20:6', '57:1', '31:26', '24:64', '48:7', '34:1', '16:49', '5:97', '45:13', '58:7', '4:126']
2025-07-07 10:16:15,868 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:16,069 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:16,070 - WARNING -   Failed to get response for question 375
2025-07-07 10:16:16,070 - INFO - Processing question 334/881 (ID: 376) for fanar
2025-07-07 10:16:16,070 - INFO -   Ground truth verses: ['77:3']
2025-07-07 10:16:16,070 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:16,271 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:16,271 - WARNING -   Failed to get response for question 376
2025-07-07 10:16:16,272 - INFO - Processing question 335/881 (ID: 377) for fanar
2025-07-07 10:16:16,272 - INFO -   Ground truth verses: ['77:3']
2025-07-07 10:16:16,272 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:16,621 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:16,621 - WARNING -   Failed to get response for question 377
2025-07-07 10:16:16,634 - INFO - Saved data to results/fanar_checkpoint_334.json
2025-07-07 10:16:16,634 - INFO - Checkpoint saved for fanar at question 335
2025-07-07 10:16:16,634 - INFO -   Progress: 335/881
2025-07-07 10:16:16,634 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:16,634 - INFO - Processing question 336/881 (ID: 379) for fanar
2025-07-07 10:16:16,634 - INFO -   Ground truth verses: ['77:3']
2025-07-07 10:16:16,634 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:16,842 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:16,843 - WARNING -   Failed to get response for question 379
2025-07-07 10:16:16,843 - INFO - Processing question 337/881 (ID: 380) for fanar
2025-07-07 10:16:16,843 - INFO -   Ground truth verses: ['77:3']
2025-07-07 10:16:16,843 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:17,047 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:17,047 - WARNING -   Failed to get response for question 380
2025-07-07 10:16:17,047 - INFO - Processing question 338/881 (ID: 382) for fanar
2025-07-07 10:16:17,047 - INFO -   Ground truth verses: ['100:10']
2025-07-07 10:16:17,047 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:17,253 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:17,253 - WARNING -   Failed to get response for question 382
2025-07-07 10:16:17,253 - INFO - Processing question 339/881 (ID: 383) for fanar
2025-07-07 10:16:17,253 - INFO -   Ground truth verses: ['100:10']
2025-07-07 10:16:17,253 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:17,455 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:17,455 - WARNING -   Failed to get response for question 383
2025-07-07 10:16:17,455 - INFO - Processing question 340/881 (ID: 384) for fanar
2025-07-07 10:16:17,455 - INFO -   Ground truth verses: ['100:10']
2025-07-07 10:16:17,456 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:17,661 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:17,662 - WARNING -   Failed to get response for question 384
2025-07-07 10:16:17,676 - INFO - Saved data to results/fanar_checkpoint_339.json
2025-07-07 10:16:17,676 - INFO - Checkpoint saved for fanar at question 340
2025-07-07 10:16:17,676 - INFO -   Progress: 340/881
2025-07-07 10:16:17,676 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:17,676 - INFO - Processing question 341/881 (ID: 385) for fanar
2025-07-07 10:16:17,676 - INFO -   Ground truth verses: ['100:10']
2025-07-07 10:16:17,676 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:17,880 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:17,881 - WARNING -   Failed to get response for question 385
2025-07-07 10:16:17,881 - INFO - Processing question 342/881 (ID: 386) for fanar
2025-07-07 10:16:17,881 - INFO -   Ground truth verses: ['27:63', '7:57', '15:22', '25:48']
2025-07-07 10:16:17,881 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:18,242 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:18,242 - WARNING -   Failed to get response for question 386
2025-07-07 10:16:18,242 - INFO - Processing question 343/881 (ID: 388) for fanar
2025-07-07 10:16:18,242 - INFO -   Ground truth verses: ['27:63', '7:57', '15:22', '25:48']
2025-07-07 10:16:18,243 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:18,433 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 10:16:18,433 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:16:18,433 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:16:18,433 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 10:16:18,433 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:16:18,433 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:16:18,433 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:16:18,433 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:16:18,433 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 10:16:18,433 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 10:16:18,433 - INFO -   Response: 4707 chars, Found 2 verses
2025-07-07 10:16:18,433 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:18,433 - INFO -   Matched verses: []
2025-07-07 10:16:18,441 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:18,441 - WARNING -   Failed to get response for question 388
2025-07-07 10:16:18,441 - INFO - Processing question 344/881 (ID: 389) for fanar
2025-07-07 10:16:18,441 - INFO -   Ground truth verses: ['27:63', '7:57', '15:22', '25:48']
2025-07-07 10:16:18,441 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:18,602 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:18,603 - WARNING -   Failed to get response for question 389
2025-07-07 10:16:18,603 - INFO - Processing question 345/881 (ID: 390) for fanar
2025-07-07 10:16:18,603 - INFO -   Ground truth verses: ['27:63', '7:57', '15:22', '25:48']
2025-07-07 10:16:18,603 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:18,761 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:18,762 - WARNING -   Failed to get response for question 390
2025-07-07 10:16:18,774 - INFO - Saved data to results/fanar_checkpoint_344.json
2025-07-07 10:16:18,774 - INFO - Checkpoint saved for fanar at question 345
2025-07-07 10:16:18,775 - INFO -   Progress: 345/881
2025-07-07 10:16:18,775 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:18,775 - INFO - Processing question 346/881 (ID: 392) for fanar
2025-07-07 10:16:18,775 - INFO -   Ground truth verses: ['70:37', '50:17']
2025-07-07 10:16:18,775 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:18,941 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:18,942 - WARNING -   Failed to get response for question 392
2025-07-07 10:16:18,942 - INFO - Processing question 347/881 (ID: 393) for fanar
2025-07-07 10:16:18,942 - INFO -   Ground truth verses: ['70:37', '50:17']
2025-07-07 10:16:18,942 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:19,107 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:19,107 - WARNING -   Failed to get response for question 393
2025-07-07 10:16:19,107 - INFO - Processing question 348/881 (ID: 394) for fanar
2025-07-07 10:16:19,107 - INFO -   Ground truth verses: ['70:37', '50:17']
2025-07-07 10:16:19,107 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:19,274 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:19,274 - WARNING -   Failed to get response for question 394
2025-07-07 10:16:19,274 - INFO - Processing question 349/881 (ID: 395) for fanar
2025-07-07 10:16:19,274 - INFO -   Ground truth verses: ['70:37', '50:17']
2025-07-07 10:16:19,274 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:19,434 - INFO - Processing question 9/881 (ID: 8) for deepseek
2025-07-07 10:16:19,434 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:16:19,435 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:16:19,584 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:19,584 - WARNING -   Failed to get response for question 395
2025-07-07 10:16:19,585 - INFO - Processing question 350/881 (ID: 396) for fanar
2025-07-07 10:16:19,585 - INFO -   Ground truth verses: ['81:24']
2025-07-07 10:16:19,585 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:19,791 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:19,791 - WARNING -   Failed to get response for question 396
2025-07-07 10:16:19,804 - INFO - Saved data to results/fanar_checkpoint_349.json
2025-07-07 10:16:19,805 - INFO - Checkpoint saved for fanar at question 350
2025-07-07 10:16:19,805 - INFO -   Progress: 350/881
2025-07-07 10:16:19,805 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:19,805 - INFO - Processing question 351/881 (ID: 397) for fanar
2025-07-07 10:16:19,805 - INFO -   Ground truth verses: ['81:24']
2025-07-07 10:16:19,805 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:19,966 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:19,967 - WARNING -   Failed to get response for question 397
2025-07-07 10:16:19,967 - INFO - Processing question 352/881 (ID: 399) for fanar
2025-07-07 10:16:19,967 - INFO -   Ground truth verses: ['81:24']
2025-07-07 10:16:19,967 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:20,135 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:20,136 - WARNING -   Failed to get response for question 399
2025-07-07 10:16:20,136 - INFO - Processing question 353/881 (ID: 400) for fanar
2025-07-07 10:16:20,136 - INFO -   Ground truth verses: ['81:24']
2025-07-07 10:16:20,136 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:20,301 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:20,301 - WARNING -   Failed to get response for question 400
2025-07-07 10:16:20,301 - INFO - Processing question 354/881 (ID: 401) for fanar
2025-07-07 10:16:20,302 - INFO -   Ground truth verses: ['18:27', '72:22']
2025-07-07 10:16:20,302 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:20,470 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:20,471 - WARNING -   Failed to get response for question 401
2025-07-07 10:16:20,471 - INFO - Processing question 355/881 (ID: 402) for fanar
2025-07-07 10:16:20,471 - INFO -   Ground truth verses: ['18:27', '72:22']
2025-07-07 10:16:20,471 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:20,633 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:20,634 - WARNING -   Failed to get response for question 402
2025-07-07 10:16:20,650 - INFO - Saved data to results/fanar_checkpoint_354.json
2025-07-07 10:16:20,650 - INFO - Checkpoint saved for fanar at question 355
2025-07-07 10:16:20,650 - INFO -   Progress: 355/881
2025-07-07 10:16:20,650 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:20,650 - INFO - Processing question 356/881 (ID: 404) for fanar
2025-07-07 10:16:20,650 - INFO -   Ground truth verses: ['18:27', '72:22']
2025-07-07 10:16:20,650 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:20,813 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:20,813 - WARNING -   Failed to get response for question 404
2025-07-07 10:16:20,813 - INFO - Processing question 357/881 (ID: 406) for fanar
2025-07-07 10:16:20,813 - INFO -   Ground truth verses: ['78:34']
2025-07-07 10:16:20,813 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:21,130 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:21,131 - WARNING -   Failed to get response for question 406
2025-07-07 10:16:21,131 - INFO - Processing question 358/881 (ID: 407) for fanar
2025-07-07 10:16:21,131 - INFO -   Ground truth verses: ['78:34']
2025-07-07 10:16:21,131 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:21,293 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:21,293 - WARNING -   Failed to get response for question 407
2025-07-07 10:16:21,293 - INFO - Processing question 359/881 (ID: 408) for fanar
2025-07-07 10:16:21,293 - INFO -   Ground truth verses: ['78:34']
2025-07-07 10:16:21,293 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:21,453 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:21,454 - WARNING -   Failed to get response for question 408
2025-07-07 10:16:21,454 - INFO - Processing question 360/881 (ID: 410) for fanar
2025-07-07 10:16:21,454 - INFO -   Ground truth verses: ['78:34']
2025-07-07 10:16:21,454 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:21,614 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:21,615 - WARNING -   Failed to get response for question 410
2025-07-07 10:16:21,628 - INFO - Saved data to results/fanar_checkpoint_359.json
2025-07-07 10:16:21,628 - INFO - Checkpoint saved for fanar at question 360
2025-07-07 10:16:21,628 - INFO -   Progress: 360/881
2025-07-07 10:16:21,628 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:21,628 - INFO - Processing question 361/881 (ID: 411) for fanar
2025-07-07 10:16:21,628 - INFO -   Ground truth verses: ['61:9', '48:28', '9:33']
2025-07-07 10:16:21,628 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:21,794 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:21,795 - WARNING -   Failed to get response for question 411
2025-07-07 10:16:21,795 - INFO - Processing question 362/881 (ID: 412) for fanar
2025-07-07 10:16:21,795 - INFO -   Ground truth verses: ['61:9', '48:28', '9:33']
2025-07-07 10:16:21,795 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:21,956 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:21,957 - WARNING -   Failed to get response for question 412
2025-07-07 10:16:21,957 - INFO - Processing question 363/881 (ID: 415) for fanar
2025-07-07 10:16:21,957 - INFO -   Ground truth verses: ['61:9', '48:28', '9:33']
2025-07-07 10:16:21,957 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:22,118 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:22,119 - WARNING -   Failed to get response for question 415
2025-07-07 10:16:22,119 - INFO - Processing question 364/881 (ID: 416) for fanar
2025-07-07 10:16:22,119 - INFO -   Ground truth verses: ['99:3', '96:5']
2025-07-07 10:16:22,119 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:22,419 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:22,420 - WARNING -   Failed to get response for question 416
2025-07-07 10:16:22,420 - INFO - Processing question 365/881 (ID: 417) for fanar
2025-07-07 10:16:22,420 - INFO -   Ground truth verses: ['99:3', '96:5']
2025-07-07 10:16:22,420 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:22,461 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 873), falling back to regex parsing
2025-07-07 10:16:22,461 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 873), falling back to regex parsing
2025-07-07 10:16:22,461 - WARNING - Failed to parse JSON response: Extra data: line 31 column 1 (char 873), falling back to regex parsing
2025-07-07 10:16:22,461 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:16:22,461 - INFO - Normalized predicted verses: []
2025-07-07 10:16:22,461 - INFO - Normalized ground truth verses: ['41:42']
2025-07-07 10:16:22,461 - INFO -   Response: 4684 chars, Found 0 verses
2025-07-07 10:16:22,461 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:22,461 - INFO -   Matched verses: []
2025-07-07 10:16:22,573 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:22,573 - WARNING -   Failed to get response for question 417
2025-07-07 10:16:22,587 - INFO - Saved data to results/fanar_checkpoint_364.json
2025-07-07 10:16:22,587 - INFO - Checkpoint saved for fanar at question 365
2025-07-07 10:16:22,587 - INFO -   Progress: 365/881
2025-07-07 10:16:22,587 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:22,587 - INFO - Processing question 366/881 (ID: 418) for fanar
2025-07-07 10:16:22,587 - INFO -   Ground truth verses: ['99:3', '96:5']
2025-07-07 10:16:22,587 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:22,726 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:22,727 - WARNING -   Failed to get response for question 418
2025-07-07 10:16:22,727 - INFO - Processing question 367/881 (ID: 419) for fanar
2025-07-07 10:16:22,727 - INFO -   Ground truth verses: ['99:3', '96:5']
2025-07-07 10:16:22,727 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:22,865 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:22,866 - WARNING -   Failed to get response for question 419
2025-07-07 10:16:22,866 - INFO - Processing question 368/881 (ID: 420) for fanar
2025-07-07 10:16:22,866 - INFO -   Ground truth verses: ['99:3', '96:5']
2025-07-07 10:16:22,866 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:23,001 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:23,002 - WARNING -   Failed to get response for question 420
2025-07-07 10:16:23,002 - INFO - Processing question 369/881 (ID: 421) for fanar
2025-07-07 10:16:23,002 - INFO -   Ground truth verses: ['7:125', '43:14', '26:50']
2025-07-07 10:16:23,002 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:23,141 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:23,142 - WARNING -   Failed to get response for question 421
2025-07-07 10:16:23,142 - INFO - Processing question 370/881 (ID: 422) for fanar
2025-07-07 10:16:23,142 - INFO -   Ground truth verses: ['7:125', '43:14', '26:50']
2025-07-07 10:16:23,142 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:23,281 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:23,282 - WARNING -   Failed to get response for question 422
2025-07-07 10:16:23,295 - INFO - Saved data to results/fanar_checkpoint_369.json
2025-07-07 10:16:23,295 - INFO - Checkpoint saved for fanar at question 370
2025-07-07 10:16:23,295 - INFO -   Progress: 370/881
2025-07-07 10:16:23,295 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:23,295 - INFO - Processing question 371/881 (ID: 423) for fanar
2025-07-07 10:16:23,295 - INFO -   Ground truth verses: ['7:125', '43:14', '26:50']
2025-07-07 10:16:23,295 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:23,430 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:23,430 - WARNING -   Failed to get response for question 423
2025-07-07 10:16:23,430 - INFO - Processing question 372/881 (ID: 425) for fanar
2025-07-07 10:16:23,430 - INFO -   Ground truth verses: ['7:125', '43:14', '26:50']
2025-07-07 10:16:23,430 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:23,462 - INFO - Processing question 299/881 (ID: 337) for qwen
2025-07-07 10:16:23,462 - INFO -   Ground truth verses: ['37:54']
2025-07-07 10:16:23,463 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:16:23,713 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:23,713 - WARNING -   Failed to get response for question 425
2025-07-07 10:16:23,713 - INFO - Processing question 373/881 (ID: 427) for fanar
2025-07-07 10:16:23,713 - INFO -   Ground truth verses: ['37:129', '37:108', '37:78', '37:119']
2025-07-07 10:16:23,713 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:23,876 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:23,876 - WARNING -   Failed to get response for question 427
2025-07-07 10:16:23,876 - INFO - Processing question 374/881 (ID: 429) for fanar
2025-07-07 10:16:23,877 - INFO -   Ground truth verses: ['37:129', '37:108', '37:78', '37:119']
2025-07-07 10:16:23,877 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:24,037 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:24,038 - WARNING -   Failed to get response for question 429
2025-07-07 10:16:24,038 - INFO - Processing question 375/881 (ID: 430) for fanar
2025-07-07 10:16:24,038 - INFO -   Ground truth verses: ['37:129', '37:108', '37:78', '37:119']
2025-07-07 10:16:24,038 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:24,198 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:24,199 - WARNING -   Failed to get response for question 430
2025-07-07 10:16:24,212 - INFO - Saved data to results/fanar_checkpoint_374.json
2025-07-07 10:16:24,212 - INFO - Checkpoint saved for fanar at question 375
2025-07-07 10:16:24,212 - INFO -   Progress: 375/881
2025-07-07 10:16:24,212 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:24,212 - INFO - Processing question 376/881 (ID: 431) for fanar
2025-07-07 10:16:24,212 - INFO -   Ground truth verses: ['54:37', '54:30', '54:21', '54:16', '54:18', '54:39']
2025-07-07 10:16:24,212 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:24,374 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:24,374 - WARNING -   Failed to get response for question 431
2025-07-07 10:16:24,374 - INFO - Processing question 377/881 (ID: 432) for fanar
2025-07-07 10:16:24,375 - INFO -   Ground truth verses: ['54:37', '54:30', '54:21', '54:16', '54:18', '54:39']
2025-07-07 10:16:24,375 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:24,536 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:24,536 - WARNING -   Failed to get response for question 432
2025-07-07 10:16:24,536 - INFO - Processing question 378/881 (ID: 433) for fanar
2025-07-07 10:16:24,536 - INFO -   Ground truth verses: ['54:37', '54:30', '54:21', '54:16', '54:18', '54:39']
2025-07-07 10:16:24,536 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:24,700 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:24,700 - WARNING -   Failed to get response for question 433
2025-07-07 10:16:24,700 - INFO - Processing question 379/881 (ID: 435) for fanar
2025-07-07 10:16:24,701 - INFO -   Ground truth verses: ['54:37', '54:30', '54:21', '54:16', '54:18', '54:39']
2025-07-07 10:16:24,701 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:25,004 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:25,005 - WARNING -   Failed to get response for question 435
2025-07-07 10:16:25,005 - INFO - Processing question 380/881 (ID: 436) for fanar
2025-07-07 10:16:25,005 - INFO -   Ground truth verses: ['26:193']
2025-07-07 10:16:25,005 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:25,168 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:25,169 - WARNING -   Failed to get response for question 436
2025-07-07 10:16:25,183 - INFO - Saved data to results/fanar_checkpoint_379.json
2025-07-07 10:16:25,183 - INFO - Checkpoint saved for fanar at question 380
2025-07-07 10:16:25,183 - INFO -   Progress: 380/881
2025-07-07 10:16:25,183 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:25,183 - INFO - Processing question 381/881 (ID: 437) for fanar
2025-07-07 10:16:25,183 - INFO -   Ground truth verses: ['26:193']
2025-07-07 10:16:25,183 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:25,344 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:25,344 - WARNING -   Failed to get response for question 437
2025-07-07 10:16:25,344 - INFO - Processing question 382/881 (ID: 439) for fanar
2025-07-07 10:16:25,344 - INFO -   Ground truth verses: ['26:193']
2025-07-07 10:16:25,344 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:25,508 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:25,508 - WARNING -   Failed to get response for question 439
2025-07-07 10:16:25,508 - INFO - Processing question 383/881 (ID: 440) for fanar
2025-07-07 10:16:25,508 - INFO -   Ground truth verses: ['26:193']
2025-07-07 10:16:25,508 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:25,672 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:25,673 - WARNING -   Failed to get response for question 440
2025-07-07 10:16:25,673 - INFO - Processing question 384/881 (ID: 441) for fanar
2025-07-07 10:16:25,673 - INFO -   Ground truth verses: ['40:5']
2025-07-07 10:16:25,673 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:25,833 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:25,833 - WARNING -   Failed to get response for question 441
2025-07-07 10:16:25,834 - INFO - Processing question 385/881 (ID: 442) for fanar
2025-07-07 10:16:25,834 - INFO -   Ground truth verses: ['40:5']
2025-07-07 10:16:25,834 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:25,994 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:25,994 - WARNING -   Failed to get response for question 442
2025-07-07 10:16:26,008 - INFO - Saved data to results/fanar_checkpoint_384.json
2025-07-07 10:16:26,008 - INFO - Checkpoint saved for fanar at question 385
2025-07-07 10:16:26,008 - INFO -   Progress: 385/881
2025-07-07 10:16:26,008 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:26,008 - INFO - Processing question 386/881 (ID: 444) for fanar
2025-07-07 10:16:26,008 - INFO -   Ground truth verses: ['40:5']
2025-07-07 10:16:26,008 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:26,172 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:26,173 - WARNING -   Failed to get response for question 444
2025-07-07 10:16:26,173 - INFO - Processing question 387/881 (ID: 445) for fanar
2025-07-07 10:16:26,173 - INFO -   Ground truth verses: ['40:5']
2025-07-07 10:16:26,173 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:26,482 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:26,482 - WARNING -   Failed to get response for question 445
2025-07-07 10:16:26,483 - INFO - Processing question 388/881 (ID: 446) for fanar
2025-07-07 10:16:26,483 - INFO -   Ground truth verses: ['26:55']
2025-07-07 10:16:26,483 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:26,647 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:26,647 - WARNING -   Failed to get response for question 446
2025-07-07 10:16:26,647 - INFO - Processing question 389/881 (ID: 451) for fanar
2025-07-07 10:16:26,647 - INFO -   Ground truth verses: ['81:29', '74:56']
2025-07-07 10:16:26,648 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:26,812 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:26,812 - WARNING -   Failed to get response for question 451
2025-07-07 10:16:26,812 - INFO - Processing question 390/881 (ID: 452) for fanar
2025-07-07 10:16:26,812 - INFO -   Ground truth verses: ['81:29', '74:56']
2025-07-07 10:16:26,812 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:26,976 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:26,976 - WARNING -   Failed to get response for question 452
2025-07-07 10:16:26,990 - INFO - Saved data to results/fanar_checkpoint_389.json
2025-07-07 10:16:26,990 - INFO - Checkpoint saved for fanar at question 390
2025-07-07 10:16:26,990 - INFO -   Progress: 390/881
2025-07-07 10:16:26,990 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:26,990 - INFO - Processing question 391/881 (ID: 455) for fanar
2025-07-07 10:16:26,990 - INFO -   Ground truth verses: ['81:29', '74:56']
2025-07-07 10:16:26,990 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:27,154 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:27,155 - WARNING -   Failed to get response for question 455
2025-07-07 10:16:27,155 - INFO - Processing question 392/881 (ID: 456) for fanar
2025-07-07 10:16:27,155 - INFO -   Ground truth verses: ['3:22', '9:69']
2025-07-07 10:16:27,155 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:27,315 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:27,316 - WARNING -   Failed to get response for question 456
2025-07-07 10:16:27,316 - INFO - Processing question 393/881 (ID: 457) for fanar
2025-07-07 10:16:27,316 - INFO -   Ground truth verses: ['3:22', '9:69']
2025-07-07 10:16:27,316 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:27,476 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:27,477 - WARNING -   Failed to get response for question 457
2025-07-07 10:16:27,477 - INFO - Processing question 394/881 (ID: 458) for fanar
2025-07-07 10:16:27,477 - INFO -   Ground truth verses: ['3:22', '9:69']
2025-07-07 10:16:27,477 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:27,787 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:27,788 - WARNING -   Failed to get response for question 458
2025-07-07 10:16:27,788 - INFO - Processing question 395/881 (ID: 459) for fanar
2025-07-07 10:16:27,788 - INFO -   Ground truth verses: ['3:22', '9:69']
2025-07-07 10:16:27,788 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:27,948 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:27,948 - WARNING -   Failed to get response for question 459
2025-07-07 10:16:27,963 - INFO - Saved data to results/fanar_checkpoint_394.json
2025-07-07 10:16:27,963 - INFO - Checkpoint saved for fanar at question 395
2025-07-07 10:16:27,963 - INFO -   Progress: 395/881
2025-07-07 10:16:27,963 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:27,963 - INFO - Processing question 396/881 (ID: 460) for fanar
2025-07-07 10:16:27,963 - INFO -   Ground truth verses: ['3:22', '9:69']
2025-07-07 10:16:27,963 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:28,123 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:28,124 - WARNING -   Failed to get response for question 460
2025-07-07 10:16:28,124 - INFO - Processing question 397/881 (ID: 461) for fanar
2025-07-07 10:16:28,124 - INFO -   Ground truth verses: ['56:12', '37:43', '56:89']
2025-07-07 10:16:28,124 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:28,284 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:28,285 - WARNING -   Failed to get response for question 461
2025-07-07 10:16:28,285 - INFO - Processing question 398/881 (ID: 462) for fanar
2025-07-07 10:16:28,285 - INFO -   Ground truth verses: ['56:12', '37:43', '56:89']
2025-07-07 10:16:28,285 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:28,447 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:28,448 - WARNING -   Failed to get response for question 462
2025-07-07 10:16:28,448 - INFO - Processing question 399/881 (ID: 463) for fanar
2025-07-07 10:16:28,448 - INFO -   Ground truth verses: ['56:12', '37:43', '56:89']
2025-07-07 10:16:28,448 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:28,610 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:28,610 - WARNING -   Failed to get response for question 463
2025-07-07 10:16:28,610 - INFO - Processing question 400/881 (ID: 464) for fanar
2025-07-07 10:16:28,610 - INFO -   Ground truth verses: ['56:12', '37:43', '56:89']
2025-07-07 10:16:28,610 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:28,772 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:28,772 - WARNING -   Failed to get response for question 464
2025-07-07 10:16:28,786 - INFO - Saved data to results/fanar_checkpoint_399.json
2025-07-07 10:16:28,787 - INFO - Checkpoint saved for fanar at question 400
2025-07-07 10:16:28,787 - INFO -   Progress: 400/881
2025-07-07 10:16:28,787 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:28,787 - INFO - Processing question 401/881 (ID: 465) for fanar
2025-07-07 10:16:28,787 - INFO -   Ground truth verses: ['56:12', '37:43', '56:89']
2025-07-07 10:16:28,787 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:29,101 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:29,102 - WARNING -   Failed to get response for question 465
2025-07-07 10:16:29,102 - INFO - Processing question 402/881 (ID: 466) for fanar
2025-07-07 10:16:29,102 - INFO -   Ground truth verses: ['20:104']
2025-07-07 10:16:29,102 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:29,267 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:29,267 - WARNING -   Failed to get response for question 466
2025-07-07 10:16:29,268 - INFO - Processing question 403/881 (ID: 467) for fanar
2025-07-07 10:16:29,268 - INFO -   Ground truth verses: ['20:104']
2025-07-07 10:16:29,268 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:29,430 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:29,430 - WARNING -   Failed to get response for question 467
2025-07-07 10:16:29,430 - INFO - Processing question 404/881 (ID: 469) for fanar
2025-07-07 10:16:29,430 - INFO -   Ground truth verses: ['20:104']
2025-07-07 10:16:29,431 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:29,593 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:29,593 - WARNING -   Failed to get response for question 469
2025-07-07 10:16:29,593 - INFO - Processing question 405/881 (ID: 471) for fanar
2025-07-07 10:16:29,593 - INFO -   Ground truth verses: ['97:5', '17:78', '89:1']
2025-07-07 10:16:29,593 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:29,755 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:29,755 - WARNING -   Failed to get response for question 471
2025-07-07 10:16:29,770 - INFO - Saved data to results/fanar_checkpoint_404.json
2025-07-07 10:16:29,770 - INFO - Checkpoint saved for fanar at question 405
2025-07-07 10:16:29,770 - INFO -   Progress: 405/881
2025-07-07 10:16:29,770 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:29,770 - INFO - Processing question 406/881 (ID: 472) for fanar
2025-07-07 10:16:29,770 - INFO -   Ground truth verses: ['97:5', '17:78', '89:1']
2025-07-07 10:16:29,770 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:29,929 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:29,930 - WARNING -   Failed to get response for question 472
2025-07-07 10:16:29,930 - INFO - Processing question 407/881 (ID: 473) for fanar
2025-07-07 10:16:29,930 - INFO -   Ground truth verses: ['97:5', '17:78', '89:1']
2025-07-07 10:16:29,930 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:30,090 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:30,091 - WARNING -   Failed to get response for question 473
2025-07-07 10:16:30,091 - INFO - Processing question 408/881 (ID: 474) for fanar
2025-07-07 10:16:30,091 - INFO -   Ground truth verses: ['97:5', '17:78', '89:1']
2025-07-07 10:16:30,091 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:30,254 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:30,254 - WARNING -   Failed to get response for question 474
2025-07-07 10:16:30,254 - INFO - Processing question 409/881 (ID: 475) for fanar
2025-07-07 10:16:30,254 - INFO -   Ground truth verses: ['97:5', '17:78', '89:1']
2025-07-07 10:16:30,254 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:30,566 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:30,567 - WARNING -   Failed to get response for question 475
2025-07-07 10:16:30,567 - INFO - Processing question 410/881 (ID: 476) for fanar
2025-07-07 10:16:30,567 - INFO -   Ground truth verses: ['53:40']
2025-07-07 10:16:30,567 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:30,733 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:30,733 - WARNING -   Failed to get response for question 476
2025-07-07 10:16:30,748 - INFO - Saved data to results/fanar_checkpoint_409.json
2025-07-07 10:16:30,749 - INFO - Checkpoint saved for fanar at question 410
2025-07-07 10:16:30,749 - INFO -   Progress: 410/881
2025-07-07 10:16:30,749 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:30,749 - INFO - Processing question 411/881 (ID: 477) for fanar
2025-07-07 10:16:30,749 - INFO -   Ground truth verses: ['53:40']
2025-07-07 10:16:30,749 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:30,910 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:30,910 - WARNING -   Failed to get response for question 477
2025-07-07 10:16:30,910 - INFO - Processing question 412/881 (ID: 478) for fanar
2025-07-07 10:16:30,911 - INFO -   Ground truth verses: ['53:40']
2025-07-07 10:16:30,911 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:31,072 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:31,072 - WARNING -   Failed to get response for question 478
2025-07-07 10:16:31,072 - INFO - Processing question 413/881 (ID: 479) for fanar
2025-07-07 10:16:31,072 - INFO -   Ground truth verses: ['53:40']
2025-07-07 10:16:31,072 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:31,236 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:31,236 - WARNING -   Failed to get response for question 479
2025-07-07 10:16:31,236 - INFO - Processing question 414/881 (ID: 480) for fanar
2025-07-07 10:16:31,236 - INFO -   Ground truth verses: ['53:40']
2025-07-07 10:16:31,236 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:31,398 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:31,399 - WARNING -   Failed to get response for question 480
2025-07-07 10:16:31,399 - INFO - Processing question 415/881 (ID: 486) for fanar
2025-07-07 10:16:31,399 - INFO -   Ground truth verses: ['17:40']
2025-07-07 10:16:31,399 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:31,563 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:31,564 - WARNING -   Failed to get response for question 486
2025-07-07 10:16:31,579 - INFO - Saved data to results/fanar_checkpoint_414.json
2025-07-07 10:16:31,579 - INFO - Checkpoint saved for fanar at question 415
2025-07-07 10:16:31,579 - INFO -   Progress: 415/881
2025-07-07 10:16:31,579 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:31,579 - INFO - Processing question 416/881 (ID: 487) for fanar
2025-07-07 10:16:31,579 - INFO -   Ground truth verses: ['17:40']
2025-07-07 10:16:31,579 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:31,743 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:31,744 - WARNING -   Failed to get response for question 487
2025-07-07 10:16:31,744 - INFO - Processing question 417/881 (ID: 490) for fanar
2025-07-07 10:16:31,744 - INFO -   Ground truth verses: ['17:40']
2025-07-07 10:16:31,744 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:32,054 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:32,054 - WARNING -   Failed to get response for question 490
2025-07-07 10:16:32,055 - INFO - Processing question 418/881 (ID: 491) for fanar
2025-07-07 10:16:32,055 - INFO -   Ground truth verses: ['37:44', '7:43', '15:47']
2025-07-07 10:16:32,055 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:32,214 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:32,215 - WARNING -   Failed to get response for question 491
2025-07-07 10:16:32,215 - INFO - Processing question 419/881 (ID: 492) for fanar
2025-07-07 10:16:32,215 - INFO -   Ground truth verses: ['37:44', '7:43', '15:47']
2025-07-07 10:16:32,215 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:32,376 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:32,377 - WARNING -   Failed to get response for question 492
2025-07-07 10:16:32,377 - INFO - Processing question 420/881 (ID: 493) for fanar
2025-07-07 10:16:32,377 - INFO -   Ground truth verses: ['37:44', '7:43', '15:47']
2025-07-07 10:16:32,377 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:32,539 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:32,540 - WARNING -   Failed to get response for question 493
2025-07-07 10:16:32,556 - INFO - Saved data to results/fanar_checkpoint_419.json
2025-07-07 10:16:32,556 - INFO - Checkpoint saved for fanar at question 420
2025-07-07 10:16:32,556 - INFO -   Progress: 420/881
2025-07-07 10:16:32,556 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:32,556 - INFO - Processing question 421/881 (ID: 494) for fanar
2025-07-07 10:16:32,556 - INFO -   Ground truth verses: ['37:44', '7:43', '15:47']
2025-07-07 10:16:32,556 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:32,721 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:32,721 - WARNING -   Failed to get response for question 494
2025-07-07 10:16:32,721 - INFO - Processing question 422/881 (ID: 495) for fanar
2025-07-07 10:16:32,721 - INFO -   Ground truth verses: ['37:44', '7:43', '15:47']
2025-07-07 10:16:32,721 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:32,883 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:32,884 - WARNING -   Failed to get response for question 495
2025-07-07 10:16:32,884 - INFO - Processing question 423/881 (ID: 496) for fanar
2025-07-07 10:16:32,884 - INFO -   Ground truth verses: ['43:49', '7:134']
2025-07-07 10:16:32,884 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:33,048 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:33,048 - WARNING -   Failed to get response for question 496
2025-07-07 10:16:33,048 - INFO - Processing question 424/881 (ID: 497) for fanar
2025-07-07 10:16:33,048 - INFO -   Ground truth verses: ['43:49', '7:134']
2025-07-07 10:16:33,048 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:33,355 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:33,356 - WARNING -   Failed to get response for question 497
2025-07-07 10:16:33,356 - INFO - Processing question 425/881 (ID: 498) for fanar
2025-07-07 10:16:33,356 - INFO -   Ground truth verses: ['43:49', '7:134']
2025-07-07 10:16:33,356 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:33,519 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:33,519 - WARNING -   Failed to get response for question 498
2025-07-07 10:16:33,535 - INFO - Saved data to results/fanar_checkpoint_424.json
2025-07-07 10:16:33,535 - INFO - Checkpoint saved for fanar at question 425
2025-07-07 10:16:33,535 - INFO -   Progress: 425/881
2025-07-07 10:16:33,535 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:33,535 - INFO - Processing question 426/881 (ID: 499) for fanar
2025-07-07 10:16:33,535 - INFO -   Ground truth verses: ['43:49', '7:134']
2025-07-07 10:16:33,535 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:33,701 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:33,701 - WARNING -   Failed to get response for question 499
2025-07-07 10:16:33,701 - INFO - Processing question 427/881 (ID: 500) for fanar
2025-07-07 10:16:33,701 - INFO -   Ground truth verses: ['43:49', '7:134']
2025-07-07 10:16:33,701 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:33,865 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:33,865 - WARNING -   Failed to get response for question 500
2025-07-07 10:16:33,865 - INFO - Processing question 428/881 (ID: 501) for fanar
2025-07-07 10:16:33,865 - INFO -   Ground truth verses: ['84:9', '84:13']
2025-07-07 10:16:33,865 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:34,027 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:34,028 - WARNING -   Failed to get response for question 501
2025-07-07 10:16:34,028 - INFO - Processing question 429/881 (ID: 502) for fanar
2025-07-07 10:16:34,028 - INFO -   Ground truth verses: ['84:9', '84:13']
2025-07-07 10:16:34,028 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:34,191 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:34,191 - WARNING -   Failed to get response for question 502
2025-07-07 10:16:34,192 - INFO - Processing question 430/881 (ID: 503) for fanar
2025-07-07 10:16:34,192 - INFO -   Ground truth verses: ['84:9', '84:13']
2025-07-07 10:16:34,192 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:34,354 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:34,354 - WARNING -   Failed to get response for question 503
2025-07-07 10:16:34,369 - INFO - Saved data to results/fanar_checkpoint_429.json
2025-07-07 10:16:34,370 - INFO - Checkpoint saved for fanar at question 430
2025-07-07 10:16:34,370 - INFO -   Progress: 430/881
2025-07-07 10:16:34,370 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:34,370 - INFO - Processing question 431/881 (ID: 504) for fanar
2025-07-07 10:16:34,370 - INFO -   Ground truth verses: ['84:9', '84:13']
2025-07-07 10:16:34,370 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:34,533 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:34,534 - WARNING -   Failed to get response for question 504
2025-07-07 10:16:34,534 - INFO - Processing question 432/881 (ID: 505) for fanar
2025-07-07 10:16:34,534 - INFO -   Ground truth verses: ['84:9', '84:13']
2025-07-07 10:16:34,534 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:34,853 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:34,853 - WARNING -   Failed to get response for question 505
2025-07-07 10:16:34,853 - INFO - Processing question 433/881 (ID: 506) for fanar
2025-07-07 10:16:34,853 - INFO -   Ground truth verses: ['76:14', '69:23']
2025-07-07 10:16:34,853 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:35,016 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:35,016 - WARNING -   Failed to get response for question 506
2025-07-07 10:16:35,017 - INFO - Processing question 434/881 (ID: 507) for fanar
2025-07-07 10:16:35,017 - INFO -   Ground truth verses: ['76:14', '69:23']
2025-07-07 10:16:35,017 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:35,182 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:35,183 - WARNING -   Failed to get response for question 507
2025-07-07 10:16:35,183 - INFO - Processing question 435/881 (ID: 508) for fanar
2025-07-07 10:16:35,183 - INFO -   Ground truth verses: ['76:14', '69:23']
2025-07-07 10:16:35,183 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:35,348 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:35,349 - WARNING -   Failed to get response for question 508
2025-07-07 10:16:35,364 - INFO - Saved data to results/fanar_checkpoint_434.json
2025-07-07 10:16:35,364 - INFO - Checkpoint saved for fanar at question 435
2025-07-07 10:16:35,364 - INFO -   Progress: 435/881
2025-07-07 10:16:35,364 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:35,364 - INFO - Processing question 436/881 (ID: 509) for fanar
2025-07-07 10:16:35,364 - INFO -   Ground truth verses: ['76:14', '69:23']
2025-07-07 10:16:35,364 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:35,525 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:35,525 - WARNING -   Failed to get response for question 509
2025-07-07 10:16:35,525 - INFO - Processing question 437/881 (ID: 510) for fanar
2025-07-07 10:16:35,525 - INFO -   Ground truth verses: ['76:14', '69:23']
2025-07-07 10:16:35,525 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:35,690 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:35,691 - WARNING -   Failed to get response for question 510
2025-07-07 10:16:35,691 - INFO - Processing question 438/881 (ID: 511) for fanar
2025-07-07 10:16:35,691 - INFO -   Ground truth verses: ['26:191', '26:175', '26:159', '26:140', '26:122', '26:68', '26:9', '36:5', '26:217', '44:42', '32:6', '26:104']
2025-07-07 10:16:35,691 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:35,855 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:35,855 - WARNING -   Failed to get response for question 511
2025-07-07 10:16:35,855 - INFO - Processing question 439/881 (ID: 512) for fanar
2025-07-07 10:16:35,855 - INFO -   Ground truth verses: ['26:191', '26:175', '26:159', '26:140', '26:122', '26:68', '26:9', '36:5', '26:217', '44:42', '32:6', '26:104']
2025-07-07 10:16:35,855 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:36,178 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:36,179 - WARNING -   Failed to get response for question 512
2025-07-07 10:16:36,179 - INFO - Processing question 440/881 (ID: 513) for fanar
2025-07-07 10:16:36,179 - INFO -   Ground truth verses: ['26:191', '26:175', '26:159', '26:140', '26:122', '26:68', '26:9', '36:5', '26:217', '44:42', '32:6', '26:104']
2025-07-07 10:16:36,179 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:36,341 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:36,342 - WARNING -   Failed to get response for question 513
2025-07-07 10:16:36,357 - INFO - Saved data to results/fanar_checkpoint_439.json
2025-07-07 10:16:36,357 - INFO - Checkpoint saved for fanar at question 440
2025-07-07 10:16:36,358 - INFO -   Progress: 440/881
2025-07-07 10:16:36,358 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:36,358 - INFO - Processing question 441/881 (ID: 514) for fanar
2025-07-07 10:16:36,358 - INFO -   Ground truth verses: ['26:191', '26:175', '26:159', '26:140', '26:122', '26:68', '26:9', '36:5', '26:217', '44:42', '32:6', '26:104']
2025-07-07 10:16:36,358 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:36,519 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:36,520 - WARNING -   Failed to get response for question 514
2025-07-07 10:16:36,520 - INFO - Processing question 442/881 (ID: 515) for fanar
2025-07-07 10:16:36,520 - INFO -   Ground truth verses: ['26:191', '26:175', '26:159', '26:140', '26:122', '26:68', '26:9', '36:5', '26:217', '44:42', '32:6', '26:104']
2025-07-07 10:16:36,520 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:36,687 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:36,688 - WARNING -   Failed to get response for question 515
2025-07-07 10:16:36,688 - INFO - Processing question 443/881 (ID: 516) for fanar
2025-07-07 10:16:36,688 - INFO -   Ground truth verses: ['6:21', '10:17', '29:68', '40:74', '61:7', '18:15', '7:37']
2025-07-07 10:16:36,688 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:36,851 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:36,851 - WARNING -   Failed to get response for question 516
2025-07-07 10:16:36,851 - INFO - Processing question 444/881 (ID: 517) for fanar
2025-07-07 10:16:36,851 - INFO -   Ground truth verses: ['6:21', '10:17', '29:68', '40:74', '61:7', '18:15', '7:37']
2025-07-07 10:16:36,851 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:37,014 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:37,015 - WARNING -   Failed to get response for question 517
2025-07-07 10:16:37,015 - INFO - Processing question 445/881 (ID: 518) for fanar
2025-07-07 10:16:37,015 - INFO -   Ground truth verses: ['6:21', '10:17', '29:68', '40:74', '61:7', '18:15', '7:37']
2025-07-07 10:16:37,015 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:37,178 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:37,178 - WARNING -   Failed to get response for question 518
2025-07-07 10:16:37,193 - INFO - Saved data to results/fanar_checkpoint_444.json
2025-07-07 10:16:37,194 - INFO - Checkpoint saved for fanar at question 445
2025-07-07 10:16:37,194 - INFO -   Progress: 445/881
2025-07-07 10:16:37,194 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:37,194 - INFO - Processing question 446/881 (ID: 519) for fanar
2025-07-07 10:16:37,194 - INFO -   Ground truth verses: ['6:21', '10:17', '29:68', '40:74', '61:7', '18:15', '7:37']
2025-07-07 10:16:37,194 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:37,532 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:37,533 - WARNING -   Failed to get response for question 519
2025-07-07 10:16:37,533 - INFO - Processing question 447/881 (ID: 520) for fanar
2025-07-07 10:16:37,533 - INFO -   Ground truth verses: ['6:21', '10:17', '29:68', '40:74', '61:7', '18:15', '7:37']
2025-07-07 10:16:37,533 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:37,702 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:37,702 - WARNING -   Failed to get response for question 520
2025-07-07 10:16:37,703 - INFO - Processing question 448/881 (ID: 521) for fanar
2025-07-07 10:16:37,703 - INFO -   Ground truth verses: ['32:1', '31:1', '30:1', '29:1', '3:1', '2:1']
2025-07-07 10:16:37,703 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:37,865 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:37,866 - WARNING -   Failed to get response for question 521
2025-07-07 10:16:37,866 - INFO - Processing question 449/881 (ID: 522) for fanar
2025-07-07 10:16:37,866 - INFO -   Ground truth verses: ['32:1', '31:1', '30:1', '29:1', '3:1', '2:1']
2025-07-07 10:16:37,866 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:38,029 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:38,030 - WARNING -   Failed to get response for question 522
2025-07-07 10:16:38,030 - INFO - Processing question 450/881 (ID: 523) for fanar
2025-07-07 10:16:38,030 - INFO -   Ground truth verses: ['32:1', '31:1', '30:1', '29:1', '3:1', '2:1']
2025-07-07 10:16:38,030 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:38,193 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:38,193 - WARNING -   Failed to get response for question 523
2025-07-07 10:16:38,209 - INFO - Saved data to results/fanar_checkpoint_449.json
2025-07-07 10:16:38,209 - INFO - Checkpoint saved for fanar at question 450
2025-07-07 10:16:38,209 - INFO -   Progress: 450/881
2025-07-07 10:16:38,209 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:38,210 - INFO - Processing question 451/881 (ID: 524) for fanar
2025-07-07 10:16:38,210 - INFO -   Ground truth verses: ['32:1', '31:1', '30:1', '29:1', '3:1', '2:1']
2025-07-07 10:16:38,210 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:38,373 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:38,373 - WARNING -   Failed to get response for question 524
2025-07-07 10:16:38,373 - INFO - Processing question 452/881 (ID: 525) for fanar
2025-07-07 10:16:38,373 - INFO -   Ground truth verses: ['32:1', '31:1', '30:1', '29:1', '3:1', '2:1']
2025-07-07 10:16:38,373 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:38,537 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:38,537 - WARNING -   Failed to get response for question 525
2025-07-07 10:16:38,538 - INFO - Processing question 453/881 (ID: 526) for fanar
2025-07-07 10:16:38,538 - INFO -   Ground truth verses: ['58:4']
2025-07-07 10:16:38,538 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:38,705 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:38,706 - WARNING -   Failed to get response for question 526
2025-07-07 10:16:38,706 - INFO - Processing question 454/881 (ID: 527) for fanar
2025-07-07 10:16:38,706 - INFO -   Ground truth verses: ['58:4']
2025-07-07 10:16:38,706 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:39,047 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:39,048 - WARNING -   Failed to get response for question 527
2025-07-07 10:16:39,048 - INFO - Processing question 455/881 (ID: 528) for fanar
2025-07-07 10:16:39,048 - INFO -   Ground truth verses: ['58:4']
2025-07-07 10:16:39,048 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:39,216 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:39,217 - WARNING -   Failed to get response for question 528
2025-07-07 10:16:39,233 - INFO - Saved data to results/fanar_checkpoint_454.json
2025-07-07 10:16:39,233 - INFO - Checkpoint saved for fanar at question 455
2025-07-07 10:16:39,233 - INFO -   Progress: 455/881
2025-07-07 10:16:39,234 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:39,234 - INFO - Processing question 456/881 (ID: 529) for fanar
2025-07-07 10:16:39,234 - INFO -   Ground truth verses: ['58:4']
2025-07-07 10:16:39,234 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:39,404 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:39,405 - WARNING -   Failed to get response for question 529
2025-07-07 10:16:39,405 - INFO - Processing question 457/881 (ID: 530) for fanar
2025-07-07 10:16:39,405 - INFO -   Ground truth verses: ['58:4']
2025-07-07 10:16:39,405 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:39,568 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:39,569 - WARNING -   Failed to get response for question 530
2025-07-07 10:16:39,569 - INFO - Processing question 458/881 (ID: 531) for fanar
2025-07-07 10:16:39,569 - INFO -   Ground truth verses: ['75:9', '6:96', '74:32', '41:37', '55:5']
2025-07-07 10:16:39,569 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:39,735 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:39,735 - WARNING -   Failed to get response for question 531
2025-07-07 10:16:39,736 - INFO - Processing question 459/881 (ID: 532) for fanar
2025-07-07 10:16:39,736 - INFO -   Ground truth verses: ['75:9', '6:96', '74:32', '41:37', '55:5']
2025-07-07 10:16:39,736 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:39,902 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:39,902 - WARNING -   Failed to get response for question 532
2025-07-07 10:16:39,902 - INFO - Processing question 460/881 (ID: 533) for fanar
2025-07-07 10:16:39,902 - INFO -   Ground truth verses: ['75:9', '6:96', '74:32', '41:37', '55:5']
2025-07-07 10:16:39,902 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:40,066 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:40,067 - WARNING -   Failed to get response for question 533
2025-07-07 10:16:40,083 - INFO - Saved data to results/fanar_checkpoint_459.json
2025-07-07 10:16:40,084 - INFO - Checkpoint saved for fanar at question 460
2025-07-07 10:16:40,084 - INFO -   Progress: 460/881
2025-07-07 10:16:40,084 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:40,084 - INFO - Processing question 461/881 (ID: 534) for fanar
2025-07-07 10:16:40,084 - INFO -   Ground truth verses: ['75:9', '6:96', '74:32', '41:37', '55:5']
2025-07-07 10:16:40,084 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:40,246 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:40,246 - WARNING -   Failed to get response for question 534
2025-07-07 10:16:40,246 - INFO - Processing question 462/881 (ID: 535) for fanar
2025-07-07 10:16:40,246 - INFO -   Ground truth verses: ['75:9', '6:96', '74:32', '41:37', '55:5']
2025-07-07 10:16:40,246 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:40,586 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:40,587 - WARNING -   Failed to get response for question 535
2025-07-07 10:16:40,587 - INFO - Processing question 463/881 (ID: 536) for fanar
2025-07-07 10:16:40,587 - INFO -   Ground truth verses: ['80:1', '74:22']
2025-07-07 10:16:40,587 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:40,751 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:40,752 - WARNING -   Failed to get response for question 536
2025-07-07 10:16:40,752 - INFO - Processing question 464/881 (ID: 538) for fanar
2025-07-07 10:16:40,752 - INFO -   Ground truth verses: ['80:1', '74:22']
2025-07-07 10:16:40,752 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:40,915 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:40,915 - WARNING -   Failed to get response for question 538
2025-07-07 10:16:40,916 - INFO - Processing question 465/881 (ID: 539) for fanar
2025-07-07 10:16:40,916 - INFO -   Ground truth verses: ['80:1', '74:22']
2025-07-07 10:16:40,916 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:41,078 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:41,079 - WARNING -   Failed to get response for question 539
2025-07-07 10:16:41,095 - INFO - Saved data to results/fanar_checkpoint_464.json
2025-07-07 10:16:41,096 - INFO - Checkpoint saved for fanar at question 465
2025-07-07 10:16:41,096 - INFO -   Progress: 465/881
2025-07-07 10:16:41,096 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:41,096 - INFO - Processing question 466/881 (ID: 540) for fanar
2025-07-07 10:16:41,096 - INFO -   Ground truth verses: ['80:1', '74:22']
2025-07-07 10:16:41,096 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:41,258 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:41,259 - WARNING -   Failed to get response for question 540
2025-07-07 10:16:41,259 - INFO - Processing question 467/881 (ID: 541) for fanar
2025-07-07 10:16:41,259 - INFO -   Ground truth verses: ['19:9', '51:30', '19:21']
2025-07-07 10:16:41,259 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:41,358 - INFO - Extracted JSON from code block: 754 characters
2025-07-07 10:16:41,359 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:16:41,359 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Ma'idah:10]' -> 'Al-Ma'idah:10'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Hijr:15]' -> 'Al-Hijr:15'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:16:41,359 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:16:41,359 - INFO - Normalized predicted verses: ['15:15', '2:255', '2:64', '50:5', '5:10']
2025-07-07 10:16:41,359 - INFO - Normalized ground truth verses: ['37:54']
2025-07-07 10:16:41,359 - INFO -   Response: 754 chars, Found 5 verses
2025-07-07 10:16:41,359 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:41,359 - INFO -   Matched verses: []
2025-07-07 10:16:41,421 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:41,422 - WARNING -   Failed to get response for question 541
2025-07-07 10:16:41,422 - INFO - Processing question 468/881 (ID: 542) for fanar
2025-07-07 10:16:41,422 - INFO -   Ground truth verses: ['19:9', '51:30', '19:21']
2025-07-07 10:16:41,422 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:41,562 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:41,562 - WARNING -   Failed to get response for question 542
2025-07-07 10:16:41,562 - INFO - Processing question 469/881 (ID: 543) for fanar
2025-07-07 10:16:41,562 - INFO -   Ground truth verses: ['19:9', '51:30', '19:21']
2025-07-07 10:16:41,562 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:41,875 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:41,875 - WARNING -   Failed to get response for question 543
2025-07-07 10:16:41,876 - INFO - Processing question 470/881 (ID: 544) for fanar
2025-07-07 10:16:41,876 - INFO -   Ground truth verses: ['19:9', '51:30', '19:21']
2025-07-07 10:16:41,876 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:42,014 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:42,014 - WARNING -   Failed to get response for question 544
2025-07-07 10:16:42,031 - INFO - Saved data to results/fanar_checkpoint_469.json
2025-07-07 10:16:42,031 - INFO - Checkpoint saved for fanar at question 470
2025-07-07 10:16:42,031 - INFO -   Progress: 470/881
2025-07-07 10:16:42,031 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:42,031 - INFO - Processing question 471/881 (ID: 545) for fanar
2025-07-07 10:16:42,031 - INFO -   Ground truth verses: ['19:9', '51:30', '19:21']
2025-07-07 10:16:42,031 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:42,167 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:42,167 - WARNING -   Failed to get response for question 545
2025-07-07 10:16:42,167 - INFO - Processing question 472/881 (ID: 546) for fanar
2025-07-07 10:16:42,167 - INFO -   Ground truth verses: ['35:29', '14:31', '28:54', '13:22']
2025-07-07 10:16:42,167 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:42,304 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:42,304 - WARNING -   Failed to get response for question 546
2025-07-07 10:16:42,304 - INFO - Processing question 473/881 (ID: 547) for fanar
2025-07-07 10:16:42,304 - INFO -   Ground truth verses: ['35:29', '14:31', '28:54', '13:22']
2025-07-07 10:16:42,304 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:42,360 - INFO - Processing question 300/881 (ID: 338) for qwen
2025-07-07 10:16:42,361 - INFO -   Ground truth verses: ['37:54']
2025-07-07 10:16:42,361 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:16:42,441 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:42,442 - WARNING -   Failed to get response for question 547
2025-07-07 10:16:42,442 - INFO - Processing question 474/881 (ID: 548) for fanar
2025-07-07 10:16:42,442 - INFO -   Ground truth verses: ['35:29', '14:31', '28:54', '13:22']
2025-07-07 10:16:42,442 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:42,603 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:42,604 - WARNING -   Failed to get response for question 548
2025-07-07 10:16:42,604 - INFO - Processing question 475/881 (ID: 549) for fanar
2025-07-07 10:16:42,604 - INFO -   Ground truth verses: ['35:29', '14:31', '28:54', '13:22']
2025-07-07 10:16:42,604 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:42,766 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:42,766 - WARNING -   Failed to get response for question 549
2025-07-07 10:16:42,783 - INFO - Saved data to results/fanar_checkpoint_474.json
2025-07-07 10:16:42,783 - INFO - Checkpoint saved for fanar at question 475
2025-07-07 10:16:42,783 - INFO -   Progress: 475/881
2025-07-07 10:16:42,783 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:42,783 - INFO - Processing question 476/881 (ID: 550) for fanar
2025-07-07 10:16:42,783 - INFO -   Ground truth verses: ['35:29', '14:31', '28:54', '13:22']
2025-07-07 10:16:42,783 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:42,945 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:42,946 - WARNING -   Failed to get response for question 550
2025-07-07 10:16:42,946 - INFO - Processing question 477/881 (ID: 551) for fanar
2025-07-07 10:16:42,946 - INFO -   Ground truth verses: ['56:38', '56:27', '56:91', '56:90', '90:18', '56:8', '20:17', '74:39']
2025-07-07 10:16:42,946 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:43,291 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:43,291 - WARNING -   Failed to get response for question 551
2025-07-07 10:16:43,291 - INFO - Processing question 478/881 (ID: 556) for fanar
2025-07-07 10:16:43,292 - INFO -   Ground truth verses: ['86:14']
2025-07-07 10:16:43,292 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:43,457 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:43,457 - WARNING -   Failed to get response for question 556
2025-07-07 10:16:43,457 - INFO - Processing question 479/881 (ID: 557) for fanar
2025-07-07 10:16:43,457 - INFO -   Ground truth verses: ['86:14']
2025-07-07 10:16:43,457 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:43,632 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:43,633 - WARNING -   Failed to get response for question 557
2025-07-07 10:16:43,633 - INFO - Processing question 480/881 (ID: 559) for fanar
2025-07-07 10:16:43,633 - INFO -   Ground truth verses: ['86:14']
2025-07-07 10:16:43,633 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:43,800 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:43,800 - WARNING -   Failed to get response for question 559
2025-07-07 10:16:43,818 - INFO - Saved data to results/fanar_checkpoint_479.json
2025-07-07 10:16:43,818 - INFO - Checkpoint saved for fanar at question 480
2025-07-07 10:16:43,818 - INFO -   Progress: 480/881
2025-07-07 10:16:43,818 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:43,818 - INFO - Processing question 481/881 (ID: 560) for fanar
2025-07-07 10:16:43,818 - INFO -   Ground truth verses: ['86:14']
2025-07-07 10:16:43,818 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:43,982 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:43,982 - WARNING -   Failed to get response for question 560
2025-07-07 10:16:43,983 - INFO - Processing question 482/881 (ID: 561) for fanar
2025-07-07 10:16:43,983 - INFO -   Ground truth verses: ['22:23', '35:33', '20:76', '18:31']
2025-07-07 10:16:43,983 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:44,146 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:44,147 - WARNING -   Failed to get response for question 561
2025-07-07 10:16:44,147 - INFO - Processing question 483/881 (ID: 562) for fanar
2025-07-07 10:16:44,147 - INFO -   Ground truth verses: ['22:23', '35:33', '20:76', '18:31']
2025-07-07 10:16:44,147 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:44,309 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:44,310 - WARNING -   Failed to get response for question 562
2025-07-07 10:16:44,310 - INFO - Processing question 484/881 (ID: 563) for fanar
2025-07-07 10:16:44,310 - INFO -   Ground truth verses: ['22:23', '35:33', '20:76', '18:31']
2025-07-07 10:16:44,310 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:44,470 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:44,471 - WARNING -   Failed to get response for question 563
2025-07-07 10:16:44,471 - INFO - Processing question 485/881 (ID: 564) for fanar
2025-07-07 10:16:44,471 - INFO -   Ground truth verses: ['22:23', '35:33', '20:76', '18:31']
2025-07-07 10:16:44,471 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:44,847 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:44,848 - WARNING -   Failed to get response for question 564
2025-07-07 10:16:44,864 - INFO - Saved data to results/fanar_checkpoint_484.json
2025-07-07 10:16:44,865 - INFO - Checkpoint saved for fanar at question 485
2025-07-07 10:16:44,865 - INFO -   Progress: 485/881
2025-07-07 10:16:44,865 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:44,865 - INFO - Processing question 486/881 (ID: 565) for fanar
2025-07-07 10:16:44,865 - INFO -   Ground truth verses: ['22:23', '35:33', '20:76', '18:31']
2025-07-07 10:16:44,865 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:45,028 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:45,029 - WARNING -   Failed to get response for question 565
2025-07-07 10:16:45,029 - INFO - Processing question 487/881 (ID: 566) for fanar
2025-07-07 10:16:45,029 - INFO -   Ground truth verses: ['31:30', '22:6', '22:62']
2025-07-07 10:16:45,029 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:45,190 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:45,190 - WARNING -   Failed to get response for question 566
2025-07-07 10:16:45,190 - INFO - Processing question 488/881 (ID: 567) for fanar
2025-07-07 10:16:45,190 - INFO -   Ground truth verses: ['31:30', '22:6', '22:62']
2025-07-07 10:16:45,190 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:45,352 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:45,353 - WARNING -   Failed to get response for question 567
2025-07-07 10:16:45,353 - INFO - Processing question 489/881 (ID: 568) for fanar
2025-07-07 10:16:45,353 - INFO -   Ground truth verses: ['31:30', '22:6', '22:62']
2025-07-07 10:16:45,353 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:45,515 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:45,515 - WARNING -   Failed to get response for question 568
2025-07-07 10:16:45,515 - INFO - Processing question 490/881 (ID: 570) for fanar
2025-07-07 10:16:45,515 - INFO -   Ground truth verses: ['31:30', '22:6', '22:62']
2025-07-07 10:16:45,515 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:45,680 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:45,681 - WARNING -   Failed to get response for question 570
2025-07-07 10:16:45,698 - INFO - Saved data to results/fanar_checkpoint_489.json
2025-07-07 10:16:45,698 - INFO - Checkpoint saved for fanar at question 490
2025-07-07 10:16:45,698 - INFO -   Progress: 490/881
2025-07-07 10:16:45,698 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:45,698 - INFO - Processing question 491/881 (ID: 571) for fanar
2025-07-07 10:16:45,698 - INFO -   Ground truth verses: ['34:14']
2025-07-07 10:16:45,698 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:45,860 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:45,861 - WARNING -   Failed to get response for question 571
2025-07-07 10:16:45,861 - INFO - Processing question 492/881 (ID: 572) for fanar
2025-07-07 10:16:45,861 - INFO -   Ground truth verses: ['34:14']
2025-07-07 10:16:45,861 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:46,026 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:46,026 - WARNING -   Failed to get response for question 572
2025-07-07 10:16:46,026 - INFO - Processing question 493/881 (ID: 573) for fanar
2025-07-07 10:16:46,026 - INFO -   Ground truth verses: ['34:14']
2025-07-07 10:16:46,026 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:46,371 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:46,371 - WARNING -   Failed to get response for question 573
2025-07-07 10:16:46,371 - INFO - Processing question 494/881 (ID: 574) for fanar
2025-07-07 10:16:46,371 - INFO -   Ground truth verses: ['34:14']
2025-07-07 10:16:46,371 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:46,536 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:46,536 - WARNING -   Failed to get response for question 574
2025-07-07 10:16:46,537 - INFO - Processing question 495/881 (ID: 575) for fanar
2025-07-07 10:16:46,537 - INFO -   Ground truth verses: ['34:14']
2025-07-07 10:16:46,537 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:46,701 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:46,701 - WARNING -   Failed to get response for question 575
2025-07-07 10:16:46,719 - INFO - Saved data to results/fanar_checkpoint_494.json
2025-07-07 10:16:46,719 - INFO - Checkpoint saved for fanar at question 495
2025-07-07 10:16:46,719 - INFO -   Progress: 495/881
2025-07-07 10:16:46,719 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:46,719 - INFO - Processing question 496/881 (ID: 576) for fanar
2025-07-07 10:16:46,719 - INFO -   Ground truth verses: ['6:119', '6:121', '6:118']
2025-07-07 10:16:46,719 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:46,887 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:46,888 - WARNING -   Failed to get response for question 576
2025-07-07 10:16:46,888 - INFO - Processing question 497/881 (ID: 577) for fanar
2025-07-07 10:16:46,888 - INFO -   Ground truth verses: ['6:119', '6:121', '6:118']
2025-07-07 10:16:46,888 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:47,051 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:47,052 - WARNING -   Failed to get response for question 577
2025-07-07 10:16:47,052 - INFO - Processing question 498/881 (ID: 578) for fanar
2025-07-07 10:16:47,052 - INFO -   Ground truth verses: ['6:119', '6:121', '6:118']
2025-07-07 10:16:47,052 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:47,219 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:47,220 - WARNING -   Failed to get response for question 578
2025-07-07 10:16:47,220 - INFO - Processing question 499/881 (ID: 579) for fanar
2025-07-07 10:16:47,220 - INFO -   Ground truth verses: ['6:119', '6:121', '6:118']
2025-07-07 10:16:47,220 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:47,382 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:47,383 - WARNING -   Failed to get response for question 579
2025-07-07 10:16:47,383 - INFO - Processing question 500/881 (ID: 580) for fanar
2025-07-07 10:16:47,383 - INFO -   Ground truth verses: ['6:119', '6:121', '6:118']
2025-07-07 10:16:47,383 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:47,547 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:47,548 - WARNING -   Failed to get response for question 580
2025-07-07 10:16:47,566 - INFO - Saved data to results/fanar_checkpoint_499.json
2025-07-07 10:16:47,566 - INFO - Checkpoint saved for fanar at question 500
2025-07-07 10:16:47,566 - INFO -   Progress: 500/881
2025-07-07 10:16:47,566 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:47,566 - INFO - Processing question 501/881 (ID: 581) for fanar
2025-07-07 10:16:47,566 - INFO -   Ground truth verses: ['28:66']
2025-07-07 10:16:47,566 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:47,922 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:47,922 - WARNING -   Failed to get response for question 581
2025-07-07 10:16:47,923 - INFO - Processing question 502/881 (ID: 582) for fanar
2025-07-07 10:16:47,923 - INFO -   Ground truth verses: ['28:66']
2025-07-07 10:16:47,923 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:48,084 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:48,085 - WARNING -   Failed to get response for question 582
2025-07-07 10:16:48,085 - INFO - Processing question 503/881 (ID: 583) for fanar
2025-07-07 10:16:48,085 - INFO -   Ground truth verses: ['28:66']
2025-07-07 10:16:48,085 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:48,245 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:48,246 - WARNING -   Failed to get response for question 583
2025-07-07 10:16:48,246 - INFO - Processing question 504/881 (ID: 585) for fanar
2025-07-07 10:16:48,246 - INFO -   Ground truth verses: ['28:66']
2025-07-07 10:16:48,246 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:48,410 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:48,411 - WARNING -   Failed to get response for question 585
2025-07-07 10:16:48,411 - INFO - Processing question 505/881 (ID: 586) for fanar
2025-07-07 10:16:48,411 - INFO -   Ground truth verses: ['3:181']
2025-07-07 10:16:48,411 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:48,573 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:48,573 - WARNING -   Failed to get response for question 586
2025-07-07 10:16:48,591 - INFO - Saved data to results/fanar_checkpoint_504.json
2025-07-07 10:16:48,591 - INFO - Checkpoint saved for fanar at question 505
2025-07-07 10:16:48,591 - INFO -   Progress: 505/881
2025-07-07 10:16:48,591 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:48,591 - INFO - Processing question 506/881 (ID: 587) for fanar
2025-07-07 10:16:48,591 - INFO -   Ground truth verses: ['3:181']
2025-07-07 10:16:48,591 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:48,752 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:48,752 - WARNING -   Failed to get response for question 587
2025-07-07 10:16:48,752 - INFO - Processing question 507/881 (ID: 588) for fanar
2025-07-07 10:16:48,752 - INFO -   Ground truth verses: ['3:181']
2025-07-07 10:16:48,752 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:48,915 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:48,916 - WARNING -   Failed to get response for question 588
2025-07-07 10:16:48,916 - INFO - Processing question 508/881 (ID: 589) for fanar
2025-07-07 10:16:48,916 - INFO -   Ground truth verses: ['3:181']
2025-07-07 10:16:48,916 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:49,259 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:49,259 - WARNING -   Failed to get response for question 589
2025-07-07 10:16:49,260 - INFO - Processing question 509/881 (ID: 590) for fanar
2025-07-07 10:16:49,260 - INFO -   Ground truth verses: ['3:181']
2025-07-07 10:16:49,260 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:49,426 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:49,426 - WARNING -   Failed to get response for question 590
2025-07-07 10:16:49,426 - INFO - Processing question 510/881 (ID: 591) for fanar
2025-07-07 10:16:49,426 - INFO -   Ground truth verses: ['32:5', '70:4']
2025-07-07 10:16:49,426 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:49,599 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:49,600 - WARNING -   Failed to get response for question 591
2025-07-07 10:16:49,618 - INFO - Saved data to results/fanar_checkpoint_509.json
2025-07-07 10:16:49,618 - INFO - Checkpoint saved for fanar at question 510
2025-07-07 10:16:49,618 - INFO -   Progress: 510/881
2025-07-07 10:16:49,618 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:49,618 - INFO - Processing question 511/881 (ID: 592) for fanar
2025-07-07 10:16:49,618 - INFO -   Ground truth verses: ['32:5', '70:4']
2025-07-07 10:16:49,618 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:49,781 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:49,782 - WARNING -   Failed to get response for question 592
2025-07-07 10:16:49,782 - INFO - Processing question 512/881 (ID: 593) for fanar
2025-07-07 10:16:49,782 - INFO -   Ground truth verses: ['32:5', '70:4']
2025-07-07 10:16:49,782 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:49,946 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:49,946 - WARNING -   Failed to get response for question 593
2025-07-07 10:16:49,947 - INFO - Processing question 513/881 (ID: 595) for fanar
2025-07-07 10:16:49,947 - INFO -   Ground truth verses: ['32:5', '70:4']
2025-07-07 10:16:49,947 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:50,109 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:50,110 - WARNING -   Failed to get response for question 595
2025-07-07 10:16:50,110 - INFO - Processing question 514/881 (ID: 596) for fanar
2025-07-07 10:16:50,110 - INFO -   Ground truth verses: ['79:21']
2025-07-07 10:16:50,110 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:50,273 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:50,273 - WARNING -   Failed to get response for question 596
2025-07-07 10:16:50,273 - INFO - Processing question 515/881 (ID: 597) for fanar
2025-07-07 10:16:50,273 - INFO -   Ground truth verses: ['79:21']
2025-07-07 10:16:50,273 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:50,437 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:50,438 - WARNING -   Failed to get response for question 597
2025-07-07 10:16:50,457 - INFO - Saved data to results/fanar_checkpoint_514.json
2025-07-07 10:16:50,457 - INFO - Checkpoint saved for fanar at question 515
2025-07-07 10:16:50,457 - INFO -   Progress: 515/881
2025-07-07 10:16:50,457 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:50,457 - INFO - Processing question 516/881 (ID: 600) for fanar
2025-07-07 10:16:50,457 - INFO -   Ground truth verses: ['79:21']
2025-07-07 10:16:50,457 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:50,812 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:50,813 - WARNING -   Failed to get response for question 600
2025-07-07 10:16:50,813 - INFO - Processing question 517/881 (ID: 601) for fanar
2025-07-07 10:16:50,813 - INFO -   Ground truth verses: ['101:1', '101:3', '69:4', '101:2']
2025-07-07 10:16:50,813 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:50,978 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:50,979 - WARNING -   Failed to get response for question 601
2025-07-07 10:16:50,979 - INFO - Processing question 518/881 (ID: 603) for fanar
2025-07-07 10:16:50,979 - INFO -   Ground truth verses: ['101:1', '101:3', '69:4', '101:2']
2025-07-07 10:16:50,979 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:51,143 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:51,144 - WARNING -   Failed to get response for question 603
2025-07-07 10:16:51,144 - INFO - Processing question 519/881 (ID: 604) for fanar
2025-07-07 10:16:51,144 - INFO -   Ground truth verses: ['101:1', '101:3', '69:4', '101:2']
2025-07-07 10:16:51,144 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:51,307 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:51,308 - WARNING -   Failed to get response for question 604
2025-07-07 10:16:51,308 - INFO - Processing question 520/881 (ID: 605) for fanar
2025-07-07 10:16:51,308 - INFO -   Ground truth verses: ['101:1', '101:3', '69:4', '101:2']
2025-07-07 10:16:51,308 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:51,470 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:51,471 - WARNING -   Failed to get response for question 605
2025-07-07 10:16:51,489 - INFO - Saved data to results/fanar_checkpoint_519.json
2025-07-07 10:16:51,489 - INFO - Checkpoint saved for fanar at question 520
2025-07-07 10:16:51,489 - INFO -   Progress: 520/881
2025-07-07 10:16:51,489 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:51,489 - INFO - Processing question 521/881 (ID: 606) for fanar
2025-07-07 10:16:51,489 - INFO -   Ground truth verses: ['55:48']
2025-07-07 10:16:51,489 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:51,656 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:51,657 - WARNING -   Failed to get response for question 606
2025-07-07 10:16:51,657 - INFO - Processing question 522/881 (ID: 609) for fanar
2025-07-07 10:16:51,657 - INFO -   Ground truth verses: ['55:48']
2025-07-07 10:16:51,657 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:51,821 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:51,822 - WARNING -   Failed to get response for question 609
2025-07-07 10:16:51,822 - INFO - Processing question 523/881 (ID: 610) for fanar
2025-07-07 10:16:51,822 - INFO -   Ground truth verses: ['55:48']
2025-07-07 10:16:51,822 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:51,984 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:51,985 - WARNING -   Failed to get response for question 610
2025-07-07 10:16:51,985 - INFO - Processing question 524/881 (ID: 611) for fanar
2025-07-07 10:16:51,985 - INFO -   Ground truth verses: ['82:15', '36:64']
2025-07-07 10:16:51,985 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:52,328 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:52,329 - WARNING -   Failed to get response for question 611
2025-07-07 10:16:52,329 - INFO - Processing question 525/881 (ID: 612) for fanar
2025-07-07 10:16:52,329 - INFO -   Ground truth verses: ['82:15', '36:64']
2025-07-07 10:16:52,329 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:52,492 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:52,493 - WARNING -   Failed to get response for question 612
2025-07-07 10:16:52,511 - INFO - Saved data to results/fanar_checkpoint_524.json
2025-07-07 10:16:52,511 - INFO - Checkpoint saved for fanar at question 525
2025-07-07 10:16:52,511 - INFO -   Progress: 525/881
2025-07-07 10:16:52,511 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:52,511 - INFO - Processing question 526/881 (ID: 614) for fanar
2025-07-07 10:16:52,511 - INFO -   Ground truth verses: ['82:15', '36:64']
2025-07-07 10:16:52,511 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:52,678 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:52,679 - WARNING -   Failed to get response for question 614
2025-07-07 10:16:52,679 - INFO - Processing question 527/881 (ID: 616) for fanar
2025-07-07 10:16:52,679 - INFO -   Ground truth verses: ['5:108']
2025-07-07 10:16:52,679 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:52,843 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:52,844 - WARNING -   Failed to get response for question 616
2025-07-07 10:16:52,844 - INFO - Processing question 528/881 (ID: 617) for fanar
2025-07-07 10:16:52,844 - INFO -   Ground truth verses: ['5:108']
2025-07-07 10:16:52,844 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:53,006 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:53,006 - WARNING -   Failed to get response for question 617
2025-07-07 10:16:53,006 - INFO - Processing question 529/881 (ID: 619) for fanar
2025-07-07 10:16:53,006 - INFO -   Ground truth verses: ['5:108']
2025-07-07 10:16:53,006 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:53,167 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:53,167 - WARNING -   Failed to get response for question 619
2025-07-07 10:16:53,167 - INFO - Processing question 530/881 (ID: 620) for fanar
2025-07-07 10:16:53,167 - INFO -   Ground truth verses: ['5:108']
2025-07-07 10:16:53,167 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:53,329 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:53,330 - WARNING -   Failed to get response for question 620
2025-07-07 10:16:53,348 - INFO - Saved data to results/fanar_checkpoint_529.json
2025-07-07 10:16:53,348 - INFO - Checkpoint saved for fanar at question 530
2025-07-07 10:16:53,348 - INFO -   Progress: 530/881
2025-07-07 10:16:53,348 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:53,348 - INFO - Processing question 531/881 (ID: 621) for fanar
2025-07-07 10:16:53,348 - INFO -   Ground truth verses: ['40:66', '6:56']
2025-07-07 10:16:53,348 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:53,515 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:53,515 - WARNING -   Failed to get response for question 621
2025-07-07 10:16:53,515 - INFO - Processing question 532/881 (ID: 622) for fanar
2025-07-07 10:16:53,515 - INFO -   Ground truth verses: ['40:66', '6:56']
2025-07-07 10:16:53,515 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:53,866 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:53,867 - WARNING -   Failed to get response for question 622
2025-07-07 10:16:53,867 - INFO - Processing question 533/881 (ID: 623) for fanar
2025-07-07 10:16:53,867 - INFO -   Ground truth verses: ['40:66', '6:56']
2025-07-07 10:16:53,867 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:54,031 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:54,031 - WARNING -   Failed to get response for question 623
2025-07-07 10:16:54,031 - INFO - Processing question 534/881 (ID: 624) for fanar
2025-07-07 10:16:54,032 - INFO -   Ground truth verses: ['40:66', '6:56']
2025-07-07 10:16:54,032 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:54,191 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:54,192 - WARNING -   Failed to get response for question 624
2025-07-07 10:16:54,192 - INFO - Processing question 535/881 (ID: 625) for fanar
2025-07-07 10:16:54,192 - INFO -   Ground truth verses: ['40:66', '6:56']
2025-07-07 10:16:54,192 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:54,355 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:54,355 - WARNING -   Failed to get response for question 625
2025-07-07 10:16:54,374 - INFO - Saved data to results/fanar_checkpoint_534.json
2025-07-07 10:16:54,374 - INFO - Checkpoint saved for fanar at question 535
2025-07-07 10:16:54,374 - INFO -   Progress: 535/881
2025-07-07 10:16:54,374 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:54,374 - INFO - Processing question 536/881 (ID: 626) for fanar
2025-07-07 10:16:54,374 - INFO -   Ground truth verses: ['23:58']
2025-07-07 10:16:54,374 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:54,538 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:54,539 - WARNING -   Failed to get response for question 626
2025-07-07 10:16:54,539 - INFO - Processing question 537/881 (ID: 627) for fanar
2025-07-07 10:16:54,539 - INFO -   Ground truth verses: ['23:58']
2025-07-07 10:16:54,539 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:54,706 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:54,706 - WARNING -   Failed to get response for question 627
2025-07-07 10:16:54,706 - INFO - Processing question 538/881 (ID: 628) for fanar
2025-07-07 10:16:54,706 - INFO -   Ground truth verses: ['23:58']
2025-07-07 10:16:54,706 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:54,873 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:54,873 - WARNING -   Failed to get response for question 628
2025-07-07 10:16:54,873 - INFO - Processing question 539/881 (ID: 629) for fanar
2025-07-07 10:16:54,873 - INFO -   Ground truth verses: ['23:58']
2025-07-07 10:16:54,873 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:55,038 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:55,039 - WARNING -   Failed to get response for question 629
2025-07-07 10:16:55,039 - INFO - Processing question 540/881 (ID: 630) for fanar
2025-07-07 10:16:55,039 - INFO -   Ground truth verses: ['23:58']
2025-07-07 10:16:55,039 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:55,396 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:55,396 - WARNING -   Failed to get response for question 630
2025-07-07 10:16:55,415 - INFO - Saved data to results/fanar_checkpoint_539.json
2025-07-07 10:16:55,415 - INFO - Checkpoint saved for fanar at question 540
2025-07-07 10:16:55,415 - INFO -   Progress: 540/881
2025-07-07 10:16:55,415 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:55,415 - INFO - Processing question 541/881 (ID: 631) for fanar
2025-07-07 10:16:55,415 - INFO -   Ground truth verses: ['12:63', '12:81']
2025-07-07 10:16:55,415 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:55,583 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:55,583 - WARNING -   Failed to get response for question 631
2025-07-07 10:16:55,583 - INFO - Processing question 542/881 (ID: 632) for fanar
2025-07-07 10:16:55,583 - INFO -   Ground truth verses: ['12:63', '12:81']
2025-07-07 10:16:55,583 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:55,748 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:55,749 - WARNING -   Failed to get response for question 632
2025-07-07 10:16:55,749 - INFO - Processing question 543/881 (ID: 633) for fanar
2025-07-07 10:16:55,749 - INFO -   Ground truth verses: ['12:63', '12:81']
2025-07-07 10:16:55,749 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:55,913 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:55,913 - WARNING -   Failed to get response for question 633
2025-07-07 10:16:55,913 - INFO - Processing question 544/881 (ID: 634) for fanar
2025-07-07 10:16:55,913 - INFO -   Ground truth verses: ['12:63', '12:81']
2025-07-07 10:16:55,913 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:56,077 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:56,078 - WARNING -   Failed to get response for question 634
2025-07-07 10:16:56,078 - INFO - Processing question 545/881 (ID: 635) for fanar
2025-07-07 10:16:56,078 - INFO -   Ground truth verses: ['12:63', '12:81']
2025-07-07 10:16:56,078 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:56,241 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:56,241 - WARNING -   Failed to get response for question 635
2025-07-07 10:16:56,260 - INFO - Saved data to results/fanar_checkpoint_544.json
2025-07-07 10:16:56,260 - INFO - Checkpoint saved for fanar at question 545
2025-07-07 10:16:56,260 - INFO -   Progress: 545/881
2025-07-07 10:16:56,260 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:56,260 - INFO - Processing question 546/881 (ID: 636) for fanar
2025-07-07 10:16:56,260 - INFO -   Ground truth verses: ['98:2']
2025-07-07 10:16:56,260 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:56,422 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:56,423 - WARNING -   Failed to get response for question 636
2025-07-07 10:16:56,423 - INFO - Processing question 547/881 (ID: 639) for fanar
2025-07-07 10:16:56,423 - INFO -   Ground truth verses: ['98:2']
2025-07-07 10:16:56,423 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:56,586 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:56,587 - WARNING -   Failed to get response for question 639
2025-07-07 10:16:56,587 - INFO - Processing question 548/881 (ID: 641) for fanar
2025-07-07 10:16:56,587 - INFO -   Ground truth verses: ['37:156', '44:19', '27:21']
2025-07-07 10:16:56,587 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:56,934 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:56,934 - WARNING -   Failed to get response for question 641
2025-07-07 10:16:56,935 - INFO - Processing question 549/881 (ID: 646) for fanar
2025-07-07 10:16:56,935 - INFO -   Ground truth verses: ['25:48', '7:57', '6:97', '27:63']
2025-07-07 10:16:56,935 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:57,100 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:57,101 - WARNING -   Failed to get response for question 646
2025-07-07 10:16:57,101 - INFO - Processing question 550/881 (ID: 647) for fanar
2025-07-07 10:16:57,101 - INFO -   Ground truth verses: ['25:48', '7:57', '6:97', '27:63']
2025-07-07 10:16:57,101 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:57,265 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:57,265 - WARNING -   Failed to get response for question 647
2025-07-07 10:16:57,284 - INFO - Saved data to results/fanar_checkpoint_549.json
2025-07-07 10:16:57,284 - INFO - Checkpoint saved for fanar at question 550
2025-07-07 10:16:57,284 - INFO -   Progress: 550/881
2025-07-07 10:16:57,284 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:57,284 - INFO - Processing question 551/881 (ID: 648) for fanar
2025-07-07 10:16:57,284 - INFO -   Ground truth verses: ['25:48', '7:57', '6:97', '27:63']
2025-07-07 10:16:57,284 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:57,446 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:57,447 - WARNING -   Failed to get response for question 648
2025-07-07 10:16:57,447 - INFO - Processing question 552/881 (ID: 649) for fanar
2025-07-07 10:16:57,447 - INFO -   Ground truth verses: ['25:48', '7:57', '6:97', '27:63']
2025-07-07 10:16:57,447 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:57,610 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:57,611 - WARNING -   Failed to get response for question 649
2025-07-07 10:16:57,611 - INFO - Processing question 553/881 (ID: 650) for fanar
2025-07-07 10:16:57,611 - INFO -   Ground truth verses: ['25:48', '7:57', '6:97', '27:63']
2025-07-07 10:16:57,611 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:57,775 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:57,776 - WARNING -   Failed to get response for question 650
2025-07-07 10:16:57,776 - INFO - Processing question 554/881 (ID: 651) for fanar
2025-07-07 10:16:57,776 - INFO -   Ground truth verses: ['12:27', '12:26']
2025-07-07 10:16:57,776 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:57,940 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:57,940 - WARNING -   Failed to get response for question 651
2025-07-07 10:16:57,940 - INFO - Processing question 555/881 (ID: 652) for fanar
2025-07-07 10:16:57,940 - INFO -   Ground truth verses: ['12:27', '12:26']
2025-07-07 10:16:57,940 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:58,103 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:58,104 - WARNING -   Failed to get response for question 652
2025-07-07 10:16:58,123 - INFO - Saved data to results/fanar_checkpoint_554.json
2025-07-07 10:16:58,123 - INFO - Checkpoint saved for fanar at question 555
2025-07-07 10:16:58,123 - INFO -   Progress: 555/881
2025-07-07 10:16:58,123 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:58,123 - INFO - Processing question 556/881 (ID: 653) for fanar
2025-07-07 10:16:58,123 - INFO -   Ground truth verses: ['12:27', '12:26']
2025-07-07 10:16:58,123 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:58,472 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:58,473 - WARNING -   Failed to get response for question 653
2025-07-07 10:16:58,473 - INFO - Processing question 557/881 (ID: 655) for fanar
2025-07-07 10:16:58,473 - INFO -   Ground truth verses: ['12:27', '12:26']
2025-07-07 10:16:58,473 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:58,639 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:58,640 - WARNING -   Failed to get response for question 655
2025-07-07 10:16:58,640 - INFO - Processing question 558/881 (ID: 656) for fanar
2025-07-07 10:16:58,640 - INFO -   Ground truth verses: ['37:15', '10:76', '43:30', '46:7', '27:13']
2025-07-07 10:16:58,640 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:58,804 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:58,805 - WARNING -   Failed to get response for question 656
2025-07-07 10:16:58,805 - INFO - Processing question 559/881 (ID: 657) for fanar
2025-07-07 10:16:58,805 - INFO -   Ground truth verses: ['37:15', '10:76', '43:30', '46:7', '27:13']
2025-07-07 10:16:58,805 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:58,971 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:58,971 - WARNING -   Failed to get response for question 657
2025-07-07 10:16:58,971 - INFO - Processing question 560/881 (ID: 659) for fanar
2025-07-07 10:16:58,971 - INFO -   Ground truth verses: ['37:15', '10:76', '43:30', '46:7', '27:13']
2025-07-07 10:16:58,972 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:59,139 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:59,139 - WARNING -   Failed to get response for question 659
2025-07-07 10:16:59,159 - INFO - Saved data to results/fanar_checkpoint_559.json
2025-07-07 10:16:59,159 - INFO - Checkpoint saved for fanar at question 560
2025-07-07 10:16:59,159 - INFO -   Progress: 560/881
2025-07-07 10:16:59,159 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:16:59,159 - INFO - Processing question 561/881 (ID: 661) for fanar
2025-07-07 10:16:59,159 - INFO -   Ground truth verses: ['12:82']
2025-07-07 10:16:59,159 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:59,320 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:59,321 - WARNING -   Failed to get response for question 661
2025-07-07 10:16:59,321 - INFO - Processing question 562/881 (ID: 663) for fanar
2025-07-07 10:16:59,321 - INFO -   Ground truth verses: ['12:82']
2025-07-07 10:16:59,321 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:59,484 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:59,485 - WARNING -   Failed to get response for question 663
2025-07-07 10:16:59,485 - INFO - Processing question 563/881 (ID: 664) for fanar
2025-07-07 10:16:59,485 - INFO -   Ground truth verses: ['12:82']
2025-07-07 10:16:59,485 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:16:59,650 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:16:59,651 - WARNING -   Failed to get response for question 664
2025-07-07 10:16:59,651 - INFO - Processing question 564/881 (ID: 666) for fanar
2025-07-07 10:16:59,651 - INFO -   Ground truth verses: ['84:16', '81:15', '69:39', '90:1', '75:1', '75:2', '69:38']
2025-07-07 10:16:59,651 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:00,011 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:00,011 - WARNING -   Failed to get response for question 666
2025-07-07 10:17:00,011 - INFO - Processing question 565/881 (ID: 668) for fanar
2025-07-07 10:17:00,011 - INFO -   Ground truth verses: ['84:16', '81:15', '69:39', '90:1', '75:1', '75:2', '69:38']
2025-07-07 10:17:00,011 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:00,111 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:17:00,112 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:17:00,112 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:17:00,112 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:17:00,112 - INFO - Normalized predicted verses: []
2025-07-07 10:17:00,112 - INFO - Normalized ground truth verses: ['37:54']
2025-07-07 10:17:00,112 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 10:17:00,112 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:00,112 - INFO -   Matched verses: []
2025-07-07 10:17:00,135 - INFO - Saved data to results/qwen_checkpoint_299.json
2025-07-07 10:17:00,135 - INFO - Checkpoint saved for qwen at question 300
2025-07-07 10:17:00,135 - INFO -   Progress: 300/881
2025-07-07 10:17:00,135 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.003
2025-07-07 10:17:00,175 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:00,176 - WARNING -   Failed to get response for question 668
2025-07-07 10:17:00,195 - INFO - Saved data to results/fanar_checkpoint_564.json
2025-07-07 10:17:00,195 - INFO - Checkpoint saved for fanar at question 565
2025-07-07 10:17:00,195 - INFO -   Progress: 565/881
2025-07-07 10:17:00,195 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:00,195 - INFO - Processing question 566/881 (ID: 671) for fanar
2025-07-07 10:17:00,195 - INFO -   Ground truth verses: ['38:33']
2025-07-07 10:17:00,195 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:00,331 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:00,332 - WARNING -   Failed to get response for question 671
2025-07-07 10:17:00,332 - INFO - Processing question 567/881 (ID: 672) for fanar
2025-07-07 10:17:00,332 - INFO -   Ground truth verses: ['38:33']
2025-07-07 10:17:00,332 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:00,468 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:00,468 - WARNING -   Failed to get response for question 672
2025-07-07 10:17:00,468 - INFO - Processing question 568/881 (ID: 673) for fanar
2025-07-07 10:17:00,469 - INFO -   Ground truth verses: ['38:33']
2025-07-07 10:17:00,469 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:00,604 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:00,605 - WARNING -   Failed to get response for question 673
2025-07-07 10:17:00,605 - INFO - Processing question 569/881 (ID: 676) for fanar
2025-07-07 10:17:00,605 - INFO -   Ground truth verses: ['36:44', '37:148', '21:111']
2025-07-07 10:17:00,605 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:00,746 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:00,747 - WARNING -   Failed to get response for question 676
2025-07-07 10:17:00,747 - INFO - Processing question 570/881 (ID: 677) for fanar
2025-07-07 10:17:00,747 - INFO -   Ground truth verses: ['36:44', '37:148', '21:111']
2025-07-07 10:17:00,747 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:00,884 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:00,884 - WARNING -   Failed to get response for question 677
2025-07-07 10:17:00,904 - INFO - Saved data to results/fanar_checkpoint_569.json
2025-07-07 10:17:00,904 - INFO - Checkpoint saved for fanar at question 570
2025-07-07 10:17:00,904 - INFO -   Progress: 570/881
2025-07-07 10:17:00,904 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:00,904 - INFO - Processing question 571/881 (ID: 678) for fanar
2025-07-07 10:17:00,904 - INFO -   Ground truth verses: ['36:44', '37:148', '21:111']
2025-07-07 10:17:00,904 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:01,040 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:01,041 - WARNING -   Failed to get response for question 678
2025-07-07 10:17:01,041 - INFO - Processing question 572/881 (ID: 679) for fanar
2025-07-07 10:17:01,041 - INFO -   Ground truth verses: ['36:44', '37:148', '21:111']
2025-07-07 10:17:01,041 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:01,137 - INFO - Processing question 301/881 (ID: 341) for qwen
2025-07-07 10:17:01,137 - INFO -   Ground truth verses: ['6:5', '43:7', '15:11', '16:34', '26:6']
2025-07-07 10:17:01,137 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:17:01,376 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:01,376 - WARNING -   Failed to get response for question 679
2025-07-07 10:17:01,376 - INFO - Processing question 573/881 (ID: 680) for fanar
2025-07-07 10:17:01,376 - INFO -   Ground truth verses: ['36:44', '37:148', '21:111']
2025-07-07 10:17:01,376 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:01,538 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:01,539 - WARNING -   Failed to get response for question 680
2025-07-07 10:17:01,539 - INFO - Processing question 574/881 (ID: 681) for fanar
2025-07-07 10:17:01,539 - INFO -   Ground truth verses: ['26:71']
2025-07-07 10:17:01,539 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:01,704 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:01,704 - WARNING -   Failed to get response for question 681
2025-07-07 10:17:01,704 - INFO - Processing question 575/881 (ID: 682) for fanar
2025-07-07 10:17:01,704 - INFO -   Ground truth verses: ['26:71']
2025-07-07 10:17:01,704 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:01,866 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:01,866 - WARNING -   Failed to get response for question 682
2025-07-07 10:17:01,886 - INFO - Saved data to results/fanar_checkpoint_574.json
2025-07-07 10:17:01,886 - INFO - Checkpoint saved for fanar at question 575
2025-07-07 10:17:01,886 - INFO -   Progress: 575/881
2025-07-07 10:17:01,886 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:01,886 - INFO - Processing question 576/881 (ID: 684) for fanar
2025-07-07 10:17:01,886 - INFO -   Ground truth verses: ['26:71']
2025-07-07 10:17:01,886 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:02,050 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:02,050 - WARNING -   Failed to get response for question 684
2025-07-07 10:17:02,051 - INFO - Processing question 577/881 (ID: 685) for fanar
2025-07-07 10:17:02,051 - INFO -   Ground truth verses: ['26:71']
2025-07-07 10:17:02,051 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:02,211 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:02,212 - WARNING -   Failed to get response for question 685
2025-07-07 10:17:02,212 - INFO - Processing question 578/881 (ID: 686) for fanar
2025-07-07 10:17:02,212 - INFO -   Ground truth verses: ['26:191', '26:175', '26:159', '26:140', '26:122', '26:104', '26:9', '36:5', '26:217', '44:42', '32:6', '26:68']
2025-07-07 10:17:02,212 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:02,375 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:02,376 - WARNING -   Failed to get response for question 686
2025-07-07 10:17:02,376 - INFO - Processing question 579/881 (ID: 687) for fanar
2025-07-07 10:17:02,376 - INFO -   Ground truth verses: ['26:191', '26:175', '26:159', '26:140', '26:122', '26:104', '26:9', '36:5', '26:217', '44:42', '32:6', '26:68']
2025-07-07 10:17:02,376 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:02,539 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:02,539 - WARNING -   Failed to get response for question 687
2025-07-07 10:17:02,539 - INFO - Processing question 580/881 (ID: 688) for fanar
2025-07-07 10:17:02,539 - INFO -   Ground truth verses: ['26:191', '26:175', '26:159', '26:140', '26:122', '26:104', '26:9', '36:5', '26:217', '44:42', '32:6', '26:68']
2025-07-07 10:17:02,539 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:02,893 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:02,894 - WARNING -   Failed to get response for question 688
2025-07-07 10:17:02,913 - INFO - Saved data to results/fanar_checkpoint_579.json
2025-07-07 10:17:02,913 - INFO - Checkpoint saved for fanar at question 580
2025-07-07 10:17:02,913 - INFO -   Progress: 580/881
2025-07-07 10:17:02,913 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:02,913 - INFO - Processing question 581/881 (ID: 690) for fanar
2025-07-07 10:17:02,913 - INFO -   Ground truth verses: ['26:191', '26:175', '26:159', '26:140', '26:122', '26:104', '26:9', '36:5', '26:217', '44:42', '32:6', '26:68']
2025-07-07 10:17:02,913 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:03,077 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:03,077 - WARNING -   Failed to get response for question 690
2025-07-07 10:17:03,077 - INFO - Processing question 582/881 (ID: 691) for fanar
2025-07-07 10:17:03,077 - INFO -   Ground truth verses: ['80:21']
2025-07-07 10:17:03,077 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:03,242 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:03,242 - WARNING -   Failed to get response for question 691
2025-07-07 10:17:03,242 - INFO - Processing question 583/881 (ID: 692) for fanar
2025-07-07 10:17:03,242 - INFO -   Ground truth verses: ['80:21']
2025-07-07 10:17:03,242 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:03,403 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:03,404 - WARNING -   Failed to get response for question 692
2025-07-07 10:17:03,404 - INFO - Processing question 584/881 (ID: 693) for fanar
2025-07-07 10:17:03,404 - INFO -   Ground truth verses: ['80:21']
2025-07-07 10:17:03,404 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:03,567 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:03,568 - WARNING -   Failed to get response for question 693
2025-07-07 10:17:03,568 - INFO - Processing question 585/881 (ID: 695) for fanar
2025-07-07 10:17:03,568 - INFO -   Ground truth verses: ['80:21']
2025-07-07 10:17:03,568 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:03,734 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:03,735 - WARNING -   Failed to get response for question 695
2025-07-07 10:17:03,755 - INFO - Saved data to results/fanar_checkpoint_584.json
2025-07-07 10:17:03,755 - INFO - Checkpoint saved for fanar at question 585
2025-07-07 10:17:03,755 - INFO -   Progress: 585/881
2025-07-07 10:17:03,755 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:03,755 - INFO - Processing question 586/881 (ID: 696) for fanar
2025-07-07 10:17:03,755 - INFO -   Ground truth verses: ['38:79', '7:14', '37:144', '26:87', '15:36']
2025-07-07 10:17:03,755 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:03,919 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:03,919 - WARNING -   Failed to get response for question 696
2025-07-07 10:17:03,920 - INFO - Processing question 587/881 (ID: 697) for fanar
2025-07-07 10:17:03,920 - INFO -   Ground truth verses: ['38:79', '7:14', '37:144', '26:87', '15:36']
2025-07-07 10:17:03,920 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:04,085 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:04,085 - WARNING -   Failed to get response for question 697
2025-07-07 10:17:04,085 - INFO - Processing question 588/881 (ID: 700) for fanar
2025-07-07 10:17:04,085 - INFO -   Ground truth verses: ['38:79', '7:14', '37:144', '26:87', '15:36']
2025-07-07 10:17:04,085 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:04,436 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:04,436 - WARNING -   Failed to get response for question 700
2025-07-07 10:17:04,436 - INFO - Processing question 589/881 (ID: 701) for fanar
2025-07-07 10:17:04,436 - INFO -   Ground truth verses: ['81:6', '82:3']
2025-07-07 10:17:04,436 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:04,600 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:04,600 - WARNING -   Failed to get response for question 701
2025-07-07 10:17:04,600 - INFO - Processing question 590/881 (ID: 702) for fanar
2025-07-07 10:17:04,600 - INFO -   Ground truth verses: ['81:6', '82:3']
2025-07-07 10:17:04,600 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:04,761 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:04,761 - WARNING -   Failed to get response for question 702
2025-07-07 10:17:04,782 - INFO - Saved data to results/fanar_checkpoint_589.json
2025-07-07 10:17:04,783 - INFO - Checkpoint saved for fanar at question 590
2025-07-07 10:17:04,783 - INFO -   Progress: 590/881
2025-07-07 10:17:04,783 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:04,783 - INFO - Processing question 591/881 (ID: 703) for fanar
2025-07-07 10:17:04,783 - INFO -   Ground truth verses: ['81:6', '82:3']
2025-07-07 10:17:04,783 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:04,944 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:04,944 - WARNING -   Failed to get response for question 703
2025-07-07 10:17:04,944 - INFO - Processing question 592/881 (ID: 704) for fanar
2025-07-07 10:17:04,944 - INFO -   Ground truth verses: ['81:6', '82:3']
2025-07-07 10:17:04,944 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:05,106 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:05,107 - WARNING -   Failed to get response for question 704
2025-07-07 10:17:05,107 - INFO - Processing question 593/881 (ID: 705) for fanar
2025-07-07 10:17:05,107 - INFO -   Ground truth verses: ['81:6', '82:3']
2025-07-07 10:17:05,107 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:05,268 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:05,269 - WARNING -   Failed to get response for question 705
2025-07-07 10:17:05,269 - INFO - Processing question 594/881 (ID: 706) for fanar
2025-07-07 10:17:05,269 - INFO -   Ground truth verses: ['26:24', '37:5', '38:66', '78:37', '44:38', '19:65', '21:16', '26:28', '38:10', '20:6', '43:82', '50:38', '43:85', '44:7']
2025-07-07 10:17:05,269 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:05,431 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:05,432 - WARNING -   Failed to get response for question 706
2025-07-07 10:17:05,432 - INFO - Processing question 595/881 (ID: 707) for fanar
2025-07-07 10:17:05,432 - INFO -   Ground truth verses: ['26:24', '37:5', '38:66', '78:37', '44:38', '19:65', '21:16', '26:28', '38:10', '20:6', '43:82', '50:38', '43:85', '44:7']
2025-07-07 10:17:05,432 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:05,599 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:05,599 - WARNING -   Failed to get response for question 707
2025-07-07 10:17:05,620 - INFO - Saved data to results/fanar_checkpoint_594.json
2025-07-07 10:17:05,620 - INFO - Checkpoint saved for fanar at question 595
2025-07-07 10:17:05,620 - INFO -   Progress: 595/881
2025-07-07 10:17:05,620 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:05,620 - INFO - Processing question 596/881 (ID: 708) for fanar
2025-07-07 10:17:05,620 - INFO -   Ground truth verses: ['26:24', '37:5', '38:66', '78:37', '44:38', '19:65', '21:16', '26:28', '38:10', '20:6', '43:82', '50:38', '43:85', '44:7']
2025-07-07 10:17:05,620 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:05,974 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:05,974 - WARNING -   Failed to get response for question 708
2025-07-07 10:17:05,974 - INFO - Processing question 597/881 (ID: 709) for fanar
2025-07-07 10:17:05,974 - INFO -   Ground truth verses: ['26:24', '37:5', '38:66', '78:37', '44:38', '19:65', '21:16', '26:28', '38:10', '20:6', '43:82', '50:38', '43:85', '44:7']
2025-07-07 10:17:05,974 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:06,140 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:06,140 - WARNING -   Failed to get response for question 709
2025-07-07 10:17:06,140 - INFO - Processing question 598/881 (ID: 710) for fanar
2025-07-07 10:17:06,140 - INFO -   Ground truth verses: ['26:24', '37:5', '38:66', '78:37', '44:38', '19:65', '21:16', '26:28', '38:10', '20:6', '43:82', '50:38', '43:85', '44:7']
2025-07-07 10:17:06,140 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:06,304 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:06,305 - WARNING -   Failed to get response for question 710
2025-07-07 10:17:06,305 - INFO - Processing question 599/881 (ID: 711) for fanar
2025-07-07 10:17:06,305 - INFO -   Ground truth verses: ['4:167', '47:34', '16:88', '47:32', '7:45', '11:19', '47:8', '58:16', '22:25', '63:2', '47:1']
2025-07-07 10:17:06,305 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:06,468 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:06,468 - WARNING -   Failed to get response for question 711
2025-07-07 10:17:06,468 - INFO - Processing question 600/881 (ID: 712) for fanar
2025-07-07 10:17:06,469 - INFO -   Ground truth verses: ['4:167', '47:34', '16:88', '47:32', '7:45', '11:19', '47:8', '58:16', '22:25', '63:2', '47:1']
2025-07-07 10:17:06,469 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:06,632 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:06,633 - WARNING -   Failed to get response for question 712
2025-07-07 10:17:06,655 - INFO - Saved data to results/fanar_checkpoint_599.json
2025-07-07 10:17:06,655 - INFO - Checkpoint saved for fanar at question 600
2025-07-07 10:17:06,655 - INFO -   Progress: 600/881
2025-07-07 10:17:06,655 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:06,655 - INFO - Processing question 601/881 (ID: 713) for fanar
2025-07-07 10:17:06,655 - INFO -   Ground truth verses: ['4:167', '47:34', '16:88', '47:32', '7:45', '11:19', '47:8', '58:16', '22:25', '63:2', '47:1']
2025-07-07 10:17:06,655 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:06,820 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:06,821 - WARNING -   Failed to get response for question 713
2025-07-07 10:17:06,821 - INFO - Processing question 602/881 (ID: 714) for fanar
2025-07-07 10:17:06,821 - INFO -   Ground truth verses: ['4:167', '47:34', '16:88', '47:32', '7:45', '11:19', '47:8', '58:16', '22:25', '63:2', '47:1']
2025-07-07 10:17:06,821 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:06,982 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:06,983 - WARNING -   Failed to get response for question 714
2025-07-07 10:17:06,983 - INFO - Processing question 603/881 (ID: 715) for fanar
2025-07-07 10:17:06,983 - INFO -   Ground truth verses: ['4:167', '47:34', '16:88', '47:32', '7:45', '11:19', '47:8', '58:16', '22:25', '63:2', '47:1']
2025-07-07 10:17:06,983 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:07,147 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:07,148 - WARNING -   Failed to get response for question 715
2025-07-07 10:17:07,148 - INFO - Processing question 604/881 (ID: 716) for fanar
2025-07-07 10:17:07,148 - INFO -   Ground truth verses: ['79:41', '53:15']
2025-07-07 10:17:07,148 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:07,500 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:07,500 - WARNING -   Failed to get response for question 716
2025-07-07 10:17:07,500 - INFO - Processing question 605/881 (ID: 717) for fanar
2025-07-07 10:17:07,500 - INFO -   Ground truth verses: ['79:41', '53:15']
2025-07-07 10:17:07,500 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:07,667 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:07,668 - WARNING -   Failed to get response for question 717
2025-07-07 10:17:07,688 - INFO - Saved data to results/fanar_checkpoint_604.json
2025-07-07 10:17:07,688 - INFO - Checkpoint saved for fanar at question 605
2025-07-07 10:17:07,688 - INFO -   Progress: 605/881
2025-07-07 10:17:07,688 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:07,688 - INFO - Processing question 606/881 (ID: 718) for fanar
2025-07-07 10:17:07,688 - INFO -   Ground truth verses: ['79:41', '53:15']
2025-07-07 10:17:07,688 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:07,849 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:07,849 - WARNING -   Failed to get response for question 718
2025-07-07 10:17:07,849 - INFO - Processing question 607/881 (ID: 719) for fanar
2025-07-07 10:17:07,849 - INFO -   Ground truth verses: ['79:41', '53:15']
2025-07-07 10:17:07,850 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:08,011 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:08,012 - WARNING -   Failed to get response for question 719
2025-07-07 10:17:08,012 - INFO - Processing question 608/881 (ID: 720) for fanar
2025-07-07 10:17:08,012 - INFO -   Ground truth verses: ['79:41', '53:15']
2025-07-07 10:17:08,012 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:08,175 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:08,175 - WARNING -   Failed to get response for question 720
2025-07-07 10:17:08,175 - INFO - Processing question 609/881 (ID: 721) for fanar
2025-07-07 10:17:08,175 - INFO -   Ground truth verses: ['30:54']
2025-07-07 10:17:08,175 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:08,338 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:08,339 - WARNING -   Failed to get response for question 721
2025-07-07 10:17:08,339 - INFO - Processing question 610/881 (ID: 722) for fanar
2025-07-07 10:17:08,339 - INFO -   Ground truth verses: ['30:54']
2025-07-07 10:17:08,339 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:08,501 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:08,501 - WARNING -   Failed to get response for question 722
2025-07-07 10:17:08,522 - INFO - Saved data to results/fanar_checkpoint_609.json
2025-07-07 10:17:08,522 - INFO - Checkpoint saved for fanar at question 610
2025-07-07 10:17:08,522 - INFO -   Progress: 610/881
2025-07-07 10:17:08,522 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:08,522 - INFO - Processing question 611/881 (ID: 723) for fanar
2025-07-07 10:17:08,522 - INFO -   Ground truth verses: ['30:54']
2025-07-07 10:17:08,522 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:08,687 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:08,687 - WARNING -   Failed to get response for question 723
2025-07-07 10:17:08,687 - INFO - Processing question 612/881 (ID: 724) for fanar
2025-07-07 10:17:08,687 - INFO -   Ground truth verses: ['30:54']
2025-07-07 10:17:08,687 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:09,040 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:09,041 - WARNING -   Failed to get response for question 724
2025-07-07 10:17:09,041 - INFO - Processing question 613/881 (ID: 725) for fanar
2025-07-07 10:17:09,041 - INFO -   Ground truth verses: ['30:54']
2025-07-07 10:17:09,041 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:09,204 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:09,205 - WARNING -   Failed to get response for question 725
2025-07-07 10:17:09,205 - INFO - Processing question 614/881 (ID: 726) for fanar
2025-07-07 10:17:09,205 - INFO -   Ground truth verses: ['36:24', '12:8']
2025-07-07 10:17:09,205 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:09,367 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:09,368 - WARNING -   Failed to get response for question 726
2025-07-07 10:17:09,368 - INFO - Processing question 615/881 (ID: 727) for fanar
2025-07-07 10:17:09,368 - INFO -   Ground truth verses: ['36:24', '12:8']
2025-07-07 10:17:09,368 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:09,532 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:09,533 - WARNING -   Failed to get response for question 727
2025-07-07 10:17:09,553 - INFO - Saved data to results/fanar_checkpoint_614.json
2025-07-07 10:17:09,553 - INFO - Checkpoint saved for fanar at question 615
2025-07-07 10:17:09,553 - INFO -   Progress: 615/881
2025-07-07 10:17:09,553 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:09,553 - INFO - Processing question 616/881 (ID: 728) for fanar
2025-07-07 10:17:09,553 - INFO -   Ground truth verses: ['36:24', '12:8']
2025-07-07 10:17:09,553 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:09,722 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:09,722 - WARNING -   Failed to get response for question 728
2025-07-07 10:17:09,723 - INFO - Processing question 617/881 (ID: 729) for fanar
2025-07-07 10:17:09,723 - INFO -   Ground truth verses: ['36:24', '12:8']
2025-07-07 10:17:09,723 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:09,886 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:09,887 - WARNING -   Failed to get response for question 729
2025-07-07 10:17:09,887 - INFO - Processing question 618/881 (ID: 730) for fanar
2025-07-07 10:17:09,887 - INFO -   Ground truth verses: ['36:24', '12:8']
2025-07-07 10:17:09,887 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:10,058 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:10,059 - WARNING -   Failed to get response for question 730
2025-07-07 10:17:10,059 - INFO - Processing question 619/881 (ID: 731) for fanar
2025-07-07 10:17:10,059 - INFO -   Ground truth verses: ['28:57']
2025-07-07 10:17:10,059 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:10,223 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:10,224 - WARNING -   Failed to get response for question 731
2025-07-07 10:17:10,224 - INFO - Processing question 620/881 (ID: 732) for fanar
2025-07-07 10:17:10,224 - INFO -   Ground truth verses: ['28:57']
2025-07-07 10:17:10,224 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:10,581 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:10,581 - WARNING -   Failed to get response for question 732
2025-07-07 10:17:10,602 - INFO - Saved data to results/fanar_checkpoint_619.json
2025-07-07 10:17:10,602 - INFO - Checkpoint saved for fanar at question 620
2025-07-07 10:17:10,602 - INFO -   Progress: 620/881
2025-07-07 10:17:10,602 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:10,602 - INFO - Processing question 621/881 (ID: 733) for fanar
2025-07-07 10:17:10,602 - INFO -   Ground truth verses: ['28:57']
2025-07-07 10:17:10,602 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:10,765 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:10,765 - WARNING -   Failed to get response for question 733
2025-07-07 10:17:10,765 - INFO - Processing question 622/881 (ID: 734) for fanar
2025-07-07 10:17:10,765 - INFO -   Ground truth verses: ['28:57']
2025-07-07 10:17:10,765 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:10,929 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:10,930 - WARNING -   Failed to get response for question 734
2025-07-07 10:17:10,930 - INFO - Processing question 623/881 (ID: 735) for fanar
2025-07-07 10:17:10,930 - INFO -   Ground truth verses: ['28:57']
2025-07-07 10:17:10,930 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:11,093 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:11,094 - WARNING -   Failed to get response for question 735
2025-07-07 10:17:11,094 - INFO - Processing question 624/881 (ID: 736) for fanar
2025-07-07 10:17:11,094 - INFO -   Ground truth verses: ['11:74']
2025-07-07 10:17:11,094 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:11,260 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:11,260 - WARNING -   Failed to get response for question 736
2025-07-07 10:17:11,260 - INFO - Processing question 625/881 (ID: 737) for fanar
2025-07-07 10:17:11,260 - INFO -   Ground truth verses: ['11:74']
2025-07-07 10:17:11,260 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:11,422 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:11,423 - WARNING -   Failed to get response for question 737
2025-07-07 10:17:11,443 - INFO - Saved data to results/fanar_checkpoint_624.json
2025-07-07 10:17:11,443 - INFO - Checkpoint saved for fanar at question 625
2025-07-07 10:17:11,444 - INFO -   Progress: 625/881
2025-07-07 10:17:11,444 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:11,444 - INFO - Processing question 626/881 (ID: 738) for fanar
2025-07-07 10:17:11,444 - INFO -   Ground truth verses: ['11:74']
2025-07-07 10:17:11,444 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:11,611 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:11,611 - WARNING -   Failed to get response for question 738
2025-07-07 10:17:11,611 - INFO - Processing question 627/881 (ID: 739) for fanar
2025-07-07 10:17:11,611 - INFO -   Ground truth verses: ['11:74']
2025-07-07 10:17:11,611 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:11,773 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:11,773 - WARNING -   Failed to get response for question 739
2025-07-07 10:17:11,774 - INFO - Processing question 628/881 (ID: 740) for fanar
2025-07-07 10:17:11,774 - INFO -   Ground truth verses: ['11:74']
2025-07-07 10:17:11,774 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:12,129 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:12,130 - WARNING -   Failed to get response for question 740
2025-07-07 10:17:12,130 - INFO - Processing question 629/881 (ID: 741) for fanar
2025-07-07 10:17:12,130 - INFO -   Ground truth verses: ['82:17', '82:18', '101:10', '77:13', '74:27', '69:3', '90:12', '101:3', '83:19', '83:8', '86:2', '104:5', '97:2', '78:17', '77:38', '37:21', '44:40', '77:14']
2025-07-07 10:17:12,130 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:12,295 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:12,295 - WARNING -   Failed to get response for question 741
2025-07-07 10:17:12,295 - INFO - Processing question 630/881 (ID: 742) for fanar
2025-07-07 10:17:12,295 - INFO -   Ground truth verses: ['82:17', '82:18', '101:10', '77:13', '74:27', '69:3', '90:12', '101:3', '83:19', '83:8', '86:2', '104:5', '97:2', '78:17', '77:38', '37:21', '44:40', '77:14']
2025-07-07 10:17:12,295 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:12,457 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:12,458 - WARNING -   Failed to get response for question 742
2025-07-07 10:17:12,479 - INFO - Saved data to results/fanar_checkpoint_629.json
2025-07-07 10:17:12,479 - INFO - Checkpoint saved for fanar at question 630
2025-07-07 10:17:12,479 - INFO -   Progress: 630/881
2025-07-07 10:17:12,479 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:12,479 - INFO - Processing question 631/881 (ID: 743) for fanar
2025-07-07 10:17:12,479 - INFO -   Ground truth verses: ['82:17', '82:18', '101:10', '77:13', '74:27', '69:3', '90:12', '101:3', '83:19', '83:8', '86:2', '104:5', '97:2', '78:17', '77:38', '37:21', '44:40', '77:14']
2025-07-07 10:17:12,479 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:12,643 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:12,643 - WARNING -   Failed to get response for question 743
2025-07-07 10:17:12,643 - INFO - Processing question 632/881 (ID: 744) for fanar
2025-07-07 10:17:12,644 - INFO -   Ground truth verses: ['82:17', '82:18', '101:10', '77:13', '74:27', '69:3', '90:12', '101:3', '83:19', '83:8', '86:2', '104:5', '97:2', '78:17', '77:38', '37:21', '44:40', '77:14']
2025-07-07 10:17:12,644 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:12,808 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:12,809 - WARNING -   Failed to get response for question 744
2025-07-07 10:17:12,809 - INFO - Processing question 633/881 (ID: 745) for fanar
2025-07-07 10:17:12,809 - INFO -   Ground truth verses: ['82:17', '82:18', '101:10', '77:13', '74:27', '69:3', '90:12', '101:3', '83:19', '83:8', '86:2', '104:5', '97:2', '78:17', '77:38', '37:21', '44:40', '77:14']
2025-07-07 10:17:12,809 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:12,971 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:12,971 - WARNING -   Failed to get response for question 745
2025-07-07 10:17:12,971 - INFO - Processing question 634/881 (ID: 746) for fanar
2025-07-07 10:17:12,971 - INFO -   Ground truth verses: ['17:60']
2025-07-07 10:17:12,971 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:13,133 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:13,134 - WARNING -   Failed to get response for question 746
2025-07-07 10:17:13,134 - INFO - Processing question 635/881 (ID: 747) for fanar
2025-07-07 10:17:13,134 - INFO -   Ground truth verses: ['17:60']
2025-07-07 10:17:13,134 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:13,297 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:13,298 - WARNING -   Failed to get response for question 747
2025-07-07 10:17:13,319 - INFO - Saved data to results/fanar_checkpoint_634.json
2025-07-07 10:17:13,319 - INFO - Checkpoint saved for fanar at question 635
2025-07-07 10:17:13,319 - INFO -   Progress: 635/881
2025-07-07 10:17:13,319 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:13,319 - INFO - Processing question 636/881 (ID: 748) for fanar
2025-07-07 10:17:13,319 - INFO -   Ground truth verses: ['17:60']
2025-07-07 10:17:13,319 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:13,688 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:13,689 - WARNING -   Failed to get response for question 748
2025-07-07 10:17:13,689 - INFO - Processing question 637/881 (ID: 749) for fanar
2025-07-07 10:17:13,689 - INFO -   Ground truth verses: ['17:60']
2025-07-07 10:17:13,689 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:13,855 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:13,855 - WARNING -   Failed to get response for question 749
2025-07-07 10:17:13,855 - INFO - Processing question 638/881 (ID: 750) for fanar
2025-07-07 10:17:13,855 - INFO -   Ground truth verses: ['17:60']
2025-07-07 10:17:13,855 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:14,018 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:14,019 - WARNING -   Failed to get response for question 750
2025-07-07 10:17:14,019 - INFO - Processing question 639/881 (ID: 751) for fanar
2025-07-07 10:17:14,019 - INFO -   Ground truth verses: ['87:5']
2025-07-07 10:17:14,019 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:14,183 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:14,184 - WARNING -   Failed to get response for question 751
2025-07-07 10:17:14,184 - INFO - Processing question 640/881 (ID: 752) for fanar
2025-07-07 10:17:14,184 - INFO -   Ground truth verses: ['87:5']
2025-07-07 10:17:14,184 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:14,350 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:14,351 - WARNING -   Failed to get response for question 752
2025-07-07 10:17:14,373 - INFO - Saved data to results/fanar_checkpoint_639.json
2025-07-07 10:17:14,373 - INFO - Checkpoint saved for fanar at question 640
2025-07-07 10:17:14,373 - INFO -   Progress: 640/881
2025-07-07 10:17:14,373 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:14,373 - INFO - Processing question 641/881 (ID: 753) for fanar
2025-07-07 10:17:14,373 - INFO -   Ground truth verses: ['87:5']
2025-07-07 10:17:14,373 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:14,538 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:14,539 - WARNING -   Failed to get response for question 753
2025-07-07 10:17:14,539 - INFO - Processing question 642/881 (ID: 754) for fanar
2025-07-07 10:17:14,539 - INFO -   Ground truth verses: ['87:5']
2025-07-07 10:17:14,539 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:14,705 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:14,706 - WARNING -   Failed to get response for question 754
2025-07-07 10:17:14,706 - INFO - Processing question 643/881 (ID: 755) for fanar
2025-07-07 10:17:14,706 - INFO -   Ground truth verses: ['87:5']
2025-07-07 10:17:14,706 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:14,870 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:14,871 - WARNING -   Failed to get response for question 755
2025-07-07 10:17:14,871 - INFO - Processing question 644/881 (ID: 756) for fanar
2025-07-07 10:17:14,871 - INFO -   Ground truth verses: ['68:7', '6:117', '53:30', '16:125']
2025-07-07 10:17:14,871 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:15,033 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:15,034 - WARNING -   Failed to get response for question 756
2025-07-07 10:17:15,034 - INFO - Processing question 645/881 (ID: 757) for fanar
2025-07-07 10:17:15,034 - INFO -   Ground truth verses: ['68:7', '6:117', '53:30', '16:125']
2025-07-07 10:17:15,034 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:15,399 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:15,400 - WARNING -   Failed to get response for question 757
2025-07-07 10:17:15,422 - INFO - Saved data to results/fanar_checkpoint_644.json
2025-07-07 10:17:15,422 - INFO - Checkpoint saved for fanar at question 645
2025-07-07 10:17:15,422 - INFO -   Progress: 645/881
2025-07-07 10:17:15,422 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:15,422 - INFO - Processing question 646/881 (ID: 758) for fanar
2025-07-07 10:17:15,422 - INFO -   Ground truth verses: ['68:7', '6:117', '53:30', '16:125']
2025-07-07 10:17:15,422 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:15,599 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:15,600 - WARNING -   Failed to get response for question 758
2025-07-07 10:17:15,600 - INFO - Processing question 647/881 (ID: 759) for fanar
2025-07-07 10:17:15,600 - INFO -   Ground truth verses: ['68:7', '6:117', '53:30', '16:125']
2025-07-07 10:17:15,600 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:15,763 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:15,764 - WARNING -   Failed to get response for question 759
2025-07-07 10:17:15,764 - INFO - Processing question 648/881 (ID: 760) for fanar
2025-07-07 10:17:15,764 - INFO -   Ground truth verses: ['68:7', '6:117', '53:30', '16:125']
2025-07-07 10:17:15,764 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:15,929 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:15,929 - WARNING -   Failed to get response for question 760
2025-07-07 10:17:15,929 - INFO - Processing question 649/881 (ID: 761) for fanar
2025-07-07 10:17:15,929 - INFO -   Ground truth verses: ['32:11']
2025-07-07 10:17:15,929 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:16,095 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:16,095 - WARNING -   Failed to get response for question 761
2025-07-07 10:17:16,095 - INFO - Processing question 650/881 (ID: 762) for fanar
2025-07-07 10:17:16,095 - INFO -   Ground truth verses: ['32:11']
2025-07-07 10:17:16,095 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:16,259 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:16,259 - WARNING -   Failed to get response for question 762
2025-07-07 10:17:16,282 - INFO - Saved data to results/fanar_checkpoint_649.json
2025-07-07 10:17:16,282 - INFO - Checkpoint saved for fanar at question 650
2025-07-07 10:17:16,282 - INFO -   Progress: 650/881
2025-07-07 10:17:16,282 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:16,282 - INFO - Processing question 651/881 (ID: 763) for fanar
2025-07-07 10:17:16,282 - INFO -   Ground truth verses: ['32:11']
2025-07-07 10:17:16,282 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:16,444 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:16,445 - WARNING -   Failed to get response for question 763
2025-07-07 10:17:16,445 - INFO - Processing question 652/881 (ID: 764) for fanar
2025-07-07 10:17:16,445 - INFO -   Ground truth verses: ['32:11']
2025-07-07 10:17:16,445 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:16,611 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:16,612 - WARNING -   Failed to get response for question 764
2025-07-07 10:17:16,612 - INFO - Processing question 653/881 (ID: 765) for fanar
2025-07-07 10:17:16,612 - INFO -   Ground truth verses: ['32:11']
2025-07-07 10:17:16,612 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:16,975 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:16,976 - WARNING -   Failed to get response for question 765
2025-07-07 10:17:16,976 - INFO - Processing question 654/881 (ID: 766) for fanar
2025-07-07 10:17:16,976 - INFO -   Ground truth verses: ['3:40', '19:20', '19:8']
2025-07-07 10:17:16,976 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:17,144 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:17,145 - WARNING -   Failed to get response for question 766
2025-07-07 10:17:17,145 - INFO - Processing question 655/881 (ID: 767) for fanar
2025-07-07 10:17:17,145 - INFO -   Ground truth verses: ['3:40', '19:20', '19:8']
2025-07-07 10:17:17,145 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:17,312 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:17,313 - WARNING -   Failed to get response for question 767
2025-07-07 10:17:17,335 - INFO - Saved data to results/fanar_checkpoint_654.json
2025-07-07 10:17:17,335 - INFO - Checkpoint saved for fanar at question 655
2025-07-07 10:17:17,335 - INFO -   Progress: 655/881
2025-07-07 10:17:17,335 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:17,335 - INFO - Processing question 656/881 (ID: 768) for fanar
2025-07-07 10:17:17,335 - INFO -   Ground truth verses: ['3:40', '19:20', '19:8']
2025-07-07 10:17:17,336 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:17,499 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:17,499 - WARNING -   Failed to get response for question 768
2025-07-07 10:17:17,499 - INFO - Processing question 657/881 (ID: 769) for fanar
2025-07-07 10:17:17,499 - INFO -   Ground truth verses: ['3:40', '19:20', '19:8']
2025-07-07 10:17:17,499 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:17,668 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:17,669 - WARNING -   Failed to get response for question 769
2025-07-07 10:17:17,669 - INFO - Processing question 658/881 (ID: 770) for fanar
2025-07-07 10:17:17,669 - INFO -   Ground truth verses: ['3:40', '19:20', '19:8']
2025-07-07 10:17:17,669 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:17,833 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:17,834 - WARNING -   Failed to get response for question 770
2025-07-07 10:17:17,834 - INFO - Processing question 659/881 (ID: 771) for fanar
2025-07-07 10:17:17,834 - INFO -   Ground truth verses: ['12:88']
2025-07-07 10:17:17,834 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:17,999 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:17,999 - WARNING -   Failed to get response for question 771
2025-07-07 10:17:17,999 - INFO - Processing question 660/881 (ID: 772) for fanar
2025-07-07 10:17:18,000 - INFO -   Ground truth verses: ['12:88']
2025-07-07 10:17:18,000 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:18,165 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:18,165 - WARNING -   Failed to get response for question 772
2025-07-07 10:17:18,188 - INFO - Saved data to results/fanar_checkpoint_659.json
2025-07-07 10:17:18,188 - INFO - Checkpoint saved for fanar at question 660
2025-07-07 10:17:18,188 - INFO -   Progress: 660/881
2025-07-07 10:17:18,188 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:18,188 - INFO - Processing question 661/881 (ID: 773) for fanar
2025-07-07 10:17:18,188 - INFO -   Ground truth verses: ['12:88']
2025-07-07 10:17:18,188 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:18,557 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:18,558 - WARNING -   Failed to get response for question 773
2025-07-07 10:17:18,558 - INFO - Processing question 662/881 (ID: 774) for fanar
2025-07-07 10:17:18,558 - INFO -   Ground truth verses: ['12:88']
2025-07-07 10:17:18,558 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:18,723 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:18,724 - WARNING -   Failed to get response for question 774
2025-07-07 10:17:18,724 - INFO - Processing question 663/881 (ID: 775) for fanar
2025-07-07 10:17:18,724 - INFO -   Ground truth verses: ['12:88']
2025-07-07 10:17:18,724 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:18,891 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:18,892 - WARNING -   Failed to get response for question 775
2025-07-07 10:17:18,892 - INFO - Processing question 664/881 (ID: 776) for fanar
2025-07-07 10:17:18,892 - INFO -   Ground truth verses: ['21:58']
2025-07-07 10:17:18,892 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:18,991 - INFO - Extracted JSON from code block: 755 characters
2025-07-07 10:17:18,991 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-An'am:65]' -> 'Al-An'am:65'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Tawbah:106]' -> 'Al-Tawbah:106'
2025-07-07 10:17:18,991 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-An'am:65]' -> 'Al-An'am:65'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-An'am:65]' -> 'Al-An'am:65'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Hijr:7]' -> 'Al-Hijr:7'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Qamar:10]' -> 'Al-Qamar:10'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Tawbah:106]' -> 'Al-Tawbah:106'
2025-07-07 10:17:18,991 - WARNING - Invalid colon format: '[Al-Tawbah:106]' -> 'Al-Tawbah:106'
2025-07-07 10:17:18,992 - INFO - Normalized predicted verses: ['15:7', '2:255', '4:65', '50:10', '9:106']
2025-07-07 10:17:18,992 - INFO - Normalized ground truth verses: ['15:11', '16:34', '26:6', '43:7', '6:5']
2025-07-07 10:17:18,992 - INFO -   Response: 755 chars, Found 5 verses
2025-07-07 10:17:18,992 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:18,992 - INFO -   Matched verses: []
2025-07-07 10:17:19,056 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:19,057 - WARNING -   Failed to get response for question 776
2025-07-07 10:17:19,057 - INFO - Processing question 665/881 (ID: 777) for fanar
2025-07-07 10:17:19,057 - INFO -   Ground truth verses: ['21:58']
2025-07-07 10:17:19,057 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:19,198 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:19,199 - WARNING -   Failed to get response for question 777
2025-07-07 10:17:19,224 - INFO - Saved data to results/fanar_checkpoint_664.json
2025-07-07 10:17:19,224 - INFO - Checkpoint saved for fanar at question 665
2025-07-07 10:17:19,224 - INFO -   Progress: 665/881
2025-07-07 10:17:19,224 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:19,224 - INFO - Processing question 666/881 (ID: 778) for fanar
2025-07-07 10:17:19,224 - INFO -   Ground truth verses: ['21:58']
2025-07-07 10:17:19,224 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:19,364 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:19,364 - WARNING -   Failed to get response for question 778
2025-07-07 10:17:19,365 - INFO - Processing question 667/881 (ID: 779) for fanar
2025-07-07 10:17:19,365 - INFO -   Ground truth verses: ['21:58']
2025-07-07 10:17:19,365 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:19,501 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:19,502 - WARNING -   Failed to get response for question 779
2025-07-07 10:17:19,502 - INFO - Processing question 668/881 (ID: 780) for fanar
2025-07-07 10:17:19,502 - INFO -   Ground truth verses: ['21:58']
2025-07-07 10:17:19,502 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:19,639 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:19,639 - WARNING -   Failed to get response for question 780
2025-07-07 10:17:19,639 - INFO - Processing question 669/881 (ID: 781) for fanar
2025-07-07 10:17:19,639 - INFO -   Ground truth verses: ['81:9']
2025-07-07 10:17:19,639 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:19,972 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:19,973 - WARNING -   Failed to get response for question 781
2025-07-07 10:17:19,973 - INFO - Processing question 670/881 (ID: 782) for fanar
2025-07-07 10:17:19,973 - INFO -   Ground truth verses: ['81:9']
2025-07-07 10:17:19,973 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:19,993 - INFO - Processing question 302/881 (ID: 342) for qwen
2025-07-07 10:17:19,993 - INFO -   Ground truth verses: ['6:5', '43:7', '15:11', '16:34', '26:6']
2025-07-07 10:17:19,993 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:17:20,120 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:20,121 - WARNING -   Failed to get response for question 782
2025-07-07 10:17:20,143 - INFO - Saved data to results/fanar_checkpoint_669.json
2025-07-07 10:17:20,143 - INFO - Checkpoint saved for fanar at question 670
2025-07-07 10:17:20,143 - INFO -   Progress: 670/881
2025-07-07 10:17:20,143 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:20,143 - INFO - Processing question 671/881 (ID: 783) for fanar
2025-07-07 10:17:20,143 - INFO -   Ground truth verses: ['81:9']
2025-07-07 10:17:20,143 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:20,315 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:20,315 - WARNING -   Failed to get response for question 783
2025-07-07 10:17:20,315 - INFO - Processing question 672/881 (ID: 784) for fanar
2025-07-07 10:17:20,315 - INFO -   Ground truth verses: ['81:9']
2025-07-07 10:17:20,315 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:20,479 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:20,480 - WARNING -   Failed to get response for question 784
2025-07-07 10:17:20,480 - INFO - Processing question 673/881 (ID: 785) for fanar
2025-07-07 10:17:20,480 - INFO -   Ground truth verses: ['81:9']
2025-07-07 10:17:20,480 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:20,648 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:20,649 - WARNING -   Failed to get response for question 785
2025-07-07 10:17:20,649 - INFO - Processing question 674/881 (ID: 786) for fanar
2025-07-07 10:17:20,649 - INFO -   Ground truth verses: ['75:19', '55:4']
2025-07-07 10:17:20,649 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:20,813 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:20,813 - WARNING -   Failed to get response for question 786
2025-07-07 10:17:20,813 - INFO - Processing question 675/881 (ID: 787) for fanar
2025-07-07 10:17:20,813 - INFO -   Ground truth verses: ['75:19', '55:4']
2025-07-07 10:17:20,813 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:20,977 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:20,977 - WARNING -   Failed to get response for question 787
2025-07-07 10:17:21,000 - INFO - Saved data to results/fanar_checkpoint_674.json
2025-07-07 10:17:21,000 - INFO - Checkpoint saved for fanar at question 675
2025-07-07 10:17:21,000 - INFO -   Progress: 675/881
2025-07-07 10:17:21,000 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:21,000 - INFO - Processing question 676/881 (ID: 788) for fanar
2025-07-07 10:17:21,000 - INFO -   Ground truth verses: ['75:19', '55:4']
2025-07-07 10:17:21,000 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:21,163 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:21,164 - WARNING -   Failed to get response for question 788
2025-07-07 10:17:21,164 - INFO - Processing question 677/881 (ID: 789) for fanar
2025-07-07 10:17:21,164 - INFO -   Ground truth verses: ['75:19', '55:4']
2025-07-07 10:17:21,164 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:21,325 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:21,325 - WARNING -   Failed to get response for question 789
2025-07-07 10:17:21,325 - INFO - Processing question 678/881 (ID: 790) for fanar
2025-07-07 10:17:21,325 - INFO -   Ground truth verses: ['75:19', '55:4']
2025-07-07 10:17:21,325 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:21,704 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:21,705 - WARNING -   Failed to get response for question 790
2025-07-07 10:17:21,705 - INFO - Processing question 679/881 (ID: 791) for fanar
2025-07-07 10:17:21,705 - INFO -   Ground truth verses: ['7:147', '22:57', '5:86', '5:10', '34:38', '2:39', '64:10', '84:22', '30:16']
2025-07-07 10:17:21,705 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:21,868 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:21,868 - WARNING -   Failed to get response for question 791
2025-07-07 10:17:21,868 - INFO - Processing question 680/881 (ID: 792) for fanar
2025-07-07 10:17:21,868 - INFO -   Ground truth verses: ['7:147', '22:57', '5:86', '5:10', '34:38', '2:39', '64:10', '84:22', '30:16']
2025-07-07 10:17:21,868 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:22,030 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:22,031 - WARNING -   Failed to get response for question 792
2025-07-07 10:17:22,054 - INFO - Saved data to results/fanar_checkpoint_679.json
2025-07-07 10:17:22,054 - INFO - Checkpoint saved for fanar at question 680
2025-07-07 10:17:22,054 - INFO -   Progress: 680/881
2025-07-07 10:17:22,054 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:22,054 - INFO - Processing question 681/881 (ID: 793) for fanar
2025-07-07 10:17:22,054 - INFO -   Ground truth verses: ['7:147', '22:57', '5:86', '5:10', '34:38', '2:39', '64:10', '84:22', '30:16']
2025-07-07 10:17:22,054 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:22,218 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:22,218 - WARNING -   Failed to get response for question 793
2025-07-07 10:17:22,218 - INFO - Processing question 682/881 (ID: 794) for fanar
2025-07-07 10:17:22,218 - INFO -   Ground truth verses: ['7:147', '22:57', '5:86', '5:10', '34:38', '2:39', '64:10', '84:22', '30:16']
2025-07-07 10:17:22,219 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:22,381 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:22,382 - WARNING -   Failed to get response for question 794
2025-07-07 10:17:22,382 - INFO - Processing question 683/881 (ID: 795) for fanar
2025-07-07 10:17:22,382 - INFO -   Ground truth verses: ['7:147', '22:57', '5:86', '5:10', '34:38', '2:39', '64:10', '84:22', '30:16']
2025-07-07 10:17:22,382 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:22,544 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:22,545 - WARNING -   Failed to get response for question 795
2025-07-07 10:17:22,545 - INFO - Processing question 684/881 (ID: 796) for fanar
2025-07-07 10:17:22,545 - INFO -   Ground truth verses: ['38:28']
2025-07-07 10:17:22,545 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:22,712 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:22,713 - WARNING -   Failed to get response for question 796
2025-07-07 10:17:22,713 - INFO - Processing question 685/881 (ID: 797) for fanar
2025-07-07 10:17:22,713 - INFO -   Ground truth verses: ['38:28']
2025-07-07 10:17:22,713 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:22,877 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:22,878 - WARNING -   Failed to get response for question 797
2025-07-07 10:17:22,900 - INFO - Saved data to results/fanar_checkpoint_684.json
2025-07-07 10:17:22,901 - INFO - Checkpoint saved for fanar at question 685
2025-07-07 10:17:22,901 - INFO -   Progress: 685/881
2025-07-07 10:17:22,901 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:22,901 - INFO - Processing question 686/881 (ID: 798) for fanar
2025-07-07 10:17:22,901 - INFO -   Ground truth verses: ['38:28']
2025-07-07 10:17:22,901 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:23,271 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:23,272 - WARNING -   Failed to get response for question 798
2025-07-07 10:17:23,272 - INFO - Processing question 687/881 (ID: 799) for fanar
2025-07-07 10:17:23,272 - INFO -   Ground truth verses: ['38:28']
2025-07-07 10:17:23,272 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:23,437 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:23,437 - WARNING -   Failed to get response for question 799
2025-07-07 10:17:23,437 - INFO - Processing question 688/881 (ID: 800) for fanar
2025-07-07 10:17:23,437 - INFO -   Ground truth verses: ['38:28']
2025-07-07 10:17:23,437 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:23,603 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:23,604 - WARNING -   Failed to get response for question 800
2025-07-07 10:17:23,604 - INFO - Processing question 689/881 (ID: 801) for fanar
2025-07-07 10:17:23,604 - INFO -   Ground truth verses: ['1:3', '32:6', '64:18', '2:163', '13:9', '23:92', '20:98', '59:22']
2025-07-07 10:17:23,604 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:23,765 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:23,766 - WARNING -   Failed to get response for question 801
2025-07-07 10:17:23,766 - INFO - Processing question 690/881 (ID: 802) for fanar
2025-07-07 10:17:23,766 - INFO -   Ground truth verses: ['1:3', '32:6', '64:18', '2:163', '13:9', '23:92', '20:98', '59:22']
2025-07-07 10:17:23,766 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:23,944 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:23,945 - WARNING -   Failed to get response for question 802
2025-07-07 10:17:23,971 - INFO - Saved data to results/fanar_checkpoint_689.json
2025-07-07 10:17:23,971 - INFO - Checkpoint saved for fanar at question 690
2025-07-07 10:17:23,971 - INFO -   Progress: 690/881
2025-07-07 10:17:23,971 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:23,972 - INFO - Processing question 691/881 (ID: 803) for fanar
2025-07-07 10:17:23,972 - INFO -   Ground truth verses: ['1:3', '32:6', '64:18', '2:163', '13:9', '23:92', '20:98', '59:22']
2025-07-07 10:17:23,972 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:24,138 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:24,138 - WARNING -   Failed to get response for question 803
2025-07-07 10:17:24,138 - INFO - Processing question 692/881 (ID: 804) for fanar
2025-07-07 10:17:24,138 - INFO -   Ground truth verses: ['1:3', '32:6', '64:18', '2:163', '13:9', '23:92', '20:98', '59:22']
2025-07-07 10:17:24,138 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:24,301 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:24,301 - WARNING -   Failed to get response for question 804
2025-07-07 10:17:24,302 - INFO - Processing question 693/881 (ID: 805) for fanar
2025-07-07 10:17:24,302 - INFO -   Ground truth verses: ['1:3', '32:6', '64:18', '2:163', '13:9', '23:92', '20:98', '59:22']
2025-07-07 10:17:24,302 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:24,466 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:24,467 - WARNING -   Failed to get response for question 805
2025-07-07 10:17:24,467 - INFO - Processing question 694/881 (ID: 806) for fanar
2025-07-07 10:17:24,467 - INFO -   Ground truth verses: ['3:57', '42:40']
2025-07-07 10:17:24,467 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:24,835 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:24,836 - WARNING -   Failed to get response for question 806
2025-07-07 10:17:24,836 - INFO - Processing question 695/881 (ID: 807) for fanar
2025-07-07 10:17:24,836 - INFO -   Ground truth verses: ['3:57', '42:40']
2025-07-07 10:17:24,836 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:25,000 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:25,000 - WARNING -   Failed to get response for question 807
2025-07-07 10:17:25,023 - INFO - Saved data to results/fanar_checkpoint_694.json
2025-07-07 10:17:25,023 - INFO - Checkpoint saved for fanar at question 695
2025-07-07 10:17:25,023 - INFO -   Progress: 695/881
2025-07-07 10:17:25,023 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:25,023 - INFO - Processing question 696/881 (ID: 808) for fanar
2025-07-07 10:17:25,023 - INFO -   Ground truth verses: ['3:57', '42:40']
2025-07-07 10:17:25,024 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:25,187 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:25,188 - WARNING -   Failed to get response for question 808
2025-07-07 10:17:25,188 - INFO - Processing question 697/881 (ID: 809) for fanar
2025-07-07 10:17:25,188 - INFO -   Ground truth verses: ['3:57', '42:40']
2025-07-07 10:17:25,188 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:25,356 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:25,356 - WARNING -   Failed to get response for question 809
2025-07-07 10:17:25,356 - INFO - Processing question 698/881 (ID: 810) for fanar
2025-07-07 10:17:25,356 - INFO -   Ground truth verses: ['3:57', '42:40']
2025-07-07 10:17:25,356 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:25,517 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:25,517 - WARNING -   Failed to get response for question 810
2025-07-07 10:17:25,517 - INFO - Processing question 699/881 (ID: 811) for fanar
2025-07-07 10:17:25,517 - INFO -   Ground truth verses: ['52:1', '95:2']
2025-07-07 10:17:25,517 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:25,683 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:25,684 - WARNING -   Failed to get response for question 811
2025-07-07 10:17:25,684 - INFO - Processing question 700/881 (ID: 812) for fanar
2025-07-07 10:17:25,684 - INFO -   Ground truth verses: ['52:1', '95:2']
2025-07-07 10:17:25,684 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:25,845 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:25,846 - WARNING -   Failed to get response for question 812
2025-07-07 10:17:25,869 - INFO - Saved data to results/fanar_checkpoint_699.json
2025-07-07 10:17:25,869 - INFO - Checkpoint saved for fanar at question 700
2025-07-07 10:17:25,869 - INFO -   Progress: 700/881
2025-07-07 10:17:25,869 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:25,869 - INFO - Processing question 701/881 (ID: 813) for fanar
2025-07-07 10:17:25,869 - INFO -   Ground truth verses: ['52:1', '95:2']
2025-07-07 10:17:25,869 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:26,033 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:26,033 - WARNING -   Failed to get response for question 813
2025-07-07 10:17:26,033 - INFO - Processing question 702/881 (ID: 814) for fanar
2025-07-07 10:17:26,033 - INFO -   Ground truth verses: ['52:1', '95:2']
2025-07-07 10:17:26,033 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:26,214 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:26,215 - WARNING -   Failed to get response for question 814
2025-07-07 10:17:26,215 - INFO - Processing question 703/881 (ID: 815) for fanar
2025-07-07 10:17:26,215 - INFO -   Ground truth verses: ['52:1', '95:2']
2025-07-07 10:17:26,215 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:26,619 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:26,620 - WARNING -   Failed to get response for question 815
2025-07-07 10:17:26,620 - INFO - Processing question 704/881 (ID: 816) for fanar
2025-07-07 10:17:26,620 - INFO -   Ground truth verses: ['8:60']
2025-07-07 10:17:26,620 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:26,785 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:26,786 - WARNING -   Failed to get response for question 816
2025-07-07 10:17:26,786 - INFO - Processing question 705/881 (ID: 817) for fanar
2025-07-07 10:17:26,786 - INFO -   Ground truth verses: ['8:60']
2025-07-07 10:17:26,786 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:26,948 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:26,949 - WARNING -   Failed to get response for question 817
2025-07-07 10:17:26,973 - INFO - Saved data to results/fanar_checkpoint_704.json
2025-07-07 10:17:26,973 - INFO - Checkpoint saved for fanar at question 705
2025-07-07 10:17:26,973 - INFO -   Progress: 705/881
2025-07-07 10:17:26,973 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:26,973 - INFO - Processing question 706/881 (ID: 818) for fanar
2025-07-07 10:17:26,973 - INFO -   Ground truth verses: ['8:60']
2025-07-07 10:17:26,973 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:27,141 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:27,141 - WARNING -   Failed to get response for question 818
2025-07-07 10:17:27,141 - INFO - Processing question 707/881 (ID: 819) for fanar
2025-07-07 10:17:27,141 - INFO -   Ground truth verses: ['8:60']
2025-07-07 10:17:27,141 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:27,310 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:27,311 - WARNING -   Failed to get response for question 819
2025-07-07 10:17:27,311 - INFO - Processing question 708/881 (ID: 820) for fanar
2025-07-07 10:17:27,311 - INFO -   Ground truth verses: ['8:60']
2025-07-07 10:17:27,311 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:27,473 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:27,473 - WARNING -   Failed to get response for question 820
2025-07-07 10:17:27,473 - INFO - Processing question 709/881 (ID: 821) for fanar
2025-07-07 10:17:27,473 - INFO -   Ground truth verses: ['7:21']
2025-07-07 10:17:27,473 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:27,639 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:27,639 - WARNING -   Failed to get response for question 821
2025-07-07 10:17:27,639 - INFO - Processing question 710/881 (ID: 822) for fanar
2025-07-07 10:17:27,639 - INFO -   Ground truth verses: ['7:21']
2025-07-07 10:17:27,639 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:27,803 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:27,804 - WARNING -   Failed to get response for question 822
2025-07-07 10:17:27,829 - INFO - Saved data to results/fanar_checkpoint_709.json
2025-07-07 10:17:27,829 - INFO - Checkpoint saved for fanar at question 710
2025-07-07 10:17:27,829 - INFO -   Progress: 710/881
2025-07-07 10:17:27,829 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:27,829 - INFO - Processing question 711/881 (ID: 823) for fanar
2025-07-07 10:17:27,829 - INFO -   Ground truth verses: ['7:21']
2025-07-07 10:17:27,829 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:28,208 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:28,209 - WARNING -   Failed to get response for question 823
2025-07-07 10:17:28,209 - INFO - Processing question 712/881 (ID: 824) for fanar
2025-07-07 10:17:28,209 - INFO -   Ground truth verses: ['7:21']
2025-07-07 10:17:28,209 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:28,371 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:28,372 - WARNING -   Failed to get response for question 824
2025-07-07 10:17:28,372 - INFO - Processing question 713/881 (ID: 825) for fanar
2025-07-07 10:17:28,372 - INFO -   Ground truth verses: ['7:21']
2025-07-07 10:17:28,372 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:28,534 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:28,535 - WARNING -   Failed to get response for question 825
2025-07-07 10:17:28,535 - INFO - Processing question 714/881 (ID: 826) for fanar
2025-07-07 10:17:28,535 - INFO -   Ground truth verses: ['68:48', '52:48', '76:24']
2025-07-07 10:17:28,535 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:28,702 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:28,702 - WARNING -   Failed to get response for question 826
2025-07-07 10:17:28,702 - INFO - Processing question 715/881 (ID: 827) for fanar
2025-07-07 10:17:28,702 - INFO -   Ground truth verses: ['68:48', '52:48', '76:24']
2025-07-07 10:17:28,702 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:28,865 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:28,866 - WARNING -   Failed to get response for question 827
2025-07-07 10:17:28,890 - INFO - Saved data to results/fanar_checkpoint_714.json
2025-07-07 10:17:28,890 - INFO - Checkpoint saved for fanar at question 715
2025-07-07 10:17:28,890 - INFO -   Progress: 715/881
2025-07-07 10:17:28,890 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:28,890 - INFO - Processing question 716/881 (ID: 828) for fanar
2025-07-07 10:17:28,890 - INFO -   Ground truth verses: ['68:48', '52:48', '76:24']
2025-07-07 10:17:28,890 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:29,053 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:29,053 - WARNING -   Failed to get response for question 828
2025-07-07 10:17:29,053 - INFO - Processing question 717/881 (ID: 829) for fanar
2025-07-07 10:17:29,053 - INFO -   Ground truth verses: ['68:48', '52:48', '76:24']
2025-07-07 10:17:29,053 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:29,242 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:29,242 - WARNING -   Failed to get response for question 829
2025-07-07 10:17:29,243 - INFO - Processing question 718/881 (ID: 830) for fanar
2025-07-07 10:17:29,243 - INFO -   Ground truth verses: ['68:48', '52:48', '76:24']
2025-07-07 10:17:29,243 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:29,407 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:29,407 - WARNING -   Failed to get response for question 830
2025-07-07 10:17:29,407 - INFO - Processing question 719/881 (ID: 831) for fanar
2025-07-07 10:17:29,408 - INFO -   Ground truth verses: ['88:19']
2025-07-07 10:17:29,408 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:29,790 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:29,791 - WARNING -   Failed to get response for question 831
2025-07-07 10:17:29,791 - INFO - Processing question 720/881 (ID: 832) for fanar
2025-07-07 10:17:29,791 - INFO -   Ground truth verses: ['88:19']
2025-07-07 10:17:29,791 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:29,956 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:29,956 - WARNING -   Failed to get response for question 832
2025-07-07 10:17:29,980 - INFO - Saved data to results/fanar_checkpoint_719.json
2025-07-07 10:17:29,980 - INFO - Checkpoint saved for fanar at question 720
2025-07-07 10:17:29,980 - INFO -   Progress: 720/881
2025-07-07 10:17:29,980 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:29,980 - INFO - Processing question 721/881 (ID: 833) for fanar
2025-07-07 10:17:29,980 - INFO -   Ground truth verses: ['88:19']
2025-07-07 10:17:29,980 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:30,144 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:30,144 - WARNING -   Failed to get response for question 833
2025-07-07 10:17:30,145 - INFO - Processing question 722/881 (ID: 834) for fanar
2025-07-07 10:17:30,145 - INFO -   Ground truth verses: ['88:19']
2025-07-07 10:17:30,145 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:30,308 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:30,308 - WARNING -   Failed to get response for question 834
2025-07-07 10:17:30,308 - INFO - Processing question 723/881 (ID: 835) for fanar
2025-07-07 10:17:30,308 - INFO -   Ground truth verses: ['88:19']
2025-07-07 10:17:30,308 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:30,471 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:30,472 - WARNING -   Failed to get response for question 835
2025-07-07 10:17:30,472 - INFO - Processing question 724/881 (ID: 836) for fanar
2025-07-07 10:17:30,472 - INFO -   Ground truth verses: ['23:87', '26:106', '26:161', '37:124', '26:142', '26:124', '26:177']
2025-07-07 10:17:30,472 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:30,638 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:30,639 - WARNING -   Failed to get response for question 836
2025-07-07 10:17:30,639 - INFO - Processing question 725/881 (ID: 837) for fanar
2025-07-07 10:17:30,639 - INFO -   Ground truth verses: ['23:87', '26:106', '26:161', '37:124', '26:142', '26:124', '26:177']
2025-07-07 10:17:30,639 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:30,804 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:30,805 - WARNING -   Failed to get response for question 837
2025-07-07 10:17:30,831 - INFO - Saved data to results/fanar_checkpoint_724.json
2025-07-07 10:17:30,831 - INFO - Checkpoint saved for fanar at question 725
2025-07-07 10:17:30,831 - INFO -   Progress: 725/881
2025-07-07 10:17:30,831 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:30,831 - INFO - Processing question 726/881 (ID: 838) for fanar
2025-07-07 10:17:30,831 - INFO -   Ground truth verses: ['23:87', '26:106', '26:161', '37:124', '26:142', '26:124', '26:177']
2025-07-07 10:17:30,831 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:30,996 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:30,997 - WARNING -   Failed to get response for question 838
2025-07-07 10:17:30,997 - INFO - Processing question 727/881 (ID: 839) for fanar
2025-07-07 10:17:30,997 - INFO -   Ground truth verses: ['23:87', '26:106', '26:161', '37:124', '26:142', '26:124', '26:177']
2025-07-07 10:17:30,997 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:31,162 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:31,163 - WARNING -   Failed to get response for question 839
2025-07-07 10:17:31,163 - INFO - Processing question 728/881 (ID: 840) for fanar
2025-07-07 10:17:31,163 - INFO -   Ground truth verses: ['23:87', '26:106', '26:161', '37:124', '26:142', '26:124', '26:177']
2025-07-07 10:17:31,163 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:31,529 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:31,530 - WARNING -   Failed to get response for question 840
2025-07-07 10:17:31,530 - INFO - Processing question 729/881 (ID: 841) for fanar
2025-07-07 10:17:31,530 - INFO -   Ground truth verses: ['76:19', '56:17']
2025-07-07 10:17:31,530 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:31,699 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:31,699 - WARNING -   Failed to get response for question 841
2025-07-07 10:17:31,700 - INFO - Processing question 730/881 (ID: 842) for fanar
2025-07-07 10:17:31,700 - INFO -   Ground truth verses: ['76:19', '56:17']
2025-07-07 10:17:31,700 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:31,790 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 10:17:31,794 - INFO - Loaded 6236 verses for validation
2025-07-07 10:17:31,795 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 10:17:31,795 - INFO - Models to test: deepseek
2025-07-07 10:17:31,795 - INFO - Questions limit: All
2025-07-07 10:17:31,795 - INFO - Using device: cuda
2025-07-07 10:17:31,874 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:31,875 - WARNING -   Failed to get response for question 842
2025-07-07 10:17:31,900 - INFO - Saved data to results/fanar_checkpoint_729.json
2025-07-07 10:17:31,900 - INFO - Checkpoint saved for fanar at question 730
2025-07-07 10:17:31,900 - INFO -   Progress: 730/881
2025-07-07 10:17:31,900 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:31,900 - INFO - Processing question 731/881 (ID: 843) for fanar
2025-07-07 10:17:31,900 - INFO -   Ground truth verses: ['76:19', '56:17']
2025-07-07 10:17:31,900 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:32,010 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:17:32,010 - INFO - 
============================================================
2025-07-07 10:17:32,010 - INFO - Starting evaluation for DEEPSEEK
2025-07-07 10:17:32,011 - INFO - ============================================================
2025-07-07 10:17:32,011 - INFO - Starting benchmark for DEEPSEEK
2025-07-07 10:17:32,011 - INFO - Using device: cuda
2025-07-07 10:17:32,011 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:17:32,013 - INFO - Loaded checkpoint for deepseek: 5 questions completed
2025-07-07 10:17:32,013 - INFO - Processing 876 questions for deepseek (starting from 6)
2025-07-07 10:17:32,013 - INFO - Processing question 6/881 (ID: 5) for deepseek
2025-07-07 10:17:32,013 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:17:32,013 - INFO - Loading model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:17:32,013 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 10:17:32,065 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:32,066 - WARNING -   Failed to get response for question 843
2025-07-07 10:17:32,066 - INFO - Processing question 732/881 (ID: 844) for fanar
2025-07-07 10:17:32,066 - INFO -   Ground truth verses: ['76:19', '56:17']
2025-07-07 10:17:32,066 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:32,227 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:32,227 - WARNING -   Failed to get response for question 844
2025-07-07 10:17:32,228 - INFO - Processing question 733/881 (ID: 845) for fanar
2025-07-07 10:17:32,228 - INFO -   Ground truth verses: ['76:19', '56:17']
2025-07-07 10:17:32,228 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:32,390 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:32,391 - WARNING -   Failed to get response for question 845
2025-07-07 10:17:32,391 - INFO - Processing question 734/881 (ID: 846) for fanar
2025-07-07 10:17:32,391 - INFO -   Ground truth verses: ['12:20']
2025-07-07 10:17:32,391 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:32,558 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:32,559 - WARNING -   Failed to get response for question 846
2025-07-07 10:17:32,559 - INFO - Processing question 735/881 (ID: 847) for fanar
2025-07-07 10:17:32,559 - INFO -   Ground truth verses: ['12:20']
2025-07-07 10:17:32,559 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:32,729 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:32,730 - WARNING -   Failed to get response for question 847
2025-07-07 10:17:32,754 - INFO - Saved data to results/fanar_checkpoint_734.json
2025-07-07 10:17:32,754 - INFO - Checkpoint saved for fanar at question 735
2025-07-07 10:17:32,754 - INFO -   Progress: 735/881
2025-07-07 10:17:32,754 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:32,754 - INFO - Processing question 736/881 (ID: 848) for fanar
2025-07-07 10:17:32,754 - INFO -   Ground truth verses: ['12:20']
2025-07-07 10:17:32,754 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:33,128 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:33,129 - WARNING -   Failed to get response for question 848
2025-07-07 10:17:33,129 - INFO - Processing question 737/881 (ID: 849) for fanar
2025-07-07 10:17:33,129 - INFO -   Ground truth verses: ['12:20']
2025-07-07 10:17:33,129 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:33,295 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:33,295 - WARNING -   Failed to get response for question 849
2025-07-07 10:17:33,295 - INFO - Processing question 738/881 (ID: 850) for fanar
2025-07-07 10:17:33,295 - INFO -   Ground truth verses: ['12:20']
2025-07-07 10:17:33,295 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:33,463 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:33,464 - WARNING -   Failed to get response for question 850
2025-07-07 10:17:33,464 - INFO - Processing question 739/881 (ID: 851) for fanar
2025-07-07 10:17:33,464 - INFO -   Ground truth verses: ['3:46']
2025-07-07 10:17:33,464 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:33,635 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:33,635 - WARNING -   Failed to get response for question 851
2025-07-07 10:17:33,635 - INFO - Processing question 740/881 (ID: 852) for fanar
2025-07-07 10:17:33,636 - INFO -   Ground truth verses: ['3:46']
2025-07-07 10:17:33,636 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:33,801 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:33,802 - WARNING -   Failed to get response for question 852
2025-07-07 10:17:33,829 - INFO - Saved data to results/fanar_checkpoint_739.json
2025-07-07 10:17:33,829 - INFO - Checkpoint saved for fanar at question 740
2025-07-07 10:17:33,829 - INFO -   Progress: 740/881
2025-07-07 10:17:33,829 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:33,829 - INFO - Processing question 741/881 (ID: 853) for fanar
2025-07-07 10:17:33,829 - INFO -   Ground truth verses: ['3:46']
2025-07-07 10:17:33,829 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:33,995 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:33,996 - WARNING -   Failed to get response for question 853
2025-07-07 10:17:33,996 - INFO - Processing question 742/881 (ID: 854) for fanar
2025-07-07 10:17:33,996 - INFO -   Ground truth verses: ['3:46']
2025-07-07 10:17:33,996 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:34,161 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:34,162 - WARNING -   Failed to get response for question 854
2025-07-07 10:17:34,162 - INFO - Processing question 743/881 (ID: 855) for fanar
2025-07-07 10:17:34,162 - INFO -   Ground truth verses: ['3:46']
2025-07-07 10:17:34,162 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:34,325 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:34,326 - WARNING -   Failed to get response for question 855
2025-07-07 10:17:34,326 - INFO - Processing question 744/881 (ID: 856) for fanar
2025-07-07 10:17:34,326 - INFO -   Ground truth verses: ['35:39']
2025-07-07 10:17:34,326 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:34,490 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:34,490 - WARNING -   Failed to get response for question 856
2025-07-07 10:17:34,490 - INFO - Processing question 745/881 (ID: 857) for fanar
2025-07-07 10:17:34,490 - INFO -   Ground truth verses: ['35:39']
2025-07-07 10:17:34,490 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:34,872 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:34,872 - WARNING -   Failed to get response for question 857
2025-07-07 10:17:34,900 - INFO - Saved data to results/fanar_checkpoint_744.json
2025-07-07 10:17:34,900 - INFO - Checkpoint saved for fanar at question 745
2025-07-07 10:17:34,900 - INFO -   Progress: 745/881
2025-07-07 10:17:34,900 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:34,900 - INFO - Processing question 746/881 (ID: 858) for fanar
2025-07-07 10:17:34,900 - INFO -   Ground truth verses: ['35:39']
2025-07-07 10:17:34,900 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:35,065 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:35,065 - WARNING -   Failed to get response for question 858
2025-07-07 10:17:35,065 - INFO - Processing question 747/881 (ID: 859) for fanar
2025-07-07 10:17:35,065 - INFO -   Ground truth verses: ['35:39']
2025-07-07 10:17:35,065 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:35,234 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:35,234 - WARNING -   Failed to get response for question 859
2025-07-07 10:17:35,234 - INFO - Processing question 748/881 (ID: 860) for fanar
2025-07-07 10:17:35,234 - INFO -   Ground truth verses: ['35:39']
2025-07-07 10:17:35,234 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:35,398 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:35,399 - WARNING -   Failed to get response for question 860
2025-07-07 10:17:35,399 - INFO - Processing question 749/881 (ID: 861) for fanar
2025-07-07 10:17:35,399 - INFO -   Ground truth verses: ['68:25']
2025-07-07 10:17:35,399 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:35,462 - INFO - Successfully loaded suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:17:35,564 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:35,565 - WARNING -   Failed to get response for question 861
2025-07-07 10:17:35,565 - INFO - Processing question 750/881 (ID: 862) for fanar
2025-07-07 10:17:35,565 - INFO -   Ground truth verses: ['68:25']
2025-07-07 10:17:35,565 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:35,733 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:35,733 - WARNING -   Failed to get response for question 862
2025-07-07 10:17:35,760 - INFO - Saved data to results/fanar_checkpoint_749.json
2025-07-07 10:17:35,760 - INFO - Checkpoint saved for fanar at question 750
2025-07-07 10:17:35,760 - INFO -   Progress: 750/881
2025-07-07 10:17:35,760 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:35,761 - INFO - Processing question 751/881 (ID: 863) for fanar
2025-07-07 10:17:35,761 - INFO -   Ground truth verses: ['68:25']
2025-07-07 10:17:35,761 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:35,927 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:35,928 - WARNING -   Failed to get response for question 863
2025-07-07 10:17:35,928 - INFO - Processing question 752/881 (ID: 864) for fanar
2025-07-07 10:17:35,928 - INFO -   Ground truth verses: ['68:25']
2025-07-07 10:17:35,928 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:36,092 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:36,093 - WARNING -   Failed to get response for question 864
2025-07-07 10:17:36,093 - INFO - Processing question 753/881 (ID: 865) for fanar
2025-07-07 10:17:36,093 - INFO -   Ground truth verses: ['68:25']
2025-07-07 10:17:36,093 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:36,469 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:36,470 - WARNING -   Failed to get response for question 865
2025-07-07 10:17:36,470 - INFO - Processing question 754/881 (ID: 866) for fanar
2025-07-07 10:17:36,470 - INFO -   Ground truth verses: ['6:48', '10:62', '46:13', '6:130', '7:35']
2025-07-07 10:17:36,470 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:36,675 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:36,676 - WARNING -   Failed to get response for question 866
2025-07-07 10:17:36,676 - INFO - Processing question 755/881 (ID: 867) for fanar
2025-07-07 10:17:36,676 - INFO -   Ground truth verses: ['6:48', '10:62', '46:13', '6:130', '7:35']
2025-07-07 10:17:36,676 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:36,872 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:36,873 - WARNING -   Failed to get response for question 867
2025-07-07 10:17:36,898 - INFO - Saved data to results/fanar_checkpoint_754.json
2025-07-07 10:17:36,898 - INFO - Checkpoint saved for fanar at question 755
2025-07-07 10:17:36,898 - INFO -   Progress: 755/881
2025-07-07 10:17:36,898 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:36,898 - INFO - Processing question 756/881 (ID: 868) for fanar
2025-07-07 10:17:36,898 - INFO -   Ground truth verses: ['6:48', '10:62', '46:13', '6:130', '7:35']
2025-07-07 10:17:36,898 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:37,106 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:37,107 - WARNING -   Failed to get response for question 868
2025-07-07 10:17:37,107 - INFO - Processing question 757/881 (ID: 869) for fanar
2025-07-07 10:17:37,107 - INFO -   Ground truth verses: ['6:48', '10:62', '46:13', '6:130', '7:35']
2025-07-07 10:17:37,107 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:37,306 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:37,307 - WARNING -   Failed to get response for question 869
2025-07-07 10:17:37,307 - INFO - Processing question 758/881 (ID: 870) for fanar
2025-07-07 10:17:37,307 - INFO -   Ground truth verses: ['6:48', '10:62', '46:13', '6:130', '7:35']
2025-07-07 10:17:37,307 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:37,506 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:37,506 - WARNING -   Failed to get response for question 870
2025-07-07 10:17:37,506 - INFO - Processing question 759/881 (ID: 871) for fanar
2025-07-07 10:17:37,506 - INFO -   Ground truth verses: ['28:81', '18:43', '40:33', '42:44', '39:36', '4:88', '42:46']
2025-07-07 10:17:37,507 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:37,709 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:37,710 - WARNING -   Failed to get response for question 871
2025-07-07 10:17:37,710 - INFO - Processing question 760/881 (ID: 872) for fanar
2025-07-07 10:17:37,710 - INFO -   Ground truth verses: ['28:81', '18:43', '40:33', '42:44', '39:36', '4:88', '42:46']
2025-07-07 10:17:37,710 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:37,906 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:37,906 - WARNING -   Failed to get response for question 872
2025-07-07 10:17:37,932 - INFO - Saved data to results/fanar_checkpoint_759.json
2025-07-07 10:17:37,932 - INFO - Checkpoint saved for fanar at question 760
2025-07-07 10:17:37,932 - INFO -   Progress: 760/881
2025-07-07 10:17:37,932 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:37,932 - INFO - Processing question 761/881 (ID: 873) for fanar
2025-07-07 10:17:37,932 - INFO -   Ground truth verses: ['28:81', '18:43', '40:33', '42:44', '39:36', '4:88', '42:46']
2025-07-07 10:17:37,932 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:38,140 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:38,141 - WARNING -   Failed to get response for question 873
2025-07-07 10:17:38,141 - INFO - Processing question 762/881 (ID: 874) for fanar
2025-07-07 10:17:38,141 - INFO -   Ground truth verses: ['28:81', '18:43', '40:33', '42:44', '39:36', '4:88', '42:46']
2025-07-07 10:17:38,141 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:38,211 - INFO - Extracted JSON from code block: 756 characters
2025-07-07 10:17:38,212 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:17:38,212 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-An'am:64]' -> 'Al-An'am:64'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Hijr:79]' -> 'Al-Hijr:79'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:17:38,212 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:17:38,212 - INFO - Normalized predicted verses: ['15:79', '2:255', '2:64', '50:5', '5:104']
2025-07-07 10:17:38,212 - INFO - Normalized ground truth verses: ['15:11', '16:34', '26:6', '43:7', '6:5']
2025-07-07 10:17:38,212 - INFO -   Response: 756 chars, Found 5 verses
2025-07-07 10:17:38,212 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:38,212 - INFO -   Matched verses: []
2025-07-07 10:17:38,535 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:38,536 - WARNING -   Failed to get response for question 874
2025-07-07 10:17:38,536 - INFO - Processing question 763/881 (ID: 875) for fanar
2025-07-07 10:17:38,536 - INFO -   Ground truth verses: ['28:81', '18:43', '40:33', '42:44', '39:36', '4:88', '42:46']
2025-07-07 10:17:38,536 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:38,705 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:38,706 - WARNING -   Failed to get response for question 875
2025-07-07 10:17:38,706 - INFO - Processing question 764/881 (ID: 876) for fanar
2025-07-07 10:17:38,706 - INFO -   Ground truth verses: ['77:31', '88:7']
2025-07-07 10:17:38,706 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:38,876 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:38,877 - WARNING -   Failed to get response for question 876
2025-07-07 10:17:38,877 - INFO - Processing question 765/881 (ID: 877) for fanar
2025-07-07 10:17:38,877 - INFO -   Ground truth verses: ['77:31', '88:7']
2025-07-07 10:17:38,877 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:39,042 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:39,043 - WARNING -   Failed to get response for question 877
2025-07-07 10:17:39,068 - INFO - Saved data to results/fanar_checkpoint_764.json
2025-07-07 10:17:39,068 - INFO - Checkpoint saved for fanar at question 765
2025-07-07 10:17:39,068 - INFO -   Progress: 765/881
2025-07-07 10:17:39,068 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:39,068 - INFO - Processing question 766/881 (ID: 878) for fanar
2025-07-07 10:17:39,068 - INFO -   Ground truth verses: ['77:31', '88:7']
2025-07-07 10:17:39,068 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:39,213 - INFO - Processing question 303/881 (ID: 343) for qwen
2025-07-07 10:17:39,214 - INFO -   Ground truth verses: ['6:5', '43:7', '15:11', '16:34', '26:6']
2025-07-07 10:17:39,214 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:17:39,238 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:39,239 - WARNING -   Failed to get response for question 878
2025-07-07 10:17:39,239 - INFO - Processing question 767/881 (ID: 879) for fanar
2025-07-07 10:17:39,239 - INFO -   Ground truth verses: ['77:31', '88:7']
2025-07-07 10:17:39,239 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:39,431 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:39,431 - WARNING -   Failed to get response for question 879
2025-07-07 10:17:39,431 - INFO - Processing question 768/881 (ID: 880) for fanar
2025-07-07 10:17:39,431 - INFO -   Ground truth verses: ['77:31', '88:7']
2025-07-07 10:17:39,432 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:39,636 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:39,637 - WARNING -   Failed to get response for question 880
2025-07-07 10:17:39,637 - INFO - Processing question 769/881 (ID: 881) for fanar
2025-07-07 10:17:39,637 - INFO -   Ground truth verses: ['54:10', '44:22']
2025-07-07 10:17:39,637 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:39,832 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:39,833 - WARNING -   Failed to get response for question 881
2025-07-07 10:17:39,833 - INFO - Processing question 770/881 (ID: 882) for fanar
2025-07-07 10:17:39,833 - INFO -   Ground truth verses: ['54:10', '44:22']
2025-07-07 10:17:39,833 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:40,030 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:40,030 - WARNING -   Failed to get response for question 882
2025-07-07 10:17:40,056 - INFO - Saved data to results/fanar_checkpoint_769.json
2025-07-07 10:17:40,056 - INFO - Checkpoint saved for fanar at question 770
2025-07-07 10:17:40,056 - INFO -   Progress: 770/881
2025-07-07 10:17:40,056 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:40,056 - INFO - Processing question 771/881 (ID: 883) for fanar
2025-07-07 10:17:40,056 - INFO -   Ground truth verses: ['54:10', '44:22']
2025-07-07 10:17:40,056 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:40,442 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:40,442 - WARNING -   Failed to get response for question 883
2025-07-07 10:17:40,442 - INFO - Processing question 772/881 (ID: 884) for fanar
2025-07-07 10:17:40,442 - INFO -   Ground truth verses: ['54:10', '44:22']
2025-07-07 10:17:40,442 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:40,636 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:40,636 - WARNING -   Failed to get response for question 884
2025-07-07 10:17:40,637 - INFO - Processing question 773/881 (ID: 885) for fanar
2025-07-07 10:17:40,637 - INFO -   Ground truth verses: ['54:10', '44:22']
2025-07-07 10:17:40,637 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:40,840 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:40,840 - WARNING -   Failed to get response for question 885
2025-07-07 10:17:40,840 - INFO - Processing question 774/881 (ID: 886) for fanar
2025-07-07 10:17:40,840 - INFO -   Ground truth verses: ['4:126', '22:64', '4:132', '61:1', '59:1', '3:109', '31:26', '62:1', '4:170', '64:1', '42:4', '3:129', '14:2', '42:53', '53:31', '34:1', '57:1', '20:6', '2:284', '16:49', '49:16', '45:13', '10:55', '3:29', '4:131']
2025-07-07 10:17:40,840 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:41,033 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:41,034 - WARNING -   Failed to get response for question 886
2025-07-07 10:17:41,034 - INFO - Processing question 775/881 (ID: 887) for fanar
2025-07-07 10:17:41,034 - INFO -   Ground truth verses: ['4:126', '22:64', '4:132', '61:1', '59:1', '3:109', '31:26', '62:1', '4:170', '64:1', '42:4', '3:129', '14:2', '42:53', '53:31', '34:1', '57:1', '20:6', '2:284', '16:49', '49:16', '45:13', '10:55', '3:29', '4:131']
2025-07-07 10:17:41,034 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:41,230 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:41,230 - WARNING -   Failed to get response for question 887
2025-07-07 10:17:41,256 - INFO - Saved data to results/fanar_checkpoint_774.json
2025-07-07 10:17:41,256 - INFO - Checkpoint saved for fanar at question 775
2025-07-07 10:17:41,256 - INFO -   Progress: 775/881
2025-07-07 10:17:41,256 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:41,256 - INFO - Processing question 776/881 (ID: 888) for fanar
2025-07-07 10:17:41,256 - INFO -   Ground truth verses: ['4:126', '22:64', '4:132', '61:1', '59:1', '3:109', '31:26', '62:1', '4:170', '64:1', '42:4', '3:129', '14:2', '42:53', '53:31', '34:1', '57:1', '20:6', '2:284', '16:49', '49:16', '45:13', '10:55', '3:29', '4:131']
2025-07-07 10:17:41,256 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:41,459 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:41,460 - WARNING -   Failed to get response for question 888
2025-07-07 10:17:41,460 - INFO - Processing question 777/881 (ID: 889) for fanar
2025-07-07 10:17:41,460 - INFO -   Ground truth verses: ['4:126', '22:64', '4:132', '61:1', '59:1', '3:109', '31:26', '62:1', '4:170', '64:1', '42:4', '3:129', '14:2', '42:53', '53:31', '34:1', '57:1', '20:6', '2:284', '16:49', '49:16', '45:13', '10:55', '3:29', '4:131']
2025-07-07 10:17:41,460 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:41,659 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:41,660 - WARNING -   Failed to get response for question 889
2025-07-07 10:17:41,660 - INFO - Processing question 778/881 (ID: 890) for fanar
2025-07-07 10:17:41,660 - INFO -   Ground truth verses: ['4:126', '22:64', '4:132', '61:1', '59:1', '3:109', '31:26', '62:1', '4:170', '64:1', '42:4', '3:129', '14:2', '42:53', '53:31', '34:1', '57:1', '20:6', '2:284', '16:49', '49:16', '45:13', '10:55', '3:29', '4:131']
2025-07-07 10:17:41,660 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:41,855 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:41,855 - WARNING -   Failed to get response for question 890
2025-07-07 10:17:41,855 - INFO - Processing question 779/881 (ID: 891) for fanar
2025-07-07 10:17:41,855 - INFO -   Ground truth verses: ['70:42', '52:45', '51:60', '43:83']
2025-07-07 10:17:41,855 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:42,239 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:42,240 - WARNING -   Failed to get response for question 891
2025-07-07 10:17:42,240 - INFO - Processing question 780/881 (ID: 892) for fanar
2025-07-07 10:17:42,240 - INFO -   Ground truth verses: ['70:42', '52:45', '51:60', '43:83']
2025-07-07 10:17:42,240 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:42,442 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:42,442 - WARNING -   Failed to get response for question 892
2025-07-07 10:17:42,467 - INFO - Saved data to results/fanar_checkpoint_779.json
2025-07-07 10:17:42,467 - INFO - Checkpoint saved for fanar at question 780
2025-07-07 10:17:42,467 - INFO -   Progress: 780/881
2025-07-07 10:17:42,467 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:42,467 - INFO - Processing question 781/881 (ID: 893) for fanar
2025-07-07 10:17:42,467 - INFO -   Ground truth verses: ['70:42', '52:45', '51:60', '43:83']
2025-07-07 10:17:42,467 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:42,671 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:42,672 - WARNING -   Failed to get response for question 893
2025-07-07 10:17:42,672 - INFO - Processing question 782/881 (ID: 894) for fanar
2025-07-07 10:17:42,672 - INFO -   Ground truth verses: ['70:42', '52:45', '51:60', '43:83']
2025-07-07 10:17:42,672 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:42,872 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:42,872 - WARNING -   Failed to get response for question 894
2025-07-07 10:17:42,873 - INFO - Processing question 783/881 (ID: 895) for fanar
2025-07-07 10:17:42,873 - INFO -   Ground truth verses: ['70:42', '52:45', '51:60', '43:83']
2025-07-07 10:17:42,873 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:43,069 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:43,069 - WARNING -   Failed to get response for question 895
2025-07-07 10:17:43,069 - INFO - Processing question 784/881 (ID: 896) for fanar
2025-07-07 10:17:43,069 - INFO -   Ground truth verses: ['66:1']
2025-07-07 10:17:43,069 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:43,269 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:43,270 - WARNING -   Failed to get response for question 896
2025-07-07 10:17:43,270 - INFO - Processing question 785/881 (ID: 897) for fanar
2025-07-07 10:17:43,270 - INFO -   Ground truth verses: ['66:1']
2025-07-07 10:17:43,270 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:43,464 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:43,465 - WARNING -   Failed to get response for question 897
2025-07-07 10:17:43,491 - INFO - Saved data to results/fanar_checkpoint_784.json
2025-07-07 10:17:43,491 - INFO - Checkpoint saved for fanar at question 785
2025-07-07 10:17:43,491 - INFO -   Progress: 785/881
2025-07-07 10:17:43,491 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:43,491 - INFO - Processing question 786/881 (ID: 898) for fanar
2025-07-07 10:17:43,491 - INFO -   Ground truth verses: ['66:1']
2025-07-07 10:17:43,491 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:43,703 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:43,703 - WARNING -   Failed to get response for question 898
2025-07-07 10:17:43,703 - INFO - Processing question 787/881 (ID: 899) for fanar
2025-07-07 10:17:43,703 - INFO -   Ground truth verses: ['66:1']
2025-07-07 10:17:43,703 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:43,900 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:43,901 - WARNING -   Failed to get response for question 899
2025-07-07 10:17:43,901 - INFO - Processing question 788/881 (ID: 900) for fanar
2025-07-07 10:17:43,901 - INFO -   Ground truth verses: ['66:1']
2025-07-07 10:17:43,901 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:44,306 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:44,307 - WARNING -   Failed to get response for question 900
2025-07-07 10:17:44,307 - INFO - Processing question 789/881 (ID: 901) for fanar
2025-07-07 10:17:44,307 - INFO -   Ground truth verses: ['9:119', '33:70', '3:102', '59:18', '2:278']
2025-07-07 10:17:44,307 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:44,506 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:44,507 - WARNING -   Failed to get response for question 901
2025-07-07 10:17:44,507 - INFO - Processing question 790/881 (ID: 902) for fanar
2025-07-07 10:17:44,507 - INFO -   Ground truth verses: ['9:119', '33:70', '3:102', '59:18', '2:278']
2025-07-07 10:17:44,507 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:44,706 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:44,707 - WARNING -   Failed to get response for question 902
2025-07-07 10:17:44,733 - INFO - Saved data to results/fanar_checkpoint_789.json
2025-07-07 10:17:44,733 - INFO - Checkpoint saved for fanar at question 790
2025-07-07 10:17:44,733 - INFO -   Progress: 790/881
2025-07-07 10:17:44,733 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:44,733 - INFO - Processing question 791/881 (ID: 903) for fanar
2025-07-07 10:17:44,733 - INFO -   Ground truth verses: ['9:119', '33:70', '3:102', '59:18', '2:278']
2025-07-07 10:17:44,733 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:44,939 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:44,940 - WARNING -   Failed to get response for question 903
2025-07-07 10:17:44,940 - INFO - Processing question 792/881 (ID: 904) for fanar
2025-07-07 10:17:44,940 - INFO -   Ground truth verses: ['9:119', '33:70', '3:102', '59:18', '2:278']
2025-07-07 10:17:44,940 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:45,138 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:45,139 - WARNING -   Failed to get response for question 904
2025-07-07 10:17:45,139 - INFO - Processing question 793/881 (ID: 905) for fanar
2025-07-07 10:17:45,139 - INFO -   Ground truth verses: ['9:119', '33:70', '3:102', '59:18', '2:278']
2025-07-07 10:17:45,139 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:45,338 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:45,339 - WARNING -   Failed to get response for question 905
2025-07-07 10:17:45,339 - INFO - Processing question 794/881 (ID: 906) for fanar
2025-07-07 10:17:45,339 - INFO -   Ground truth verses: ['75:5']
2025-07-07 10:17:45,339 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:45,537 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:45,538 - WARNING -   Failed to get response for question 906
2025-07-07 10:17:45,538 - INFO - Processing question 795/881 (ID: 907) for fanar
2025-07-07 10:17:45,538 - INFO -   Ground truth verses: ['75:5']
2025-07-07 10:17:45,538 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:45,739 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:45,739 - WARNING -   Failed to get response for question 907
2025-07-07 10:17:45,767 - INFO - Saved data to results/fanar_checkpoint_794.json
2025-07-07 10:17:45,767 - INFO - Checkpoint saved for fanar at question 795
2025-07-07 10:17:45,767 - INFO -   Progress: 795/881
2025-07-07 10:17:45,767 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:45,767 - INFO - Processing question 796/881 (ID: 908) for fanar
2025-07-07 10:17:45,767 - INFO -   Ground truth verses: ['75:5']
2025-07-07 10:17:45,767 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:45,972 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:45,973 - WARNING -   Failed to get response for question 908
2025-07-07 10:17:45,973 - INFO - Processing question 797/881 (ID: 909) for fanar
2025-07-07 10:17:45,973 - INFO -   Ground truth verses: ['75:5']
2025-07-07 10:17:45,973 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:46,386 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:46,387 - WARNING -   Failed to get response for question 909
2025-07-07 10:17:46,387 - INFO - Processing question 798/881 (ID: 910) for fanar
2025-07-07 10:17:46,387 - INFO -   Ground truth verses: ['75:5']
2025-07-07 10:17:46,387 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:46,588 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:46,589 - WARNING -   Failed to get response for question 910
2025-07-07 10:17:46,589 - INFO - Processing question 799/881 (ID: 911) for fanar
2025-07-07 10:17:46,589 - INFO -   Ground truth verses: ['36:71']
2025-07-07 10:17:46,589 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:46,790 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:46,790 - WARNING -   Failed to get response for question 911
2025-07-07 10:17:46,790 - INFO - Processing question 800/881 (ID: 912) for fanar
2025-07-07 10:17:46,790 - INFO -   Ground truth verses: ['36:71']
2025-07-07 10:17:46,790 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:46,990 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:46,990 - WARNING -   Failed to get response for question 912
2025-07-07 10:17:47,017 - INFO - Saved data to results/fanar_checkpoint_799.json
2025-07-07 10:17:47,017 - INFO - Checkpoint saved for fanar at question 800
2025-07-07 10:17:47,017 - INFO -   Progress: 800/881
2025-07-07 10:17:47,017 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:47,017 - INFO - Processing question 801/881 (ID: 913) for fanar
2025-07-07 10:17:47,017 - INFO -   Ground truth verses: ['36:71']
2025-07-07 10:17:47,017 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:47,229 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:47,230 - WARNING -   Failed to get response for question 913
2025-07-07 10:17:47,230 - INFO - Processing question 802/881 (ID: 914) for fanar
2025-07-07 10:17:47,230 - INFO -   Ground truth verses: ['36:71']
2025-07-07 10:17:47,230 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:47,430 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:47,431 - WARNING -   Failed to get response for question 914
2025-07-07 10:17:47,431 - INFO - Processing question 803/881 (ID: 915) for fanar
2025-07-07 10:17:47,431 - INFO -   Ground truth verses: ['36:71']
2025-07-07 10:17:47,431 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:47,628 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:47,629 - WARNING -   Failed to get response for question 915
2025-07-07 10:17:47,629 - INFO - Processing question 804/881 (ID: 916) for fanar
2025-07-07 10:17:47,629 - INFO -   Ground truth verses: ['14:20', '27:9', '35:17']
2025-07-07 10:17:47,629 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:47,828 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:47,828 - WARNING -   Failed to get response for question 916
2025-07-07 10:17:47,828 - INFO - Processing question 805/881 (ID: 917) for fanar
2025-07-07 10:17:47,828 - INFO -   Ground truth verses: ['14:20', '27:9', '35:17']
2025-07-07 10:17:47,828 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:48,241 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:48,241 - WARNING -   Failed to get response for question 917
2025-07-07 10:17:48,267 - INFO - Saved data to results/fanar_checkpoint_804.json
2025-07-07 10:17:48,267 - INFO - Checkpoint saved for fanar at question 805
2025-07-07 10:17:48,268 - INFO -   Progress: 805/881
2025-07-07 10:17:48,268 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:48,268 - INFO - Processing question 806/881 (ID: 918) for fanar
2025-07-07 10:17:48,268 - INFO -   Ground truth verses: ['14:20', '27:9', '35:17']
2025-07-07 10:17:48,268 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:48,474 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:48,475 - WARNING -   Failed to get response for question 918
2025-07-07 10:17:48,475 - INFO - Processing question 807/881 (ID: 919) for fanar
2025-07-07 10:17:48,475 - INFO -   Ground truth verses: ['14:20', '27:9', '35:17']
2025-07-07 10:17:48,475 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:48,679 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:48,679 - WARNING -   Failed to get response for question 919
2025-07-07 10:17:48,679 - INFO - Processing question 808/881 (ID: 920) for fanar
2025-07-07 10:17:48,679 - INFO -   Ground truth verses: ['14:20', '27:9', '35:17']
2025-07-07 10:17:48,679 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:48,879 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:48,880 - WARNING -   Failed to get response for question 920
2025-07-07 10:17:48,880 - INFO - Processing question 809/881 (ID: 921) for fanar
2025-07-07 10:17:48,880 - INFO -   Ground truth verses: ['21:14', '68:31', '21:46']
2025-07-07 10:17:48,880 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:49,077 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:49,078 - WARNING -   Failed to get response for question 921
2025-07-07 10:17:49,078 - INFO - Processing question 810/881 (ID: 922) for fanar
2025-07-07 10:17:49,078 - INFO -   Ground truth verses: ['21:14', '68:31', '21:46']
2025-07-07 10:17:49,078 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:49,277 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:49,278 - WARNING -   Failed to get response for question 922
2025-07-07 10:17:49,305 - INFO - Saved data to results/fanar_checkpoint_809.json
2025-07-07 10:17:49,305 - INFO - Checkpoint saved for fanar at question 810
2025-07-07 10:17:49,305 - INFO -   Progress: 810/881
2025-07-07 10:17:49,305 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:49,305 - INFO - Processing question 811/881 (ID: 924) for fanar
2025-07-07 10:17:49,305 - INFO -   Ground truth verses: ['21:14', '68:31', '21:46']
2025-07-07 10:17:49,305 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:49,512 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:49,512 - WARNING -   Failed to get response for question 924
2025-07-07 10:17:49,512 - INFO - Processing question 812/881 (ID: 925) for fanar
2025-07-07 10:17:49,512 - INFO -   Ground truth verses: ['21:14', '68:31', '21:46']
2025-07-07 10:17:49,512 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:49,715 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:49,716 - WARNING -   Failed to get response for question 925
2025-07-07 10:17:49,716 - INFO - Processing question 813/881 (ID: 926) for fanar
2025-07-07 10:17:49,716 - INFO -   Ground truth verses: ['30:9', '22:42', '14:9', '29:40', '38:12', '9:70']
2025-07-07 10:17:49,716 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:49,919 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:49,920 - WARNING -   Failed to get response for question 926
2025-07-07 10:17:49,920 - INFO - Processing question 814/881 (ID: 927) for fanar
2025-07-07 10:17:49,920 - INFO -   Ground truth verses: ['30:9', '22:42', '14:9', '29:40', '38:12', '9:70']
2025-07-07 10:17:49,920 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:50,331 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:50,331 - WARNING -   Failed to get response for question 927
2025-07-07 10:17:50,331 - INFO - Processing question 815/881 (ID: 928) for fanar
2025-07-07 10:17:50,331 - INFO -   Ground truth verses: ['30:9', '22:42', '14:9', '29:40', '38:12', '9:70']
2025-07-07 10:17:50,331 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:50,533 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:50,533 - WARNING -   Failed to get response for question 928
2025-07-07 10:17:50,561 - INFO - Saved data to results/fanar_checkpoint_814.json
2025-07-07 10:17:50,561 - INFO - Checkpoint saved for fanar at question 815
2025-07-07 10:17:50,561 - INFO -   Progress: 815/881
2025-07-07 10:17:50,561 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:50,561 - INFO - Processing question 816/881 (ID: 929) for fanar
2025-07-07 10:17:50,561 - INFO -   Ground truth verses: ['30:9', '22:42', '14:9', '29:40', '38:12', '9:70']
2025-07-07 10:17:50,561 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:50,767 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:50,768 - WARNING -   Failed to get response for question 929
2025-07-07 10:17:50,768 - INFO - Processing question 817/881 (ID: 930) for fanar
2025-07-07 10:17:50,768 - INFO -   Ground truth verses: ['30:9', '22:42', '14:9', '29:40', '38:12', '9:70']
2025-07-07 10:17:50,768 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:50,968 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:50,968 - WARNING -   Failed to get response for question 930
2025-07-07 10:17:50,969 - INFO - Processing question 818/881 (ID: 931) for fanar
2025-07-07 10:17:50,969 - INFO -   Ground truth verses: ['26:94']
2025-07-07 10:17:50,969 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:51,173 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:51,174 - WARNING -   Failed to get response for question 931
2025-07-07 10:17:51,174 - INFO - Processing question 819/881 (ID: 932) for fanar
2025-07-07 10:17:51,174 - INFO -   Ground truth verses: ['26:94']
2025-07-07 10:17:51,174 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:51,370 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:51,371 - WARNING -   Failed to get response for question 932
2025-07-07 10:17:51,371 - INFO - Processing question 820/881 (ID: 933) for fanar
2025-07-07 10:17:51,371 - INFO -   Ground truth verses: ['26:94']
2025-07-07 10:17:51,371 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:51,571 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:51,571 - WARNING -   Failed to get response for question 933
2025-07-07 10:17:51,582 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:17:51,582 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:17:51,582 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:17:51,582 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:17:51,583 - INFO - Normalized predicted verses: []
2025-07-07 10:17:51,583 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 10:17:51,583 - INFO -   Response: 3091 chars, Found 0 verses
2025-07-07 10:17:51,583 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:51,583 - INFO -   Matched verses: []
2025-07-07 10:17:51,583 - INFO - Processing question 7/881 (ID: 6) for deepseek
2025-07-07 10:17:51,583 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:17:51,583 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:17:51,598 - INFO - Saved data to results/fanar_checkpoint_819.json
2025-07-07 10:17:51,598 - INFO - Checkpoint saved for fanar at question 820
2025-07-07 10:17:51,598 - INFO -   Progress: 820/881
2025-07-07 10:17:51,598 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:51,598 - INFO - Processing question 821/881 (ID: 934) for fanar
2025-07-07 10:17:51,598 - INFO -   Ground truth verses: ['26:94']
2025-07-07 10:17:51,598 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:51,798 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:51,799 - WARNING -   Failed to get response for question 934
2025-07-07 10:17:51,799 - INFO - Processing question 822/881 (ID: 935) for fanar
2025-07-07 10:17:51,799 - INFO -   Ground truth verses: ['68:16']
2025-07-07 10:17:51,799 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:51,995 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:51,995 - WARNING -   Failed to get response for question 935
2025-07-07 10:17:51,995 - INFO - Processing question 823/881 (ID: 936) for fanar
2025-07-07 10:17:51,995 - INFO -   Ground truth verses: ['68:16']
2025-07-07 10:17:51,995 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:52,405 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:52,406 - WARNING -   Failed to get response for question 936
2025-07-07 10:17:52,406 - INFO - Processing question 824/881 (ID: 937) for fanar
2025-07-07 10:17:52,406 - INFO -   Ground truth verses: ['68:16']
2025-07-07 10:17:52,406 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:52,603 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:52,604 - WARNING -   Failed to get response for question 937
2025-07-07 10:17:52,604 - INFO - Processing question 825/881 (ID: 938) for fanar
2025-07-07 10:17:52,604 - INFO -   Ground truth verses: ['68:16']
2025-07-07 10:17:52,604 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:52,798 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:52,798 - WARNING -   Failed to get response for question 938
2025-07-07 10:17:52,825 - INFO - Saved data to results/fanar_checkpoint_824.json
2025-07-07 10:17:52,825 - INFO - Checkpoint saved for fanar at question 825
2025-07-07 10:17:52,825 - INFO -   Progress: 825/881
2025-07-07 10:17:52,826 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:52,826 - INFO - Processing question 826/881 (ID: 939) for fanar
2025-07-07 10:17:52,826 - INFO -   Ground truth verses: ['68:16']
2025-07-07 10:17:52,826 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:53,029 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:53,029 - WARNING -   Failed to get response for question 939
2025-07-07 10:17:53,029 - INFO - Processing question 827/881 (ID: 940) for fanar
2025-07-07 10:17:53,029 - INFO -   Ground truth verses: ['20:127']
2025-07-07 10:17:53,029 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:53,225 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:53,226 - WARNING -   Failed to get response for question 940
2025-07-07 10:17:53,226 - INFO - Processing question 828/881 (ID: 941) for fanar
2025-07-07 10:17:53,226 - INFO -   Ground truth verses: ['20:127']
2025-07-07 10:17:53,226 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:53,423 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:53,423 - WARNING -   Failed to get response for question 941
2025-07-07 10:17:53,423 - INFO - Processing question 829/881 (ID: 942) for fanar
2025-07-07 10:17:53,423 - INFO -   Ground truth verses: ['20:127']
2025-07-07 10:17:53,424 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:53,623 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:53,623 - WARNING -   Failed to get response for question 942
2025-07-07 10:17:53,623 - INFO - Processing question 830/881 (ID: 943) for fanar
2025-07-07 10:17:53,623 - INFO -   Ground truth verses: ['20:127']
2025-07-07 10:17:53,623 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:53,820 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:53,820 - WARNING -   Failed to get response for question 943
2025-07-07 10:17:53,849 - INFO - Saved data to results/fanar_checkpoint_829.json
2025-07-07 10:17:53,849 - INFO - Checkpoint saved for fanar at question 830
2025-07-07 10:17:53,849 - INFO -   Progress: 830/881
2025-07-07 10:17:53,849 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:53,849 - INFO - Processing question 831/881 (ID: 944) for fanar
2025-07-07 10:17:53,849 - INFO -   Ground truth verses: ['20:127']
2025-07-07 10:17:53,849 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:54,051 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:54,052 - WARNING -   Failed to get response for question 944
2025-07-07 10:17:54,052 - INFO - Processing question 832/881 (ID: 945) for fanar
2025-07-07 10:17:54,052 - INFO -   Ground truth verses: ['17:105', '48:8', '21:107', '33:45', '34:28', '6:48', '25:56']
2025-07-07 10:17:54,052 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:54,463 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:54,464 - WARNING -   Failed to get response for question 945
2025-07-07 10:17:54,464 - INFO - Processing question 833/881 (ID: 946) for fanar
2025-07-07 10:17:54,464 - INFO -   Ground truth verses: ['17:105', '48:8', '21:107', '33:45', '34:28', '6:48', '25:56']
2025-07-07 10:17:54,464 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:54,660 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:54,661 - WARNING -   Failed to get response for question 946
2025-07-07 10:17:54,661 - INFO - Processing question 834/881 (ID: 947) for fanar
2025-07-07 10:17:54,661 - INFO -   Ground truth verses: ['17:105', '48:8', '21:107', '33:45', '34:28', '6:48', '25:56']
2025-07-07 10:17:54,661 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:54,859 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:54,859 - WARNING -   Failed to get response for question 947
2025-07-07 10:17:54,860 - INFO - Processing question 835/881 (ID: 948) for fanar
2025-07-07 10:17:54,860 - INFO -   Ground truth verses: ['17:105', '48:8', '21:107', '33:45', '34:28', '6:48', '25:56']
2025-07-07 10:17:54,860 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:55,058 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:55,058 - WARNING -   Failed to get response for question 948
2025-07-07 10:17:55,086 - INFO - Saved data to results/fanar_checkpoint_834.json
2025-07-07 10:17:55,086 - INFO - Checkpoint saved for fanar at question 835
2025-07-07 10:17:55,086 - INFO -   Progress: 835/881
2025-07-07 10:17:55,086 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:55,086 - INFO - Processing question 836/881 (ID: 949) for fanar
2025-07-07 10:17:55,086 - INFO -   Ground truth verses: ['17:105', '48:8', '21:107', '33:45', '34:28', '6:48', '25:56']
2025-07-07 10:17:55,086 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:55,293 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:55,294 - WARNING -   Failed to get response for question 949
2025-07-07 10:17:55,294 - INFO - Processing question 837/881 (ID: 950) for fanar
2025-07-07 10:17:55,294 - INFO -   Ground truth verses: ['93:1', '91:1']
2025-07-07 10:17:55,294 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:55,491 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:55,491 - WARNING -   Failed to get response for question 950
2025-07-07 10:17:55,491 - INFO - Processing question 838/881 (ID: 951) for fanar
2025-07-07 10:17:55,491 - INFO -   Ground truth verses: ['93:1', '91:1']
2025-07-07 10:17:55,491 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:55,692 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:55,693 - WARNING -   Failed to get response for question 951
2025-07-07 10:17:55,693 - INFO - Processing question 839/881 (ID: 952) for fanar
2025-07-07 10:17:55,693 - INFO -   Ground truth verses: ['93:1', '91:1']
2025-07-07 10:17:55,693 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:55,892 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:55,892 - WARNING -   Failed to get response for question 952
2025-07-07 10:17:55,893 - INFO - Processing question 840/881 (ID: 953) for fanar
2025-07-07 10:17:55,893 - INFO -   Ground truth verses: ['93:1', '91:1']
2025-07-07 10:17:55,893 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:56,308 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:56,309 - WARNING -   Failed to get response for question 953
2025-07-07 10:17:56,336 - INFO - Saved data to results/fanar_checkpoint_839.json
2025-07-07 10:17:56,336 - INFO - Checkpoint saved for fanar at question 840
2025-07-07 10:17:56,336 - INFO -   Progress: 840/881
2025-07-07 10:17:56,336 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:56,336 - INFO - Processing question 841/881 (ID: 954) for fanar
2025-07-07 10:17:56,337 - INFO -   Ground truth verses: ['93:1', '91:1']
2025-07-07 10:17:56,337 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:56,539 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:56,540 - WARNING -   Failed to get response for question 954
2025-07-07 10:17:56,540 - INFO - Processing question 842/881 (ID: 955) for fanar
2025-07-07 10:17:56,540 - INFO -   Ground truth verses: ['60:6', '33:21']
2025-07-07 10:17:56,540 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:56,740 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:56,741 - WARNING -   Failed to get response for question 955
2025-07-07 10:17:56,741 - INFO - Processing question 843/881 (ID: 956) for fanar
2025-07-07 10:17:56,741 - INFO -   Ground truth verses: ['60:6', '33:21']
2025-07-07 10:17:56,741 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:56,940 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:56,941 - WARNING -   Failed to get response for question 956
2025-07-07 10:17:56,941 - INFO - Processing question 844/881 (ID: 957) for fanar
2025-07-07 10:17:56,941 - INFO -   Ground truth verses: ['60:6', '33:21']
2025-07-07 10:17:56,941 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:57,141 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:57,141 - WARNING -   Failed to get response for question 957
2025-07-07 10:17:57,141 - INFO - Processing question 845/881 (ID: 958) for fanar
2025-07-07 10:17:57,141 - INFO -   Ground truth verses: ['60:6', '33:21']
2025-07-07 10:17:57,141 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:57,349 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:57,349 - WARNING -   Failed to get response for question 958
2025-07-07 10:17:57,377 - INFO - Saved data to results/fanar_checkpoint_844.json
2025-07-07 10:17:57,377 - INFO - Checkpoint saved for fanar at question 845
2025-07-07 10:17:57,377 - INFO -   Progress: 845/881
2025-07-07 10:17:57,377 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:57,377 - INFO - Processing question 846/881 (ID: 959) for fanar
2025-07-07 10:17:57,377 - INFO -   Ground truth verses: ['60:6', '33:21']
2025-07-07 10:17:57,377 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:57,584 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:57,584 - WARNING -   Failed to get response for question 959
2025-07-07 10:17:57,584 - INFO - Processing question 847/881 (ID: 960) for fanar
2025-07-07 10:17:57,584 - INFO -   Ground truth verses: ['40:72']
2025-07-07 10:17:57,584 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:57,783 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:57,783 - WARNING -   Failed to get response for question 960
2025-07-07 10:17:57,783 - INFO - Processing question 848/881 (ID: 961) for fanar
2025-07-07 10:17:57,783 - INFO -   Ground truth verses: ['40:72']
2025-07-07 10:17:57,783 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:57,981 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:57,982 - WARNING -   Failed to get response for question 961
2025-07-07 10:17:57,982 - INFO - Processing question 849/881 (ID: 962) for fanar
2025-07-07 10:17:57,982 - INFO -   Ground truth verses: ['40:72']
2025-07-07 10:17:57,982 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:58,397 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:58,397 - WARNING -   Failed to get response for question 962
2025-07-07 10:17:58,397 - INFO - Processing question 850/881 (ID: 963) for fanar
2025-07-07 10:17:58,397 - INFO -   Ground truth verses: ['40:72']
2025-07-07 10:17:58,397 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:58,606 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:58,607 - WARNING -   Failed to get response for question 963
2025-07-07 10:17:58,635 - INFO - Saved data to results/fanar_checkpoint_849.json
2025-07-07 10:17:58,635 - INFO - Checkpoint saved for fanar at question 850
2025-07-07 10:17:58,635 - INFO -   Progress: 850/881
2025-07-07 10:17:58,635 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:58,635 - INFO - Processing question 851/881 (ID: 964) for fanar
2025-07-07 10:17:58,635 - INFO -   Ground truth verses: ['40:72']
2025-07-07 10:17:58,635 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:58,843 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:58,843 - WARNING -   Failed to get response for question 964
2025-07-07 10:17:58,844 - INFO - Processing question 852/881 (ID: 965) for fanar
2025-07-07 10:17:58,844 - INFO -   Ground truth verses: ['18:92', '18:89', '18:85']
2025-07-07 10:17:58,844 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:59,040 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:59,040 - WARNING -   Failed to get response for question 965
2025-07-07 10:17:59,040 - INFO - Processing question 853/881 (ID: 966) for fanar
2025-07-07 10:17:59,040 - INFO -   Ground truth verses: ['18:92', '18:89', '18:85']
2025-07-07 10:17:59,040 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:59,243 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:59,243 - WARNING -   Failed to get response for question 966
2025-07-07 10:17:59,243 - INFO - Processing question 854/881 (ID: 967) for fanar
2025-07-07 10:17:59,243 - INFO -   Ground truth verses: ['18:92', '18:89', '18:85']
2025-07-07 10:17:59,243 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:59,442 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:59,442 - WARNING -   Failed to get response for question 967
2025-07-07 10:17:59,443 - INFO - Processing question 855/881 (ID: 968) for fanar
2025-07-07 10:17:59,443 - INFO -   Ground truth verses: ['18:92', '18:89', '18:85']
2025-07-07 10:17:59,443 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:59,644 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:59,644 - WARNING -   Failed to get response for question 968
2025-07-07 10:17:59,674 - INFO - Saved data to results/fanar_checkpoint_854.json
2025-07-07 10:17:59,674 - INFO - Checkpoint saved for fanar at question 855
2025-07-07 10:17:59,674 - INFO -   Progress: 855/881
2025-07-07 10:17:59,674 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:17:59,674 - INFO - Processing question 856/881 (ID: 969) for fanar
2025-07-07 10:17:59,674 - INFO -   Ground truth verses: ['18:92', '18:89', '18:85']
2025-07-07 10:17:59,674 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:17:59,877 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:17:59,878 - WARNING -   Failed to get response for question 969
2025-07-07 10:17:59,878 - INFO - Processing question 857/881 (ID: 970) for fanar
2025-07-07 10:17:59,878 - INFO -   Ground truth verses: ['54:52']
2025-07-07 10:17:59,878 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:00,077 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:00,077 - WARNING -   Failed to get response for question 970
2025-07-07 10:18:00,077 - INFO - Processing question 858/881 (ID: 971) for fanar
2025-07-07 10:18:00,077 - INFO -   Ground truth verses: ['54:52']
2025-07-07 10:18:00,077 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:00,496 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:00,497 - WARNING -   Failed to get response for question 971
2025-07-07 10:18:00,497 - INFO - Processing question 859/881 (ID: 972) for fanar
2025-07-07 10:18:00,497 - INFO -   Ground truth verses: ['54:52']
2025-07-07 10:18:00,497 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:00,629 - INFO - Extracted JSON from code block: 453 characters
2025-07-07 10:18:00,629 - INFO - Parsed JSON response: 3 verses sorted by relevance
2025-07-07 10:18:00,629 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 10:18:00,629 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 10:18:00,629 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 10:18:00,629 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 10:18:00,629 - WARNING - Unrecognized verse format: '[Yunus]' -> 'Yunus'
2025-07-07 10:18:00,629 - INFO - Successfully parsed 3 verses from JSON with relevance scores
2025-07-07 10:18:00,629 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 10:18:00,629 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 10:18:00,629 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 10:18:00,629 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 10:18:00,630 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 10:18:00,630 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 10:18:00,630 - WARNING - Could not normalize verse reference: '[Al-A'raf]'
2025-07-07 10:18:00,630 - WARNING - Unrecognized verse format: '[Al-A'raf]' -> 'Al-A'raf'
2025-07-07 10:18:00,630 - WARNING - Unrecognized verse format: '[Yunus]' -> 'Yunus'
2025-07-07 10:18:00,630 - WARNING - Unrecognized verse format: '[Yunus]' -> 'Yunus'
2025-07-07 10:18:00,630 - INFO - Normalized predicted verses: ['10:64', '2:255', '7:113']
2025-07-07 10:18:00,630 - INFO - Normalized ground truth verses: ['15:11', '16:34', '26:6', '43:7', '6:5']
2025-07-07 10:18:00,630 - INFO -   Response: 453 chars, Found 3 verses
2025-07-07 10:18:00,630 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:00,630 - INFO -   Matched verses: []
2025-07-07 10:18:00,696 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:00,696 - WARNING -   Failed to get response for question 972
2025-07-07 10:18:00,696 - INFO - Processing question 860/881 (ID: 973) for fanar
2025-07-07 10:18:00,696 - INFO -   Ground truth verses: ['54:52']
2025-07-07 10:18:00,696 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:00,863 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:00,863 - WARNING -   Failed to get response for question 973
2025-07-07 10:18:00,891 - INFO - Saved data to results/fanar_checkpoint_859.json
2025-07-07 10:18:00,891 - INFO - Checkpoint saved for fanar at question 860
2025-07-07 10:18:00,891 - INFO -   Progress: 860/881
2025-07-07 10:18:00,891 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:00,891 - INFO - Processing question 861/881 (ID: 974) for fanar
2025-07-07 10:18:00,891 - INFO -   Ground truth verses: ['54:52']
2025-07-07 10:18:00,891 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:01,061 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:01,062 - WARNING -   Failed to get response for question 974
2025-07-07 10:18:01,062 - INFO - Processing question 862/881 (ID: 975) for fanar
2025-07-07 10:18:01,062 - INFO -   Ground truth verses: ['20:11', '28:30']
2025-07-07 10:18:01,062 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:01,236 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:01,237 - WARNING -   Failed to get response for question 975
2025-07-07 10:18:01,237 - INFO - Processing question 863/881 (ID: 976) for fanar
2025-07-07 10:18:01,237 - INFO -   Ground truth verses: ['20:11', '28:30']
2025-07-07 10:18:01,237 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:01,409 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:01,410 - WARNING -   Failed to get response for question 976
2025-07-07 10:18:01,410 - INFO - Processing question 864/881 (ID: 977) for fanar
2025-07-07 10:18:01,410 - INFO -   Ground truth verses: ['20:11', '28:30']
2025-07-07 10:18:01,410 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:01,578 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:01,578 - WARNING -   Failed to get response for question 977
2025-07-07 10:18:01,578 - INFO - Processing question 865/881 (ID: 978) for fanar
2025-07-07 10:18:01,578 - INFO -   Ground truth verses: ['20:11', '28:30']
2025-07-07 10:18:01,578 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:01,631 - INFO - Processing question 304/881 (ID: 344) for qwen
2025-07-07 10:18:01,631 - INFO -   Ground truth verses: ['6:5', '43:7', '15:11', '16:34', '26:6']
2025-07-07 10:18:01,631 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:18:01,747 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:01,747 - WARNING -   Failed to get response for question 978
2025-07-07 10:18:01,776 - INFO - Saved data to results/fanar_checkpoint_864.json
2025-07-07 10:18:01,776 - INFO - Checkpoint saved for fanar at question 865
2025-07-07 10:18:01,776 - INFO -   Progress: 865/881
2025-07-07 10:18:01,776 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:01,776 - INFO - Processing question 866/881 (ID: 979) for fanar
2025-07-07 10:18:01,776 - INFO -   Ground truth verses: ['20:11', '28:30']
2025-07-07 10:18:01,776 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:01,980 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:01,981 - WARNING -   Failed to get response for question 979
2025-07-07 10:18:01,981 - INFO - Processing question 867/881 (ID: 980) for fanar
2025-07-07 10:18:01,981 - INFO -   Ground truth verses: ['45:5', '3:190', '16:65', '10:6', '30:24', '2:164']
2025-07-07 10:18:01,981 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:02,396 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:02,397 - WARNING -   Failed to get response for question 980
2025-07-07 10:18:02,397 - INFO - Processing question 868/881 (ID: 981) for fanar
2025-07-07 10:18:02,397 - INFO -   Ground truth verses: ['45:5', '3:190', '16:65', '10:6', '30:24', '2:164']
2025-07-07 10:18:02,397 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:02,602 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:02,603 - WARNING -   Failed to get response for question 981
2025-07-07 10:18:02,603 - INFO - Processing question 869/881 (ID: 982) for fanar
2025-07-07 10:18:02,603 - INFO -   Ground truth verses: ['45:5', '3:190', '16:65', '10:6', '30:24', '2:164']
2025-07-07 10:18:02,603 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:02,803 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:02,804 - WARNING -   Failed to get response for question 982
2025-07-07 10:18:02,804 - INFO - Processing question 870/881 (ID: 983) for fanar
2025-07-07 10:18:02,804 - INFO -   Ground truth verses: ['45:5', '3:190', '16:65', '10:6', '30:24', '2:164']
2025-07-07 10:18:02,804 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:03,000 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:03,000 - WARNING -   Failed to get response for question 983
2025-07-07 10:18:03,029 - INFO - Saved data to results/fanar_checkpoint_869.json
2025-07-07 10:18:03,029 - INFO - Checkpoint saved for fanar at question 870
2025-07-07 10:18:03,029 - INFO -   Progress: 870/881
2025-07-07 10:18:03,029 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:03,029 - INFO - Processing question 871/881 (ID: 984) for fanar
2025-07-07 10:18:03,029 - INFO -   Ground truth verses: ['45:5', '3:190', '16:65', '10:6', '30:24', '2:164']
2025-07-07 10:18:03,029 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:03,234 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:03,235 - WARNING -   Failed to get response for question 984
2025-07-07 10:18:03,235 - INFO - Processing question 872/881 (ID: 985) for fanar
2025-07-07 10:18:03,235 - INFO -   Ground truth verses: ['70:30', '23:6']
2025-07-07 10:18:03,235 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:03,432 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:03,432 - WARNING -   Failed to get response for question 985
2025-07-07 10:18:03,432 - INFO - Processing question 873/881 (ID: 986) for fanar
2025-07-07 10:18:03,433 - INFO -   Ground truth verses: ['70:30', '23:6']
2025-07-07 10:18:03,433 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:03,632 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:03,632 - WARNING -   Failed to get response for question 986
2025-07-07 10:18:03,632 - INFO - Processing question 874/881 (ID: 987) for fanar
2025-07-07 10:18:03,632 - INFO -   Ground truth verses: ['70:30', '23:6']
2025-07-07 10:18:03,632 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:03,830 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:03,830 - WARNING -   Failed to get response for question 987
2025-07-07 10:18:03,831 - INFO - Processing question 875/881 (ID: 988) for fanar
2025-07-07 10:18:03,831 - INFO -   Ground truth verses: ['70:30', '23:6']
2025-07-07 10:18:03,831 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:04,027 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:04,027 - WARNING -   Failed to get response for question 988
2025-07-07 10:18:04,056 - INFO - Saved data to results/fanar_checkpoint_874.json
2025-07-07 10:18:04,056 - INFO - Checkpoint saved for fanar at question 875
2025-07-07 10:18:04,057 - INFO -   Progress: 875/881
2025-07-07 10:18:04,057 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:04,057 - INFO - Processing question 876/881 (ID: 989) for fanar
2025-07-07 10:18:04,057 - INFO -   Ground truth verses: ['70:30', '23:6']
2025-07-07 10:18:04,057 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:04,482 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:04,483 - WARNING -   Failed to get response for question 989
2025-07-07 10:18:04,483 - INFO - Processing question 877/881 (ID: 990) for fanar
2025-07-07 10:18:04,483 - INFO -   Ground truth verses: ['54:53']
2025-07-07 10:18:04,483 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:04,692 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:04,693 - WARNING -   Failed to get response for question 990
2025-07-07 10:18:04,693 - INFO - Processing question 878/881 (ID: 991) for fanar
2025-07-07 10:18:04,693 - INFO -   Ground truth verses: ['54:53']
2025-07-07 10:18:04,693 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:04,891 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:04,892 - WARNING -   Failed to get response for question 991
2025-07-07 10:18:04,892 - INFO - Processing question 879/881 (ID: 992) for fanar
2025-07-07 10:18:04,892 - INFO -   Ground truth verses: ['54:53']
2025-07-07 10:18:04,892 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:05,090 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:05,091 - WARNING -   Failed to get response for question 992
2025-07-07 10:18:05,091 - INFO - Processing question 880/881 (ID: 993) for fanar
2025-07-07 10:18:05,091 - INFO -   Ground truth verses: ['54:53']
2025-07-07 10:18:05,091 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:05,289 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:05,289 - WARNING -   Failed to get response for question 993
2025-07-07 10:18:05,318 - INFO - Saved data to results/fanar_checkpoint_879.json
2025-07-07 10:18:05,318 - INFO - Checkpoint saved for fanar at question 880
2025-07-07 10:18:05,318 - INFO -   Progress: 880/881
2025-07-07 10:18:05,318 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:05,318 - INFO - Processing question 881/881 (ID: 994) for fanar
2025-07-07 10:18:05,318 - INFO -   Ground truth verses: ['54:53']
2025-07-07 10:18:05,318 - INFO - Using cached model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:18:05,522 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:18:05,523 - WARNING -   Failed to get response for question 994
2025-07-07 10:18:05,797 - INFO - Cleared model cache and GPU memory
2025-07-07 10:18:05,825 - INFO - Saved data to results/fanar_final_20250707_101805.json
2025-07-07 10:18:05,825 - INFO - Final results for fanar:
2025-07-07 10:18:05,825 - INFO -   Total questions: 881
2025-07-07 10:18:05,825 - INFO -   Successful responses: 0
2025-07-07 10:18:05,825 - INFO -   Overall F1 score: 0.000
2025-07-07 10:18:05,825 - INFO -   Overall Precision: 0.000
2025-07-07 10:18:05,826 - INFO -   Overall Recall: 0.000
2025-07-07 10:18:05,826 - INFO -   Average F1 score: 0.000
2025-07-07 10:18:05,826 - INFO -   Average response time: 0.27s
2025-07-07 10:18:06,053 - INFO - Cleared model cache and GPU memory
2025-07-07 10:18:06,057 - INFO - Saved data to results/benchmark_summary_20250707_101806.json
2025-07-07 10:18:11,581 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:18:11,581 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:18:11,581 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:18:11,581 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:18:11,581 - INFO - Normalized predicted verses: []
2025-07-07 10:18:11,581 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 10:18:11,581 - INFO -   Response: 4254 chars, Found 0 verses
2025-07-07 10:18:11,581 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:11,581 - INFO -   Matched verses: []
2025-07-07 10:18:11,581 - INFO - Processing question 8/881 (ID: 7) for deepseek
2025-07-07 10:18:11,581 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:18:11,581 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:18:20,069 - INFO - Extracted JSON from code block: 749 characters
2025-07-07 10:18:20,069 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:18:20,069 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:18:20,069 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:18:20,069 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:18:20,069 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 10:18:20,069 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 10:18:20,069 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:18:20,070 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:18:20,070 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:18:20,070 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:18:20,070 - WARNING - Invalid colon format: '[Al-An'am:85]' -> 'Al-An'am:85'
2025-07-07 10:18:20,070 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:18:20,070 - WARNING - Invalid colon format: '[Al-An'am:86]' -> 'Al-An'am:86'
2025-07-07 10:18:20,070 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 10:18:20,070 - WARNING - Invalid colon format: '[Al-An'am:87]' -> 'Al-An'am:87'
2025-07-07 10:18:20,070 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 10:18:20,070 - WARNING - Invalid colon format: '[Al-An'am:88]' -> 'Al-An'am:88'
2025-07-07 10:18:20,070 - INFO - Normalized predicted verses: ['6:84', '6:85', '6:86', '6:87', '6:88']
2025-07-07 10:18:20,070 - INFO - Normalized ground truth verses: ['15:11', '16:34', '26:6', '43:7', '6:5']
2025-07-07 10:18:20,070 - INFO -   Response: 749 chars, Found 5 verses
2025-07-07 10:18:20,070 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:20,070 - INFO -   Matched verses: []
2025-07-07 10:18:21,071 - INFO - Processing question 305/881 (ID: 345) for qwen
2025-07-07 10:18:21,071 - INFO -   Ground truth verses: ['6:5', '43:7', '15:11', '16:34', '26:6']
2025-07-07 10:18:21,071 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:18:29,093 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 10:18:29,093 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:18:29,093 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:18:29,093 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 10:18:29,093 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:18:29,093 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:18:29,093 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:18:29,093 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:18:29,093 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 10:18:29,093 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 10:18:29,093 - INFO -   Response: 4462 chars, Found 2 verses
2025-07-07 10:18:29,093 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:29,093 - INFO -   Matched verses: []
2025-07-07 10:18:29,093 - INFO - Processing question 9/881 (ID: 8) for deepseek
2025-07-07 10:18:29,093 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:18:29,093 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:18:35,236 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 629), falling back to regex parsing
2025-07-07 10:18:35,236 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 629), falling back to regex parsing
2025-07-07 10:18:35,236 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 629), falling back to regex parsing
2025-07-07 10:18:35,236 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:18:35,236 - INFO - Normalized predicted verses: []
2025-07-07 10:18:35,236 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 10:18:35,236 - INFO -   Response: 1116 chars, Found 0 verses
2025-07-07 10:18:35,236 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:35,236 - INFO -   Matched verses: []
2025-07-07 10:18:35,236 - INFO - Processing question 10/881 (ID: 9) for deepseek
2025-07-07 10:18:35,236 - INFO -   Ground truth verses: ['6:43']
2025-07-07 10:18:35,236 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:18:38,772 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:18:38,772 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:18:38,772 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:18:38,772 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:18:38,772 - INFO - Normalized predicted verses: []
2025-07-07 10:18:38,772 - INFO - Normalized ground truth verses: ['15:11', '16:34', '26:6', '43:7', '6:5']
2025-07-07 10:18:38,772 - INFO -   Response: 4585 chars, Found 0 verses
2025-07-07 10:18:38,772 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:38,772 - INFO -   Matched verses: []
2025-07-07 10:18:38,797 - INFO - Saved data to results/qwen_checkpoint_304.json
2025-07-07 10:18:38,797 - INFO - Checkpoint saved for qwen at question 305
2025-07-07 10:18:38,797 - INFO -   Progress: 305/881
2025-07-07 10:18:38,797 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.003
2025-07-07 10:18:39,798 - INFO - Processing question 306/881 (ID: 346) for qwen
2025-07-07 10:18:39,798 - INFO -   Ground truth verses: ['69:49']
2025-07-07 10:18:39,798 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:18:43,320 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 633), falling back to regex parsing
2025-07-07 10:18:43,320 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 633), falling back to regex parsing
2025-07-07 10:18:43,321 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 633), falling back to regex parsing
2025-07-07 10:18:43,321 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:18:43,321 - INFO - Normalized predicted verses: []
2025-07-07 10:18:43,321 - INFO - Normalized ground truth verses: ['6:43']
2025-07-07 10:18:43,321 - INFO -   Response: 1520 chars, Found 0 verses
2025-07-07 10:18:43,321 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:43,321 - INFO -   Matched verses: []
2025-07-07 10:18:43,325 - INFO - Saved data to results/deepseek_checkpoint_9.json
2025-07-07 10:18:43,325 - INFO - Checkpoint saved for deepseek at question 10
2025-07-07 10:18:43,325 - INFO -   Progress: 10/881
2025-07-07 10:18:43,326 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:43,326 - INFO - Processing question 11/881 (ID: 10) for deepseek
2025-07-07 10:18:43,326 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 10:18:43,326 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:18:57,542 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:18:57,542 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:18:57,542 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:18:57,542 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:18:57,542 - INFO - Normalized predicted verses: []
2025-07-07 10:18:57,542 - INFO - Normalized ground truth verses: ['69:49']
2025-07-07 10:18:57,542 - INFO -   Response: 4015 chars, Found 0 verses
2025-07-07 10:18:57,542 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:18:57,542 - INFO -   Matched verses: []
2025-07-07 10:18:58,544 - INFO - Processing question 307/881 (ID: 347) for qwen
2025-07-07 10:18:58,544 - INFO -   Ground truth verses: ['69:49']
2025-07-07 10:18:58,544 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:19:00,948 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:19:00,949 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:19:00,949 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:19:00,949 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:19:00,949 - INFO - Normalized predicted verses: []
2025-07-07 10:19:00,949 - INFO - Normalized ground truth verses: ['10:85', '60:5']
2025-07-07 10:19:00,949 - INFO -   Response: 4214 chars, Found 0 verses
2025-07-07 10:19:00,949 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:19:00,949 - INFO -   Matched verses: []
2025-07-07 10:19:00,949 - INFO - Processing question 12/881 (ID: 11) for deepseek
2025-07-07 10:19:00,949 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 10:19:00,949 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:19:16,354 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:19:16,354 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:19:16,355 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-An'am:84]' -> 'Al-An'am:84'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Ma'idah:104]' -> 'Al-Ma'idah:104'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Hijr:10]' -> 'Al-Hijr:10'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:19:16,355 - WARNING - Invalid colon format: '[Al-Qamar:5]' -> 'Al-Qamar:5'
2025-07-07 10:19:16,355 - INFO - Normalized predicted verses: ['15:10', '2:255', '57:5', '5:104', '6:84']
2025-07-07 10:19:16,355 - INFO - Normalized ground truth verses: ['69:49']
2025-07-07 10:19:16,355 - INFO -   Response: 4683 chars, Found 5 verses
2025-07-07 10:19:16,355 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:19:16,355 - INFO -   Matched verses: []
2025-07-07 10:19:17,356 - INFO - Processing question 308/881 (ID: 349) for qwen
2025-07-07 10:19:17,356 - INFO -   Ground truth verses: ['69:49']
2025-07-07 10:19:17,357 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:19:18,602 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:19:18,602 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:19:18,602 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:19:18,602 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:19:18,602 - INFO - Normalized predicted verses: []
2025-07-07 10:19:18,602 - INFO - Normalized ground truth verses: ['10:85', '60:5']
2025-07-07 10:19:18,602 - INFO -   Response: 3212 chars, Found 0 verses
2025-07-07 10:19:18,602 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:19:18,602 - INFO -   Matched verses: []
2025-07-07 10:19:18,602 - INFO - Processing question 13/881 (ID: 12) for deepseek
2025-07-07 10:19:18,602 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 10:19:18,602 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:19:35,122 - INFO - Extracted JSON from code block: 103 characters
2025-07-07 10:19:35,122 - INFO - Parsed JSON response: 0 verses sorted by relevance
2025-07-07 10:19:35,123 - INFO - Successfully parsed 0 verses from JSON with relevance scores
2025-07-07 10:19:35,123 - INFO - Normalized predicted verses: []
2025-07-07 10:19:35,123 - INFO - Normalized ground truth verses: ['69:49']
2025-07-07 10:19:35,123 - INFO -   Response: 103 chars, Found 0 verses
2025-07-07 10:19:35,123 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:19:35,123 - INFO -   Matched verses: []
2025-07-07 10:19:36,124 - INFO - Processing question 309/881 (ID: 350) for qwen
2025-07-07 10:19:36,124 - INFO -   Ground truth verses: ['69:49']
2025-07-07 10:19:36,124 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:19:36,179 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:19:36,179 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:19:36,179 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:19:36,179 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:19:36,180 - INFO - Normalized predicted verses: []
2025-07-07 10:19:36,180 - INFO - Normalized ground truth verses: ['10:85', '60:5']
2025-07-07 10:19:36,180 - INFO -   Response: 4390 chars, Found 0 verses
2025-07-07 10:19:36,180 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:19:36,180 - INFO -   Matched verses: []
2025-07-07 10:19:36,180 - INFO - Processing question 14/881 (ID: 13) for deepseek
2025-07-07 10:19:36,180 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 10:19:36,180 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:19:45,904 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:19:45,905 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:19:45,905 - WARNING - Failed to parse JSON response: Extra data: line 15 column 11 (char 382), falling back to regex parsing
2025-07-07 10:19:45,905 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:19:45,905 - INFO - Normalized predicted verses: []
2025-07-07 10:19:45,905 - INFO - Normalized ground truth verses: ['10:85', '60:5']
2025-07-07 10:19:45,905 - INFO -   Response: 2114 chars, Found 0 verses
2025-07-07 10:19:45,905 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:19:45,905 - INFO -   Matched verses: []
2025-07-07 10:19:45,905 - INFO - Processing question 15/881 (ID: 14) for deepseek
2025-07-07 10:19:45,905 - INFO -   Ground truth verses: ['60:5', '10:85']
2025-07-07 10:19:45,905 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:19:53,720 - INFO - Extracted JSON from code block: 321 characters
2025-07-07 10:19:53,720 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 10:19:53,720 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:19:53,720 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:19:53,720 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 10:19:53,720 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:19:53,720 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:19:53,720 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:19:53,720 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:19:53,720 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 10:19:53,721 - INFO - Normalized ground truth verses: ['69:49']
2025-07-07 10:19:53,721 - INFO -   Response: 321 chars, Found 2 verses
2025-07-07 10:19:53,721 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:19:53,721 - INFO -   Matched verses: []
2025-07-07 10:19:54,722 - INFO - Processing question 310/881 (ID: 351) for qwen
2025-07-07 10:19:54,722 - INFO -   Ground truth verses: ['42:13']
2025-07-07 10:19:54,722 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:20:03,626 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:20:03,626 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:20:03,626 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 867), falling back to regex parsing
2025-07-07 10:20:03,626 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:20:03,626 - INFO - Normalized predicted verses: []
2025-07-07 10:20:03,626 - INFO - Normalized ground truth verses: ['10:85', '60:5']
2025-07-07 10:20:03,626 - INFO -   Response: 4230 chars, Found 0 verses
2025-07-07 10:20:03,626 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:20:03,626 - INFO -   Matched verses: []
2025-07-07 10:20:03,632 - INFO - Saved data to results/deepseek_checkpoint_14.json
2025-07-07 10:20:03,632 - INFO - Checkpoint saved for deepseek at question 15
2025-07-07 10:20:03,632 - INFO -   Progress: 15/881
2025-07-07 10:20:03,632 - INFO -   Running averages - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:20:03,632 - INFO - Processing question 16/881 (ID: 15) for deepseek
2025-07-07 10:20:03,632 - INFO -   Ground truth verses: ['52:44']
2025-07-07 10:20:03,632 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:20:05,488 - INFO - Parsed JSON response: 2 verses sorted by relevance
2025-07-07 10:20:05,488 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:20:05,488 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:20:05,488 - INFO - Successfully parsed 2 verses from JSON with relevance scores
2025-07-07 10:20:05,488 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:20:05,488 - WARNING - Invalid colon format: '[Al-Baqarah:255]' -> 'Al-Baqarah:255'
2025-07-07 10:20:05,488 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:20:05,488 - WARNING - Invalid colon format: '[Al-A'raf:113]' -> 'Al-A'raf:113'
2025-07-07 10:20:05,488 - INFO - Normalized predicted verses: ['2:255', '7:113']
2025-07-07 10:20:05,488 - INFO - Normalized ground truth verses: ['52:44']
2025-07-07 10:20:05,488 - INFO -   Response: 312 chars, Found 2 verses
2025-07-07 10:20:05,488 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:20:05,488 - INFO -   Matched verses: []
2025-07-07 10:20:05,488 - INFO - Processing question 17/881 (ID: 16) for deepseek
2025-07-07 10:20:05,488 - INFO -   Ground truth verses: ['52:44']
2025-07-07 10:20:05,488 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:20:11,213 - INFO - Loaded 881 questions from final_bloom_questions_cleaned.jsonl
2025-07-07 10:20:11,217 - INFO - Loaded 6236 verses for validation
2025-07-07 10:20:11,218 - INFO - Starting LLM Benchmark for Quranic Similar Verse Identification (Local Models)
2025-07-07 10:20:11,218 - INFO - Models to test: fanar
2025-07-07 10:20:11,218 - INFO - Questions limit: All
2025-07-07 10:20:11,218 - INFO - Using device: cuda
2025-07-07 10:20:11,431 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:20:11,432 - INFO - 
============================================================
2025-07-07 10:20:11,432 - INFO - Starting evaluation for FANAR
2025-07-07 10:20:11,432 - INFO - ============================================================
2025-07-07 10:20:11,432 - INFO - Starting benchmark for FANAR
2025-07-07 10:20:11,432 - INFO - Using device: cuda
2025-07-07 10:20:11,432 - INFO - GPU: NVIDIA H100 80GB HBM3, Memory: 79.1 GB
2025-07-07 10:20:11,440 - INFO - Loaded checkpoint for fanar: 880 questions completed
2025-07-07 10:20:11,440 - INFO - Processing 1 questions for fanar (starting from 881)
2025-07-07 10:20:11,440 - INFO - Processing question 881/881 (ID: 994) for fanar
2025-07-07 10:20:11,440 - INFO -   Ground truth verses: ['54:53']
2025-07-07 10:20:11,440 - INFO - Loading model: QCRI/Fanar-1-9B-Instruct
2025-07-07 10:20:11,440 - INFO - Device: cuda, Dtype: torch.float16, Quantization: False
2025-07-07 10:20:12,578 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:20:12,578 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:20:12,578 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:20:12,578 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:20:12,579 - INFO - Normalized predicted verses: []
2025-07-07 10:20:12,579 - INFO - Normalized ground truth verses: ['42:13']
2025-07-07 10:20:12,579 - INFO -   Response: 3001 chars, Found 0 verses
2025-07-07 10:20:12,579 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:20:12,579 - INFO -   Matched verses: []
2025-07-07 10:20:12,603 - INFO - Saved data to results/qwen_checkpoint_309.json
2025-07-07 10:20:12,603 - INFO - Checkpoint saved for qwen at question 310
2025-07-07 10:20:12,603 - INFO -   Progress: 310/881
2025-07-07 10:20:12,603 - INFO -   Running averages - F1: 0.001, Precision: 0.001, Recall: 0.003
2025-07-07 10:20:13,604 - INFO - Processing question 311/881 (ID: 352) for qwen
2025-07-07 10:20:13,604 - INFO -   Ground truth verses: ['42:13']
2025-07-07 10:20:13,604 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:20:13,799 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 629), falling back to regex parsing
2025-07-07 10:20:13,799 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 629), falling back to regex parsing
2025-07-07 10:20:13,799 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 629), falling back to regex parsing
2025-07-07 10:20:13,799 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:20:13,799 - INFO - Normalized predicted verses: []
2025-07-07 10:20:13,799 - INFO - Normalized ground truth verses: ['52:44']
2025-07-07 10:20:13,799 - INFO -   Response: 1529 chars, Found 0 verses
2025-07-07 10:20:13,799 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:20:13,799 - INFO -   Matched verses: []
2025-07-07 10:20:13,799 - INFO - Processing question 18/881 (ID: 17) for deepseek
2025-07-07 10:20:13,799 - INFO -   Ground truth verses: ['52:44']
2025-07-07 10:20:13,799 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:20:17,662 - INFO - Successfully loaded QCRI/Fanar-1-9B-Instruct
2025-07-07 10:20:19,740 - ERROR - Error during inference with fanar: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}

from user code:
   File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 584, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 471, in forward
    layer_outputs = decoder_layer(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 273, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 221, in forward
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
  File "/home/local/QCRI/easgari/anaconda3/envs/llmlab/lib/python3.9/site-packages/transformers/utils/generic.py", line 973, in __getitem__
    return self._global_mapping[key]

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-07-07 10:20:19,741 - WARNING -   Failed to get response for question 994
2025-07-07 10:20:19,952 - INFO - Cleared model cache and GPU memory
2025-07-07 10:20:19,980 - INFO - Saved data to results/fanar_final_20250707_102019.json
2025-07-07 10:20:19,980 - INFO - Final results for fanar:
2025-07-07 10:20:19,980 - INFO -   Total questions: 881
2025-07-07 10:20:19,980 - INFO -   Successful responses: 0
2025-07-07 10:20:19,980 - INFO -   Overall F1 score: 0.000
2025-07-07 10:20:19,980 - INFO -   Overall Precision: 0.000
2025-07-07 10:20:19,980 - INFO -   Overall Recall: 0.000
2025-07-07 10:20:19,980 - INFO -   Average F1 score: 0.000
2025-07-07 10:20:19,980 - INFO -   Average response time: 0.28s
2025-07-07 10:20:21,975 - INFO - Cleared model cache and GPU memory
2025-07-07 10:20:21,979 - INFO - Saved data to results/benchmark_summary_20250707_102021.json
2025-07-07 10:20:31,760 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 863), falling back to regex parsing
2025-07-07 10:20:31,760 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 863), falling back to regex parsing
2025-07-07 10:20:31,761 - WARNING - Failed to parse JSON response: Extra data: line 30 column 9 (char 863), falling back to regex parsing
2025-07-07 10:20:31,761 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:20:31,761 - INFO - Normalized predicted verses: []
2025-07-07 10:20:31,761 - INFO - Normalized ground truth verses: ['52:44']
2025-07-07 10:20:31,761 - INFO -   Response: 4207 chars, Found 0 verses
2025-07-07 10:20:31,761 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:20:31,761 - INFO -   Matched verses: []
2025-07-07 10:20:31,761 - INFO - Processing question 19/881 (ID: 18) for deepseek
2025-07-07 10:20:31,761 - INFO -   Ground truth verses: ['52:44']
2025-07-07 10:20:31,761 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:20:31,978 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:20:31,979 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:20:31,979 - WARNING - Failed to parse JSON response: Extra data: line 1 column 184 (char 183), falling back to regex parsing
2025-07-07 10:20:31,979 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:20:31,979 - INFO - Normalized predicted verses: []
2025-07-07 10:20:31,979 - INFO - Normalized ground truth verses: ['42:13']
2025-07-07 10:20:31,979 - INFO -   Response: 3093 chars, Found 0 verses
2025-07-07 10:20:31,979 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:20:31,979 - INFO -   Matched verses: []
2025-07-07 10:20:32,980 - INFO - Processing question 312/881 (ID: 353) for qwen
2025-07-07 10:20:32,980 - INFO -   Ground truth verses: ['42:13']
2025-07-07 10:20:32,980 - INFO - Using cached model: Qwen/Qwen2.5-1.5B-Instruct
2025-07-07 10:20:39,981 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 629), falling back to regex parsing
2025-07-07 10:20:39,982 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 629), falling back to regex parsing
2025-07-07 10:20:39,982 - WARNING - Failed to parse JSON response: Extra data: line 30 column 1 (char 629), falling back to regex parsing
2025-07-07 10:20:39,982 - INFO - Regex fallback created 0 detailed verses
2025-07-07 10:20:39,982 - INFO - Normalized predicted verses: []
2025-07-07 10:20:39,982 - INFO - Normalized ground truth verses: ['52:44']
2025-07-07 10:20:39,982 - INFO -   Response: 1480 chars, Found 0 verses
2025-07-07 10:20:39,982 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:20:39,982 - INFO -   Matched verses: []
2025-07-07 10:20:39,982 - INFO - Processing question 20/881 (ID: 19) for deepseek
2025-07-07 10:20:39,982 - INFO -   Ground truth verses: ['52:44']
2025-07-07 10:20:39,982 - INFO - Using cached model: suayptalha/DeepSeek-R1-Distill-Llama-3B
2025-07-07 10:20:50,691 - INFO - Extracted JSON from code block: 745 characters
2025-07-07 10:20:50,692 - INFO - Parsed JSON response: 5 verses sorted by relevance
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-Tawbah]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-Tawbah]' -> 'Al-Tawbah'
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-Hujurat]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-Hujurat]' -> 'Al-Hujurat'
2025-07-07 10:20:50,692 - INFO - Successfully parsed 5 verses from JSON with relevance scores
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-Baqarah]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-Baqarah]' -> 'Al-Baqarah'
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-An'am]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-An'am]' -> 'Al-An'am'
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 10:20:50,692 - WARNING - Could not normalize verse reference: '[Al-Ma'idah]'
2025-07-07 10:20:50,692 - WARNING - Unrecognized verse format: '[Al-Ma'idah]' -> 'Al-Ma'idah'
2025-07-07 10:20:50,693 - WARNING - Could not normalize verse reference: '[Al-Tawbah]'
2025-07-07 10:20:50,693 - WARNING - Unrecognized verse format: '[Al-Tawbah]' -> 'Al-Tawbah'
2025-07-07 10:20:50,693 - WARNING - Could not normalize verse reference: '[Al-Tawbah]'
2025-07-07 10:20:50,693 - WARNING - Unrecognized verse format: '[Al-Tawbah]' -> 'Al-Tawbah'
2025-07-07 10:20:50,693 - WARNING - Could not normalize verse reference: '[Al-Hujurat]'
2025-07-07 10:20:50,693 - WARNING - Unrecognized verse format: '[Al-Hujurat]' -> 'Al-Hujurat'
2025-07-07 10:20:50,693 - WARNING - Could not normalize verse reference: '[Al-Hujurat]'
2025-07-07 10:20:50,693 - WARNING - Unrecognized verse format: '[Al-Hujurat]' -> 'Al-Hujurat'
2025-07-07 10:20:50,693 - INFO - Normalized predicted verses: ['2:255', '49:13', '5:48', '6:159', '9:33']
2025-07-07 10:20:50,693 - INFO - Normalized ground truth verses: ['42:13']
2025-07-07 10:20:50,693 - INFO -   Response: 745 chars, Found 5 verses
2025-07-07 10:20:50,693 - INFO -   Similarity metrics - F1: 0.000, Precision: 0.000, Recall: 0.000
2025-07-07 10:20:50,693 - INFO -   Matched verses: []
